<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="mahmood" />
<meta name="generator" content="Org Mode" />
<title>cuda programming in common lisp</title><!-- lambda icon, frail attempt -->
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%221em%22 font-size=%22100%22 color=%22red%22>λ</text></svg>">
<!-- not-so-awesome awesome font -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

<link rel="stylesheet" href="/main.css">
</head>
<body>
<div id="preamble" class="status">
<div class="navbar">
  <a href='/'>home</a>
  <a href='/blog.html'>blog</a>
  <a href='/archive.html'>archive</a>
  <a href='/about.html'>about</a>
</div><h1 class="main-title">cuda programming in common lisp</h1>
</div>
<div id="content" class="content">
<p>
using <code>cl-cuda</code>
load it using quicklisp:
</p>
<div class="org-src-container">
<pre class="src src-lisp"><span style="color: #d0730f;">(</span>ql:quickload <span style="color: #ff7a7f; font-weight: bold;">:cl-cuda</span><span style="color: #d0730f;">)</span>
</pre>
</div>
<div id="outline-container-org147d150" class="outline-2">
<h2 id="org147d150">arch linux setup</h2>
<div class="outline-text-2" id="text-org147d150">
<p>
i had to install <code>cuda</code> for the <code>nvcc</code> compiler and more, and <code>nvidia</code> for the gpu driver, at first you may have to use <code>/opt/cuda/bin/nvcc</code> until you reboot or extend your $PATH.
</p>

<p>
to check which nvidia driver is in use run:
</p>
<div class="org-src-container">
<pre class="src src-sh">lspci -k | grep -A 2 -E <span style="color: #f06a3f;">"(VGA|3D)"</span>
</pre>
</div>

<p>
if you have installed the <code>nvidia</code> package and your os still uses <code>nouveau</code> you need to reboot. if it still doesnt use <code>nvidia</code> you need to update your while system with <code>pacman -Syyu</code> (you've been doing partial upgrades).
</p>
</div>
</div>
<div id="outline-container-orgaf3758d" class="outline-2">
<h2 id="orgaf3758d">essential concepts</h2>
<div class="outline-text-2" id="text-orgaf3758d">
</div>
<div id="outline-container-orge529e0f" class="outline-3">
<h3 id="orge529e0f">threads</h3>
<div class="outline-text-3" id="text-orge529e0f">
<p>
mostly from <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model</a>.
</p>

<p>
i also read the examples from the cl-cuda's source code.
</p>

<p>
<b>kernels</b> are just functions that are run by different cuda threads in a <b>thread block</b>, a thread block is a group of threads each given an index, thread blocks can be nested up to 3 times, an index is a 3d vector with each value corresponding to the index in one of the dimensions x,y,z, there is a limit to the number of threads per block, which on current GPUs is 1024. the index of a thread is stored in the special variable <code>threadIdx</code>, the index of a block is stored in the special variable <code>blockIdx</code>, the dimensions of a block can be given by <code>blockDim</code>, all of these variables are 3d vectors and have <code>x,y,z</code> members.
</p>

<p>
a kernel can be executed by multiple equally-shaped thread blocks, so that the total number of threads is equal to the number of threads per block times the number of blocks.
</p>

<p>
blocks are organized into a one-dimensional, two-dimensional, or three-dimensional <b>grid of thread blocks</b>. the number of thread blocks in a grid is usually dictated by the size of the data being processed, which typically exceeds the number of processors in the system.
</p>

<p>
a thread block size of 16x16 (256 threads) is a common choice.
</p>

<p>
thread blocks are required to execute independently: it must be possible to execute them in any order, in parallel or in series. this independence requirement allows thread blocks to be scheduled in any order across any number of cores, enabling programmers to write code that scales with the number of cores.
</p>

<p>
threads within a block can cooperate by sharing data through some <b>shared memory</b> and by synchronizing their execution to coordinate memory accesses. more precisely, one can specify synchronization points in the kernel by calling the <code>__syncthreads()</code> intrinsic function; <code>__syncthreads()</code> acts as a barrier at which all threads in the block must wait before any is allowed to proceed. in addition to <code>__syncthreads()</code>, the cooperative groups api provides a rich set of thread-synchronization primitives.
</p>

<p>
for efficient cooperation, the shared memory is expected to be a low-latency memory near each processor core (much like an L1 cache) and <code>__syncthreads()</code> is expected to be lightweight.
</p>
</div>
</div>
<div id="outline-container-org7ea1d1b" class="outline-3">
<h3 id="org7ea1d1b">memory</h3>
<div class="outline-text-3" id="text-org7ea1d1b">
<p>
CUDA threads may access data from multiple memory spaces during their execution. each thread has private <b>local memory</b>. each thread block has <b>shared memory</b> visible to all threads of the block and with the same lifetime as the block. thread blocks in a thread block cluster can perform read, write, and atomics operations on each other’s shared memory. all threads have access to the same <b>global memory</b>.
</p>

<p>
there are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces. the global, constant, and texture memory spaces are optimized for different memory usages. texture memory also offers different addressing modes, as well as data filtering, for some specific data formats.
</p>

<p>
the global, constant, and texture memory spaces are persistent across kernel launches by the same application.
</p>

<p>
<b>unified memory</b> provides <b>managed memory</b> to bridge the host and <b>device memory</b> spaces. managed memory is accessible from all CPUs and GPUs in the system as a single, coherent memory image with a common address space. this capability enables oversubscription of device memory and can greatly simplify the task of porting applications by eliminating the need to explicitly mirror data on host and device.
</p>

<p>
the CUDA programming model also assumes that both the host and the device maintain their own separate memory spaces in DRAM, referred to as host memory and device memory, respectively. therefore, a program manages the global, constant, and texture memory spaces visible to kernels through calls to the CUDA runtime. this includes device memory allocation and deallocation as well as data transfer between host and device memory.
</p>

<p>
unified memory provides managed memory to bridge the host and device memory spaces. managed memory is accessible from all CPUs and GPUs in the system as a single, coherent memory image with a common address space. this capability enables oversubscription of device memory and can greatly simplify the task of porting applications by eliminating the need to explicitly mirror data on host and device.
</p>
</div>
</div>
<div id="outline-container-orgfb59e1e" class="outline-3">
<h3 id="orgfb59e1e">device memory</h3>
<div class="outline-text-3" id="text-orgfb59e1e">
<p>
the CUDA programming model assumes a system composed of a host and a device, each with their own separate memory. kernels operate out of device memory, so the runtime provides functions to allocate, deallocate, and copy device memory, as well as transfer data between host memory and device memory.
</p>

<p>
device memory can be allocated either as linear memory or as <b>CUDA arrays</b>.
</p>

<p>
CUDA arrays are opaque memory layouts optimized for texture fetching.
</p>

<p>
<b>linear memory</b> is allocated in a single unified address space, which means that separately allocated entities can reference one another via pointers, for example, in a binary tree or linked list. the size of the address space depends on the host system (CPU) and the compute capability of the used GPU.
</p>
</div>
</div>
</div>
<div id="outline-container-orgfe3c9c2" class="outline-2">
<h2 id="orgfe3c9c2">code</h2>
<div class="outline-text-2" id="text-orgfe3c9c2">
<p>
see <a href="/connect_to_remote_common_lisp_repl_with_sly_slime.html">connect to remote common lisp repl with sly/slime</a> for connecting to a remote common lisp repl with emacs.
</p>

<div class="math-block note" data-before="note" data-after="" id="orgf3f714d">
<p>
notice that when working cl-cuda the data that we're passing to the gpu is gonna be duplicated (because its a binding from cl to C and uses whats called foreign memory), i never considered how bad that is, atleast in this case, as mostly when doing gpu calculations we're ofcourse passing lots of data and then copying it back, and the maximum amount of memory that i can use is already cut in half, if i have 16gb of memory i already know i cant use more than 8gb of that, thats pretty horrible (assuming i have a gpu with ~16gb memory), i can have my way around this, e.g. free memory from sbcl as it gets copied to C, but that'd be much more complexity than i wanna have to deal with
</p>

</div>
</div>
<div id="outline-container-orga2673cf" class="outline-3">
<h3 id="orga2673cf">cuda vector addition</h3>
<div class="outline-text-3" id="text-orga2673cf">
<p>
we start with the vectorAdd example, a simple example in C, stolen from <a href="https://github.com/NVIDIA/cuda-samples/blob/master/Samples/0_Introduction/vectorAdd/vectorAdd.cu">https://github.com/NVIDIA/cuda-samples/blob/master/Samples/0_Introduction/vectorAdd/vectorAdd.cu</a> and modified to reduce boilerplate code.
</p>

<p>
the syntax for calling a kernel in c++ is <code>vector&lt;&lt;&lt; blocks per grid, threads per block &gt;&gt;&gt;(arg1,arg2,...)</code>
consider the following program which does vector addition
place this code in a file with a <code>.cu</code> extension (only .cu works)
</p>
<div class="org-src-container">
<pre class="src src-C"><span style="color: #d570af;">#include</span> <span style="color: #d0730f;">&lt;</span><span style="color: #f06a3f;">stdio.h</span><span style="color: #d0730f;">&gt;</span>

__global__ <span style="color: #2fa526;">void</span> <span style="color: #3dbbb0;">vector_add</span><span style="color: #d0730f;">(</span><span style="color: #2fa526;">float</span> *<span style="color: #6fafff;">c</span>, <span style="color: #2fa526;">float</span> *<span style="color: #6fafff;">a</span>, <span style="color: #2fa526;">float</span> *<span style="color: #6fafff;">b</span>, <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">n</span><span style="color: #d0730f;">)</span> <span style="color: #d0730f;">{</span>
  <span style="color: #c48702; font-weight: bold;">for</span> <span style="color: #64aa0f;">(</span><span style="color: #2fa526;">int</span> <span style="color: #6fafff;">i</span> = 0; i &lt; n; i++<span style="color: #64aa0f;">)</span> <span style="color: #64aa0f;">{</span>
    c<span style="color: #ef656a;">[</span>i<span style="color: #ef656a;">]</span> = a<span style="color: #ef656a;">[</span>i<span style="color: #ef656a;">]</span> + b<span style="color: #ef656a;">[</span>i<span style="color: #ef656a;">]</span>;
  <span style="color: #64aa0f;">}</span>
<span style="color: #d0730f;">}</span>

<span style="color: #2fa526;">int</span> <span style="color: #3dbbb0;">main</span><span style="color: #d0730f;">()</span> <span style="color: #d0730f;">{</span>
  <span style="color: #c48702; font-weight: bold;">const</span> <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">N</span> = 1000; <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">length of vectors</span>
  <span style="color: #2fa526;">size_t</span> <span style="color: #6fafff;">size</span> = N * <span style="color: #c48702; font-weight: bold;">sizeof</span><span style="color: #64aa0f;">(</span><span style="color: #2fa526;">float</span><span style="color: #64aa0f;">)</span>; <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">size of a vector in memory</span>

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">allocate vectors in host memory</span>
  <span style="color: #2fa526;">float</span>* <span style="color: #6fafff;">host_a</span> = <span style="color: #64aa0f;">(</span><span style="color: #2fa526;">float</span>*<span style="color: #64aa0f;">)</span>malloc<span style="color: #64aa0f;">(</span>size<span style="color: #64aa0f;">)</span>;
  <span style="color: #2fa526;">float</span>* <span style="color: #6fafff;">host_b</span> = <span style="color: #64aa0f;">(</span><span style="color: #2fa526;">float</span>*<span style="color: #64aa0f;">)</span>malloc<span style="color: #64aa0f;">(</span>size<span style="color: #64aa0f;">)</span>;
  <span style="color: #2fa526;">float</span>* <span style="color: #6fafff;">host_c</span> = <span style="color: #64aa0f;">(</span><span style="color: #2fa526;">float</span>*<span style="color: #64aa0f;">)</span>malloc<span style="color: #64aa0f;">(</span>size<span style="color: #64aa0f;">)</span>; <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">output vector a+b=c</span>

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">initialize host input vectors</span>
  <span style="color: #c48702; font-weight: bold;">for</span> <span style="color: #64aa0f;">(</span><span style="color: #2fa526;">int</span> <span style="color: #6fafff;">i</span> = 0; i &lt; N; i++<span style="color: #64aa0f;">)</span> <span style="color: #64aa0f;">{</span>
    host_a<span style="color: #ef656a;">[</span>i<span style="color: #ef656a;">]</span> = 1.0f;
    host_b<span style="color: #ef656a;">[</span>i<span style="color: #ef656a;">]</span> = 2.0f;
  <span style="color: #64aa0f;">}</span>

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">allocate (global) linear memory for vectors on the device (gpu)</span>
  <span style="color: #2fa526;">float</span>* <span style="color: #6fafff;">device_a</span>;
  <span style="color: #2fa526;">float</span>* <span style="color: #6fafff;">device_b</span>;
  <span style="color: #2fa526;">float</span>* <span style="color: #6fafff;">device_c</span>;
  cudaMalloc<span style="color: #64aa0f;">(</span>&amp;device_a, size<span style="color: #64aa0f;">)</span>;
  cudaMalloc<span style="color: #64aa0f;">(</span>&amp;device_b, size<span style="color: #64aa0f;">)</span>;
  cudaMalloc<span style="color: #64aa0f;">(</span>&amp;device_c, size<span style="color: #64aa0f;">)</span>;

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">copy vectors from host memory to device memory</span>
  cudaMemcpy<span style="color: #64aa0f;">(</span>device_a, host_a, size, cudaMemcpyHostToDevice<span style="color: #64aa0f;">)</span>;
  cudaMemcpy<span style="color: #64aa0f;">(</span>device_b, host_b, size, cudaMemcpyHostToDevice<span style="color: #64aa0f;">)</span>;

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">invoke kernel</span>
  <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">threads_per_block</span> = 256;
  <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">blocks_per_grid</span> =
    <span style="color: #64aa0f;">(</span>N + threads_per_block - 1<span style="color: #64aa0f;">)</span> / threads_per_block;
  vector_add&lt;&lt;&lt;blocks_per_grid, threads_per_block&gt;&gt;&gt;<span style="color: #64aa0f;">(</span>device_c, device_b, device_a, N<span style="color: #64aa0f;">)</span>;

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">copy result from device memory to host memory</span>
  cudaMemcpy<span style="color: #64aa0f;">(</span>host_c, device_c, size, cudaMemcpyDeviceToHost<span style="color: #64aa0f;">)</span>;

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">free device memory</span>
  cudaFree<span style="color: #64aa0f;">(</span>device_a<span style="color: #64aa0f;">)</span>;
  cudaFree<span style="color: #64aa0f;">(</span>device_b<span style="color: #64aa0f;">)</span>;
  cudaFree<span style="color: #64aa0f;">(</span>device_c<span style="color: #64aa0f;">)</span>;

  <span style="color: #c48702; font-weight: bold;">for</span> <span style="color: #64aa0f;">(</span><span style="color: #2fa526;">int</span> <span style="color: #6fafff;">i</span> = 0; i &lt; N; ++i<span style="color: #64aa0f;">)</span> <span style="color: #64aa0f;">{</span>
    printf<span style="color: #ef656a;">(</span><span style="color: #f06a3f;">"%d: %f\n"</span>, i, host_c<span style="color: #3dbbb0;">[</span>i<span style="color: #3dbbb0;">]</span><span style="color: #ef656a;">)</span>;
  <span style="color: #64aa0f;">}</span>

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">free host memory</span>
  free<span style="color: #64aa0f;">(</span>host_a<span style="color: #64aa0f;">)</span>;
  free<span style="color: #64aa0f;">(</span>host_b<span style="color: #64aa0f;">)</span>;
  free<span style="color: #64aa0f;">(</span>host_c<span style="color: #64aa0f;">)</span>;
<span style="color: #d0730f;">}</span>
</pre>
</div>
<p>
compile with <code>nvcc &lt;name&gt;.cu -o vector_add</code> and run with <code>./vector_add</code>.
</p>

<p>
similar same code but in cl:
</p>
<div class="org-src-container">
<pre class="src src-lisp"><span style="color: #d0730f;">(</span><span style="color: #c48702; font-weight: bold;">defun</span> <span style="color: #3dbbb0;">random-init</span> <span style="color: #64aa0f;">(</span>data n<span style="color: #64aa0f;">)</span>
  <span style="color: #64aa0f;">(</span><span style="color: #c48702; font-weight: bold;">dotimes</span> <span style="color: #ef656a;">(</span>i n<span style="color: #ef656a;">)</span>
    <span style="color: #ef656a;">(</span>setf <span style="color: #3dbbb0;">(</span>cl-cuda:memory-block-aref data i<span style="color: #3dbbb0;">)</span> <span style="color: #3dbbb0;">(</span>random 1.0<span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span><span style="color: #64aa0f;">)</span><span style="color: #d0730f;">)</span>

<span style="color: #d0730f;">(</span>cl-cuda:defkernel vec-add-kernel <span style="color: #64aa0f;">(</span>cl-cuda:void <span style="color: #ef656a;">(</span><span style="color: #3dbbb0;">(</span>a cl-cuda:float*<span style="color: #3dbbb0;">)</span> <span style="color: #3dbbb0;">(</span>b cl-cuda:float*<span style="color: #3dbbb0;">)</span> <span style="color: #3dbbb0;">(</span>c cl-cuda:float*<span style="color: #3dbbb0;">)</span> <span style="color: #3dbbb0;">(</span>n cl-cuda:int<span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span><span style="color: #64aa0f;">)</span>
  <span style="color: #64aa0f;">(</span><span style="color: #c48702; font-weight: bold;">let</span> <span style="color: #ef656a;">(</span><span style="color: #3dbbb0;">(</span>i <span style="color: #df8f6f;">(</span>+ <span style="color: #379cf6;">(</span>* cl-cuda:block-dim-x cl-cuda:block-idx-x<span style="color: #379cf6;">)</span> cl-cuda:thread-idx-x<span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span>
    <span style="color: #ef656a;">(</span><span style="color: #c48702; font-weight: bold;">if</span> <span style="color: #3dbbb0;">(</span>&lt; i n<span style="color: #3dbbb0;">)</span>
        <span style="color: #3dbbb0;">(</span>set <span style="color: #df8f6f;">(</span>aref c i<span style="color: #df8f6f;">)</span>
             <span style="color: #df8f6f;">(</span>+ <span style="color: #379cf6;">(</span>aref a i<span style="color: #379cf6;">)</span> <span style="color: #379cf6;">(</span>aref b i<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span><span style="color: #64aa0f;">)</span><span style="color: #d0730f;">)</span>

<span style="color: #d0730f;">(</span><span style="color: #c48702; font-weight: bold;">defun</span> <span style="color: #3dbbb0;">main</span> <span style="color: #64aa0f;">()</span>
  <span style="color: #64aa0f;">(</span><span style="color: #c48702; font-weight: bold;">let*</span> <span style="color: #ef656a;">(</span><span style="color: #3dbbb0;">(</span>dev-id 0<span style="color: #3dbbb0;">)</span>
         <span style="color: #3dbbb0;">(</span>n 1024<span style="color: #3dbbb0;">)</span>
         <span style="color: #3dbbb0;">(</span>threads-per-block 256<span style="color: #3dbbb0;">)</span>
         <span style="color: #3dbbb0;">(</span>blocks-per-grid <span style="color: #df8f6f;">(</span>/ n threads-per-block<span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span>
    <span style="color: #ef656a;">(</span>cl-cuda:with-cuda <span style="color: #3dbbb0;">(</span>dev-id<span style="color: #3dbbb0;">)</span>
      <span style="color: #3dbbb0;">(</span>cl-cuda:with-memory-blocks <span style="color: #df8f6f;">(</span><span style="color: #379cf6;">(</span>a 'float n<span style="color: #379cf6;">)</span> <span style="color: #cf9f7f; font-style: italic;">;; </span><span style="color: #cf9f7f; font-style: italic;">allocate (global) linear memory on both host+device</span>
                                   <span style="color: #379cf6;">(</span>b 'float n<span style="color: #379cf6;">)</span>
                                   <span style="color: #379cf6;">(</span>c 'float n<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span>random-init a n<span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span>random-init b n<span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span>cl-cuda:sync-memory-block a <span style="color: #ff7a7f; font-weight: bold;">:host-to-device</span><span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span>cl-cuda:sync-memory-block b <span style="color: #ff7a7f; font-weight: bold;">:host-to-device</span><span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span>vec-add-kernel a b c n
                        <span style="color: #ff7a7f; font-weight: bold;">:grid-dim</span> <span style="color: #379cf6;">(</span>list blocks-per-grid 1 1<span style="color: #379cf6;">)</span>
                        <span style="color: #ff7a7f; font-weight: bold;">:block-dim</span> <span style="color: #379cf6;">(</span>list threads-per-block 1 1<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span>cl-cuda:sync-memory-block c <span style="color: #ff7a7f; font-weight: bold;">:device-to-host</span><span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span><span style="color: #c48702; font-weight: bold;">loop</span> for i from 0 below 1024
              do <span style="color: #379cf6;">(</span>print <span style="color: #ff7a7f;">(</span>cl-cuda:memory-block-aref c i<span style="color: #ff7a7f;">)</span><span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span><span style="color: #64aa0f;">)</span><span style="color: #d0730f;">)</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orgcdcc8fe" class="outline-3">
<h3 id="orgcdcc8fe">cuda matrix multiplication</h3>
<div class="outline-text-3" id="text-orgcdcc8fe">
<p>
mostly taken from <a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html">https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html</a> and <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model</a>.
</p>

<p>
before implementing matrix multiplication for the <b>gpu</b> we need to consider some things.
</p>

<p>
the product of two matrices <img src="ltx/7bfaa93f10a.svg" alt="\(A_{M \times K},B_{K \times N}\)" style="height: 0.9967em; vertical-align: -0.2779em; display: inline-block" class="org-latex org-latex-inline" /> results in a matrix <img src="ltx/12cd4179641.svg" alt="\(C_{M \times N}\)" style="height: 0.9967em; vertical-align: -0.2779em; display: inline-block" class="org-latex org-latex-inline" /> which contains <img src="ltx/aa99bb3e3b6.svg" alt="\(M \cdot N\)" style="height: 0.7680em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> values, each of which is a dot product of K-element vectors. thus a total of <img src="ltx/0a4381989ac.svg" alt="\(M \cdot N \cdot K\)" style="height: 0.7680em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> fused multiply-adds are needed to compute the product. each FMA is 2 operations, a (scalar) multiplication and an addition, so a total of <img src="ltx/6443b310d51.svg" alt="\(2 \cdot M \cdot N \cdot K\)" style="height: 0.7680em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> flops are required, if we were to consider gemm, the parameters <img src="ltx/3c7d77d7b98.svg" alt="\(\alpha,\beta\)" style="height: 0.9695em; vertical-align: -0.2397em; display: inline-block" class="org-latex org-latex-inline" /> would also play a role in the number of flops required, but the effect these scalars have can be negligable for sufficiently large matrices.
</p>

<p>
in the example of vector addition, we used <b>global memory</b> (using <code>cudaMalloc</code> in C and <code>with-memory-block</code> in CL), which is (very) slow compared to <b>shared memory</b> which is allocated per <b>thread block</b>. we also did one <b>floating point operation</b> per memory access, so with as many flops as we have you can already see how slow we are going. to speed this up we're gonna use shared memory so that threads in a thread block can collaborate in computing a submatrix of the output matrix and to reduce calls to global memory.
</p>

<p>
first, an implementation without shared memory:
</p>
<div class="org-src-container">
<pre class="src src-C"><span style="color: #d570af;">#include</span> <span style="color: #d0730f;">&lt;</span><span style="color: #f06a3f;">stdio.h</span><span style="color: #d0730f;">&gt;</span>
<span style="color: #d570af;">#include</span> <span style="color: #d0730f;">&lt;</span><span style="color: #f06a3f;">stdlib.h</span><span style="color: #d0730f;">&gt;</span>

<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">matrices are stored in row-major order:</span>
<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">M(row, col) = *(M.elements + row * M.width + col)</span>
<span style="color: #c48702; font-weight: bold;">typedef</span> <span style="color: #c48702; font-weight: bold;">struct</span> <span style="color: #d0730f;">{</span>
  <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">width</span>;
  <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">height</span>;
  <span style="color: #2fa526;">float</span>* <span style="color: #6fafff;">elements</span>;
<span style="color: #d0730f;">}</span> <span style="color: #2fa526;">Matrix</span>;

<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">thread block size</span>
<span style="color: #d570af;">#define</span> <span style="color: #6fafff;">BLOCK_SIZE</span> 16

<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">forward declaration of the matrix multiplication kernel</span>
__global__ <span style="color: #2fa526;">void</span> <span style="color: #3dbbb0;">MatMulKernel</span><span style="color: #d0730f;">(</span><span style="color: #c48702; font-weight: bold;">const</span> <span style="color: #6fafff;">Matrix</span>, <span style="color: #c48702; font-weight: bold;">const</span> <span style="color: #6fafff;">Matrix</span>, <span style="color: #2fa526;">Matrix</span><span style="color: #d0730f;">)</span>;

<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">matrix multiplication - host code</span>
<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">matrix dimensions are assumed to be multiples of BLOCK_SIZE</span>
<span style="color: #2fa526;">void</span> <span style="color: #3dbbb0;">MatMul</span><span style="color: #d0730f;">(</span><span style="color: #c48702; font-weight: bold;">const</span> <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">A</span>, <span style="color: #c48702; font-weight: bold;">const</span> <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">B</span>, <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">C</span><span style="color: #d0730f;">)</span>
<span style="color: #d0730f;">{</span>
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">load A and B to device memory</span>
  <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">d_A</span>;
  d_A.width = A.width; d_A.height = A.height;
  <span style="color: #2fa526;">size_t</span> <span style="color: #6fafff;">size</span> = A.width * A.height * <span style="color: #c48702; font-weight: bold;">sizeof</span><span style="color: #64aa0f;">(</span><span style="color: #2fa526;">float</span><span style="color: #64aa0f;">)</span>;
  cudaMalloc<span style="color: #64aa0f;">(</span>&amp;d_A.elements, size<span style="color: #64aa0f;">)</span>;
  cudaMemcpy<span style="color: #64aa0f;">(</span>d_A.elements, A.elements, size,
             cudaMemcpyHostToDevice<span style="color: #64aa0f;">)</span>;
  <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">d_B</span>;
  d_B.width = B.width; d_B.height = B.height;
  size = B.width * B.height * <span style="color: #c48702; font-weight: bold;">sizeof</span><span style="color: #64aa0f;">(</span><span style="color: #2fa526;">float</span><span style="color: #64aa0f;">)</span>;
  cudaMalloc<span style="color: #64aa0f;">(</span>&amp;d_B.elements, size<span style="color: #64aa0f;">)</span>;
  cudaMemcpy<span style="color: #64aa0f;">(</span>d_B.elements, B.elements, size,
             cudaMemcpyHostToDevice<span style="color: #64aa0f;">)</span>;

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">allocate C in device memory</span>
  <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">d_C</span>;
  d_C.width = C.width; d_C.height = C.height;
  size = C.width * C.height * <span style="color: #c48702; font-weight: bold;">sizeof</span><span style="color: #64aa0f;">(</span><span style="color: #2fa526;">float</span><span style="color: #64aa0f;">)</span>;
  cudaMalloc<span style="color: #64aa0f;">(</span>&amp;d_C.elements, size<span style="color: #64aa0f;">)</span>;

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">invoke kernel</span>
  <span style="color: #2fa526;">dim3</span> <span style="color: #3dbbb0;">dimBlock</span><span style="color: #64aa0f;">(</span>BLOCK_SIZE, BLOCK_SIZE<span style="color: #64aa0f;">)</span>;
  <span style="color: #2fa526;">dim3</span> <span style="color: #3dbbb0;">dimGrid</span><span style="color: #64aa0f;">(</span>B.width / dimBlock.x, A.height / dimBlock.y<span style="color: #64aa0f;">)</span>;
  MatMulKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;<span style="color: #64aa0f;">(</span>d_A, d_B, d_C<span style="color: #64aa0f;">)</span>;

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">read C from device memory</span>
  cudaMemcpy<span style="color: #64aa0f;">(</span>C.elements, d_C.elements, size,
             cudaMemcpyDeviceToHost<span style="color: #64aa0f;">)</span>;

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">free device memory</span>
  cudaFree<span style="color: #64aa0f;">(</span>d_A.elements<span style="color: #64aa0f;">)</span>;
  cudaFree<span style="color: #64aa0f;">(</span>d_B.elements<span style="color: #64aa0f;">)</span>;
  cudaFree<span style="color: #64aa0f;">(</span>d_C.elements<span style="color: #64aa0f;">)</span>;
<span style="color: #d0730f;">}</span>

<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">matrix multiplication kernel called by MatMul()</span>
__global__ <span style="color: #2fa526;">void</span> <span style="color: #3dbbb0;">MatMulKernel</span><span style="color: #d0730f;">(</span><span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">A</span>, <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">B</span>, <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">C</span><span style="color: #d0730f;">)</span>
<span style="color: #d0730f;">{</span>
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">each thread computes one element of C</span>
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">by accumulating results into Cvalue</span>
  <span style="color: #2fa526;">float</span> <span style="color: #6fafff;">Cvalue</span> = 0;
  <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">row</span> = blockIdx.y * blockDim.y + threadIdx.y;
  <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">col</span> = blockIdx.x * blockDim.x + threadIdx.x;
  <span style="color: #c48702; font-weight: bold;">for</span> <span style="color: #64aa0f;">(</span><span style="color: #2fa526;">int</span> <span style="color: #6fafff;">e</span> = 0; e &lt; A.width; ++e<span style="color: #64aa0f;">)</span>
    Cvalue += A.elements<span style="color: #64aa0f;">[</span>row * A.width + e<span style="color: #64aa0f;">]</span>
      * B.elements<span style="color: #64aa0f;">[</span>e * B.width + col<span style="color: #64aa0f;">]</span>;
  C.elements<span style="color: #64aa0f;">[</span>row * C.width + col<span style="color: #64aa0f;">]</span> = Cvalue;
<span style="color: #d0730f;">}</span>

<span style="color: #2fa526;">int</span> <span style="color: #3dbbb0;">main</span><span style="color: #d0730f;">()</span> <span style="color: #d0730f;">{</span>
  <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">a</span>,<span style="color: #6fafff;">b</span>,<span style="color: #6fafff;">c</span>;
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">width*height should be a multiple of 16</span>
  <span style="color: #2fa526;">float</span> <span style="color: #6fafff;">elements</span><span style="color: #64aa0f;">[</span>16*16<span style="color: #64aa0f;">]</span>;
  c.width = c.height = a.width = a.height = b.width = b.height = 16;
  a.elements = b.elements = elements;
  c.elements = <span style="color: #64aa0f;">(</span><span style="color: #2fa526;">float</span>*<span style="color: #64aa0f;">)</span>malloc<span style="color: #64aa0f;">(</span><span style="color: #c48702; font-weight: bold;">sizeof</span><span style="color: #ef656a;">(</span><span style="color: #2fa526;">float</span><span style="color: #ef656a;">)</span>*c.width*c.height<span style="color: #64aa0f;">)</span>;
  <span style="color: #c48702; font-weight: bold;">for</span> <span style="color: #64aa0f;">(</span><span style="color: #2fa526;">int</span> <span style="color: #6fafff;">i</span> = 0; i &lt; c.width * c.height; ++i<span style="color: #64aa0f;">)</span> <span style="color: #64aa0f;">{</span>
    a.elements<span style="color: #ef656a;">[</span>i<span style="color: #ef656a;">]</span> = b.elements<span style="color: #ef656a;">[</span>i<span style="color: #ef656a;">]</span> = c.elements<span style="color: #ef656a;">[</span>i<span style="color: #ef656a;">]</span> = i;
  <span style="color: #64aa0f;">}</span>
  MatMul<span style="color: #64aa0f;">(</span>a,b,c<span style="color: #64aa0f;">)</span>;
  <span style="color: #c48702; font-weight: bold;">for</span> <span style="color: #64aa0f;">(</span><span style="color: #2fa526;">int</span> <span style="color: #6fafff;">i</span> = 0; i &lt; c.width * c.height; ++i<span style="color: #64aa0f;">)</span> <span style="color: #64aa0f;">{</span>
    printf<span style="color: #ef656a;">(</span><span style="color: #f06a3f;">"%f\n"</span>, c.elements<span style="color: #3dbbb0;">[</span>i<span style="color: #3dbbb0;">]</span><span style="color: #ef656a;">)</span>;
  <span style="color: #64aa0f;">}</span>
<span style="color: #d0730f;">}</span>
</pre>
</div>
<p>
to divide the work with shared memory we load submatrices into each thread block so that threads of each block can operate on the respective submatrix.
</p>

<p>
<img src="/matrix-multiplication-with-shared-memory.png" />
the threads cooperate in loading from memory the values of the input matrices to minimize access to global memory, different thread blocks load different portions of the input matrices, the portions they load are exactly the part of the matrices needed to find the values of the submatrix of C they operate on.
</p>

<p>
the best way to understand this is to read the code:
</p>
<div class="org-src-container">
<pre class="src src-C"><span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">matrices are stored in row-major order:</span>
<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">M(row, col) = *(M.elements + row * M.stride + col)</span>
<span style="color: #c48702; font-weight: bold;">typedef</span> <span style="color: #c48702; font-weight: bold;">struct</span> <span style="color: #d0730f;">{</span>
  <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">width</span>;
  <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">height</span>;
  <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">stride</span>;
  <span style="color: #2fa526;">float</span>* <span style="color: #6fafff;">elements</span>;
<span style="color: #d0730f;">}</span> <span style="color: #2fa526;">Matrix</span>;

<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">get a matrix element</span>
__device__ <span style="color: #2fa526;">float</span> <span style="color: #3dbbb0;">GetElement</span><span style="color: #d0730f;">(</span><span style="color: #c48702; font-weight: bold;">const</span> <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">A</span>, <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">row</span>, <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">col</span><span style="color: #d0730f;">)</span> <span style="color: #d0730f;">{</span>
  <span style="color: #c48702; font-weight: bold;">return</span> A.elements<span style="color: #64aa0f;">[</span>row * A.stride + col<span style="color: #64aa0f;">]</span>;
<span style="color: #d0730f;">}</span>

<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">set a matrix element</span>
__device__ <span style="color: #2fa526;">void</span> <span style="color: #3dbbb0;">SetElement</span><span style="color: #d0730f;">(</span><span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">A</span>, <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">row</span>, <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">col</span>, <span style="color: #2fa526;">float</span> <span style="color: #6fafff;">value</span><span style="color: #d0730f;">)</span> <span style="color: #d0730f;">{</span>
  A.elements<span style="color: #64aa0f;">[</span>row * A.stride + col<span style="color: #64aa0f;">]</span> = value;
<span style="color: #d0730f;">}</span>
<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">get the BLOCK_SIZExBLOCK_SIZE sub-matrix Asub of A that is</span>
<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">located col sub-matrices to the right and row sub-matrices down</span>
<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">from the upper-left corner of A</span>
__device__ <span style="color: #2fa526;">Matrix</span> <span style="color: #3dbbb0;">GetSubMatrix</span><span style="color: #d0730f;">(</span><span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">A</span>, <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">row</span>, <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">col</span><span style="color: #d0730f;">)</span> <span style="color: #d0730f;">{</span>
  <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">Asub</span>;
  Asub.width    = BLOCK_SIZE;
  Asub.height   = BLOCK_SIZE;
  Asub.stride   = A.stride;
  Asub.elements = &amp;A.elements<span style="color: #64aa0f;">[</span>A.stride * BLOCK_SIZE * row
                              + BLOCK_SIZE * col<span style="color: #64aa0f;">]</span>;
  <span style="color: #c48702; font-weight: bold;">return</span> Asub;
<span style="color: #d0730f;">}</span>

<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">thread block size</span>
<span style="color: #d570af;">#define</span> <span style="color: #6fafff;">BLOCK_SIZE</span> 16

<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">forward declaration of the matrix multiplication kernel</span>
__global__ <span style="color: #2fa526;">void</span> <span style="color: #3dbbb0;">MatMulKernel</span><span style="color: #d0730f;">(</span><span style="color: #c48702; font-weight: bold;">const</span> <span style="color: #6fafff;">Matrix</span>, <span style="color: #c48702; font-weight: bold;">const</span> <span style="color: #6fafff;">Matrix</span>, <span style="color: #2fa526;">Matrix</span><span style="color: #d0730f;">)</span>;

<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">matrix multiplication - host code</span>
<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">matrix dimensions are assumed to be multiples of BLOCK_SIZE</span>
<span style="color: #2fa526;">void</span> <span style="color: #3dbbb0;">MatMul</span><span style="color: #d0730f;">(</span><span style="color: #c48702; font-weight: bold;">const</span> <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">A</span>, <span style="color: #c48702; font-weight: bold;">const</span> <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">B</span>, <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">C</span><span style="color: #d0730f;">)</span> <span style="color: #d0730f;">{</span>
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">load A and B to device memory</span>
  <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">d_A</span>;
  d_A.width = d_A.stride = A.width; d_A.height = A.height;
  <span style="color: #2fa526;">size_t</span> <span style="color: #6fafff;">size</span> = A.width * A.height * <span style="color: #c48702; font-weight: bold;">sizeof</span><span style="color: #64aa0f;">(</span><span style="color: #2fa526;">float</span><span style="color: #64aa0f;">)</span>;
  cudaMalloc<span style="color: #64aa0f;">(</span>&amp;d_A.elements, size<span style="color: #64aa0f;">)</span>;
  cudaMemcpy<span style="color: #64aa0f;">(</span>d_A.elements, A.elements, size,
             cudaMemcpyHostToDevice<span style="color: #64aa0f;">)</span>;
  <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">d_B</span>;
  d_B.width = d_B.stride = B.width; d_B.height = B.height;
  size = B.width * B.height * <span style="color: #c48702; font-weight: bold;">sizeof</span><span style="color: #64aa0f;">(</span><span style="color: #2fa526;">float</span><span style="color: #64aa0f;">)</span>;
  cudaMalloc<span style="color: #64aa0f;">(</span>&amp;d_B.elements, size<span style="color: #64aa0f;">)</span>;
  cudaMemcpy<span style="color: #64aa0f;">(</span>d_B.elements, B.elements, size,
             cudaMemcpyHostToDevice<span style="color: #64aa0f;">)</span>;

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">allocate C in device memory</span>
  <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">d_C</span>;
  d_C.width = d_C.stride = C.width; d_C.height = C.height;
  size = C.width * C.height * <span style="color: #c48702; font-weight: bold;">sizeof</span><span style="color: #64aa0f;">(</span><span style="color: #2fa526;">float</span><span style="color: #64aa0f;">)</span>;
  cudaMalloc<span style="color: #64aa0f;">(</span>&amp;d_C.elements, size<span style="color: #64aa0f;">)</span>;

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">invoke kernel</span>
  <span style="color: #2fa526;">dim3</span> <span style="color: #3dbbb0;">dimBlock</span><span style="color: #64aa0f;">(</span>BLOCK_SIZE, BLOCK_SIZE<span style="color: #64aa0f;">)</span>;
  <span style="color: #2fa526;">dim3</span> <span style="color: #3dbbb0;">dimGrid</span><span style="color: #64aa0f;">(</span>B.width / dimBlock.x, A.height / dimBlock.y<span style="color: #64aa0f;">)</span>;
  MatMulKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;<span style="color: #64aa0f;">(</span>d_A, d_B, d_C<span style="color: #64aa0f;">)</span>;

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">read C from device memory</span>
  cudaMemcpy<span style="color: #64aa0f;">(</span>C.elements, d_C.elements, size,
             cudaMemcpyDeviceToHost<span style="color: #64aa0f;">)</span>;

  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">free device memory</span>
  cudaFree<span style="color: #64aa0f;">(</span>d_A.elements<span style="color: #64aa0f;">)</span>;
  cudaFree<span style="color: #64aa0f;">(</span>d_B.elements<span style="color: #64aa0f;">)</span>;
  cudaFree<span style="color: #64aa0f;">(</span>d_C.elements<span style="color: #64aa0f;">)</span>;
<span style="color: #d0730f;">}</span>

<span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">matrix multiplication kernel called by MatMul()</span>
__global__ <span style="color: #2fa526;">void</span> <span style="color: #3dbbb0;">MatMulKernel</span><span style="color: #d0730f;">(</span><span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">A</span>, <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">B</span>, <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">C</span><span style="color: #d0730f;">)</span> <span style="color: #d0730f;">{</span>
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">block row and column</span>
  <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">blockRow</span> = blockIdx.y;
  <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">blockCol</span> = blockIdx.x;
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">each thread block computes one sub-matrix Csub of C</span>
  <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">Csub</span> = GetSubMatrix<span style="color: #64aa0f;">(</span>C, blockRow, blockCol<span style="color: #64aa0f;">)</span>;
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">each thread computes one element of Csub</span>
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">by accumulating results into Cvalue</span>
  <span style="color: #2fa526;">float</span> <span style="color: #6fafff;">Cvalue</span> = 0;
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">thread row and column within Csub</span>
  <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">row</span> = threadIdx.y;
  <span style="color: #2fa526;">int</span> <span style="color: #6fafff;">col</span> = threadIdx.x;
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">loop over all the sub-matrices of A and B that are</span>
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">required to compute Csub</span>
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">multiply each pair of sub-matrices together</span>
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">and accumulate the results</span>
  <span style="color: #c48702; font-weight: bold;">for</span> <span style="color: #64aa0f;">(</span><span style="color: #2fa526;">int</span> <span style="color: #6fafff;">m</span> = 0; m &lt; <span style="color: #ef656a;">(</span>A.width / BLOCK_SIZE<span style="color: #ef656a;">)</span>; ++m<span style="color: #64aa0f;">)</span> <span style="color: #64aa0f;">{</span>
    <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">get sub-matrix Asub of A</span>
    <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">Asub</span> = GetSubMatrix<span style="color: #ef656a;">(</span>A, blockRow, m<span style="color: #ef656a;">)</span>;
    <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">get sub-matrix Bsub of B</span>
    <span style="color: #2fa526;">Matrix</span> <span style="color: #6fafff;">Bsub</span> = GetSubMatrix<span style="color: #ef656a;">(</span>B, m, blockCol<span style="color: #ef656a;">)</span>;
    <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">shared memory (static variables) used to store Asub and Bsub respectively</span>
    __shared__ <span style="color: #2fa526;">float</span> <span style="color: #6fafff;">As</span><span style="color: #ef656a;">[</span>BLOCK_SIZE<span style="color: #ef656a;">][</span>BLOCK_SIZE<span style="color: #ef656a;">]</span>;
    __shared__ <span style="color: #2fa526;">float</span> <span style="color: #6fafff;">Bs</span><span style="color: #ef656a;">[</span>BLOCK_SIZE<span style="color: #ef656a;">][</span>BLOCK_SIZE<span style="color: #ef656a;">]</span>;
    <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">load Asub and Bsub from device memory to shared memory</span>
    <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">each thread loads one element of each sub-matrix</span>
    As<span style="color: #ef656a;">[</span>row<span style="color: #ef656a;">][</span>col<span style="color: #ef656a;">]</span> = GetElement<span style="color: #ef656a;">(</span>Asub, row, col<span style="color: #ef656a;">)</span>;
    Bs<span style="color: #ef656a;">[</span>row<span style="color: #ef656a;">][</span>col<span style="color: #ef656a;">]</span> = GetElement<span style="color: #ef656a;">(</span>Bsub, row, col<span style="color: #ef656a;">)</span>;
    <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">synchronize to make sure the sub-matrices are loaded</span>
    <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">before starting the computation</span>
    __syncthreads<span style="color: #ef656a;">()</span>;
    <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">multiply Asub and Bsub together</span>
    <span style="color: #c48702; font-weight: bold;">for</span> <span style="color: #ef656a;">(</span><span style="color: #2fa526;">int</span> <span style="color: #6fafff;">e</span> = 0; e &lt; BLOCK_SIZE; ++e<span style="color: #ef656a;">)</span>
      Cvalue += As<span style="color: #ef656a;">[</span>row<span style="color: #ef656a;">][</span>e<span style="color: #ef656a;">]</span> * Bs<span style="color: #ef656a;">[</span>e<span style="color: #ef656a;">][</span>col<span style="color: #ef656a;">]</span>;
    <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">synchronize to make sure that the preceding</span>
    <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">computation is done before loading two new</span>
    <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">sub-matrices of A and B in the next iteration</span>
    __syncthreads<span style="color: #ef656a;">()</span>;
  <span style="color: #64aa0f;">}</span>
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">write Csub to device memory</span>
  <span style="color: #cf9f7f; font-style: italic;">// </span><span style="color: #cf9f7f; font-style: italic;">each thread writes one element</span>
  SetElement<span style="color: #64aa0f;">(</span>Csub, row, col, Cvalue<span style="color: #64aa0f;">)</span>;
<span style="color: #d0730f;">}</span>
</pre>
</div>
<p>
some points to consider before implementating in lisp:
</p>
<ol class="org-ol">
<li>we cant pass structs like in C, unfortunately, we can only allocate linear memory of basic types</li>
<li>for matrices on cpu, i used cl's arrays, for matrices on gpu, im gonna use just C pointers, like in the C example</li>
<li>common lisp implementation will be more complete (i cba doing the same for C), e.g. no assumptions on matrix sizes</li>
<li>loops in kernels can only be done using the <code>do</code> statement</li>
</ol>
<div class="org-src-container">
<pre class="src src-lisp"><span style="color: #d0730f;">(</span><span style="color: #c48702; font-weight: bold;">defun</span> <span style="color: #3dbbb0;">random-init</span> <span style="color: #64aa0f;">(</span>data n<span style="color: #64aa0f;">)</span>
  <span style="color: #64aa0f;">(</span><span style="color: #c48702; font-weight: bold;">dotimes</span> <span style="color: #ef656a;">(</span>i n<span style="color: #ef656a;">)</span>
    <span style="color: #ef656a;">(</span>setf <span style="color: #3dbbb0;">(</span>cl-cuda:memory-block-aref data i<span style="color: #3dbbb0;">)</span> <span style="color: #3dbbb0;">(</span>random 1.0<span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span><span style="color: #64aa0f;">)</span><span style="color: #d0730f;">)</span>

<span style="color: #d0730f;">(</span>cl-cuda:defkernel matrix-mul-kernel <span style="color: #64aa0f;">(</span>cl-cuda:void <span style="color: #ef656a;">(</span><span style="color: #3dbbb0;">(</span>arr1      cl-cuda:float*<span style="color: #3dbbb0;">)</span>
                                                    <span style="color: #3dbbb0;">(</span>arr2      cl-cuda:float*<span style="color: #3dbbb0;">)</span>
                                                    <span style="color: #3dbbb0;">(</span>out       cl-cuda:float*<span style="color: #3dbbb0;">)</span>
                                                    <span style="color: #3dbbb0;">(</span>arr1-rows cl-cuda:int<span style="color: #3dbbb0;">)</span>
                                                    <span style="color: #3dbbb0;">(</span>arr2-cols cl-cuda:int<span style="color: #3dbbb0;">)</span>
                                                    <span style="color: #3dbbb0;">(</span>arr2-rows cl-cuda:int<span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span><span style="color: #64aa0f;">)</span>
  <span style="color: #64aa0f;">(</span><span style="color: #c48702; font-weight: bold;">let*</span> <span style="color: #ef656a;">(</span><span style="color: #3dbbb0;">(</span>block-row cl-cuda:block-idx-y<span style="color: #3dbbb0;">)</span> <span style="color: #cf9f7f; font-style: italic;">;; </span><span style="color: #cf9f7f; font-style: italic;">location of current thread block in the grid</span>
         <span style="color: #3dbbb0;">(</span>block-col cl-cuda:block-idx-x<span style="color: #3dbbb0;">)</span>
         <span style="color: #3dbbb0;">(</span>row cl-cuda:thread-idx-y<span style="color: #3dbbb0;">)</span> <span style="color: #cf9f7f; font-style: italic;">;; </span><span style="color: #cf9f7f; font-style: italic;">cell this thread is responsible for in output matrix</span>
         <span style="color: #3dbbb0;">(</span>col cl-cuda:thread-idx-x<span style="color: #3dbbb0;">)</span>
         <span style="color: #3dbbb0;">(</span>c-value 10.0<span style="color: #3dbbb0;">)</span>
         <span style="color: #3dbbb0;">(</span>arr1-cols arr2-rows<span style="color: #3dbbb0;">)</span>
         <span style="color: #3dbbb0;">(</span>block-size cl-cuda:block-dim-x<span style="color: #3dbbb0;">)</span> <span style="color: #cf9f7f; font-style: italic;">;; </span><span style="color: #cf9f7f; font-style: italic;">16 unless modified</span>
         <span style="color: #3dbbb0;">(</span>out-size arr1-rows<span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span>
    <span style="color: #cf9f7f; font-style: italic;">;; </span><span style="color: #cf9f7f; font-style: italic;">start iterating through input submatrices</span>
    <span style="color: #cf9f7f; font-style: italic;">;; </span><span style="color: #cf9f7f; font-style: italic;">(set c-value (float arr1-cols))</span>
    <span style="color: #ef656a;">(</span><span style="color: #c48702; font-weight: bold;">do</span> <span style="color: #3dbbb0;">(</span><span style="color: #df8f6f;">(</span>i 0 <span style="color: #379cf6;">(</span>+ i 1<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span>
        <span style="color: #3dbbb0;">(</span><span style="color: #df8f6f;">(</span>&gt;= i <span style="color: #379cf6;">(</span>/ arr1-cols block-size<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span>
      <span style="color: #3dbbb0;">(</span>cl-cuda:with-shared-memory <span style="color: #df8f6f;">(</span><span style="color: #379cf6;">(</span>arr1-sub float 16 16<span style="color: #379cf6;">)</span>
                                   <span style="color: #379cf6;">(</span>arr2-sub float 16 16<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span>set <span style="color: #379cf6;">(</span>aref arr1-sub row col<span style="color: #379cf6;">)</span> <span style="color: #379cf6;">(</span>aref arr1 <span style="color: #ff7a7f;">(</span>+ <span style="color: #2fa526;">(</span>* block-row block-size<span style="color: #2fa526;">)</span>
                                                   <span style="color: #2fa526;">(</span>* row block-size<span style="color: #2fa526;">)</span>
                                                   <span style="color: #2fa526;">(</span>* i block-size<span style="color: #2fa526;">)</span>
                                                   col<span style="color: #ff7a7f;">)</span><span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span>set <span style="color: #379cf6;">(</span>aref arr2-sub row col<span style="color: #379cf6;">)</span> <span style="color: #379cf6;">(</span>aref arr2 <span style="color: #ff7a7f;">(</span>+ <span style="color: #2fa526;">(</span>* i block-size<span style="color: #2fa526;">)</span>
                                                   <span style="color: #2fa526;">(</span>* row block-size<span style="color: #2fa526;">)</span>
                                                   <span style="color: #2fa526;">(</span>* block-col block-size<span style="color: #2fa526;">)</span>
                                                   col<span style="color: #ff7a7f;">)</span><span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span>cl-cuda:syncthreads<span style="color: #df8f6f;">)</span> <span style="color: #cf9f7f; font-style: italic;">;; </span><span style="color: #cf9f7f; font-style: italic;">synchronize to make sure submatrices are loaded</span>
        <span style="color: #df8f6f;">(</span><span style="color: #c48702; font-weight: bold;">do</span> <span style="color: #379cf6;">(</span><span style="color: #ff7a7f;">(</span>j 0 <span style="color: #2fa526;">(</span>+ j 1<span style="color: #2fa526;">)</span><span style="color: #ff7a7f;">)</span><span style="color: #379cf6;">)</span>
            <span style="color: #379cf6;">(</span><span style="color: #ff7a7f;">(</span>&gt;= j block-size<span style="color: #ff7a7f;">)</span><span style="color: #379cf6;">)</span>
          <span style="color: #379cf6;">(</span>set c-value <span style="color: #ff7a7f;">(</span>+ c-value <span style="color: #2fa526;">(</span>* <span style="color: #c48702;">(</span>aref arr1-sub row j<span style="color: #c48702;">)</span> <span style="color: #c48702;">(</span>aref arr2-sub j col<span style="color: #c48702;">)</span><span style="color: #2fa526;">)</span><span style="color: #ff7a7f;">)</span><span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span>
        <span style="color: #cf9f7f; font-style: italic;">;; </span><span style="color: #cf9f7f; font-style: italic;">synchronize to make sure that the preceding</span>
        <span style="color: #cf9f7f; font-style: italic;">;; </span><span style="color: #cf9f7f; font-style: italic;">computation is done before loading two new</span>
        <span style="color: #cf9f7f; font-style: italic;">;; </span><span style="color: #cf9f7f; font-style: italic;">sub-matrices of A and B in the next iteration</span>
        <span style="color: #df8f6f;">(</span>cl-cuda:syncthreads<span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span>
    <span style="color: #ef656a;">(</span>set <span style="color: #3dbbb0;">(</span>aref out <span style="color: #df8f6f;">(</span>+ <span style="color: #379cf6;">(</span>* block-row block-size<span style="color: #379cf6;">)</span>
                      <span style="color: #379cf6;">(</span>* block-col block-size<span style="color: #379cf6;">)</span>
                      <span style="color: #379cf6;">(</span>* row block-size<span style="color: #379cf6;">)</span>
                      col<span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span> <span style="color: #c48702;">c-value</span><span style="color: #ef656a;">)</span><span style="color: #64aa0f;">)</span><span style="color: #d0730f;">)</span>

<span style="color: #d0730f;">(</span><span style="color: #c48702; font-weight: bold;">defun</span> <span style="color: #3dbbb0;">cuda-matrix-mul</span> <span style="color: #64aa0f;">(</span>arr1 arr2<span style="color: #64aa0f;">)</span>
  <span style="color: #64aa0f;">(</span><span style="color: #c48702; font-weight: bold;">let*</span> <span style="color: #ef656a;">(</span><span style="color: #3dbbb0;">(</span>out-size <span style="color: #df8f6f;">(</span>* <span style="color: #379cf6;">(</span>array-rows arr1<span style="color: #379cf6;">)</span> <span style="color: #379cf6;">(</span>array-cols arr2<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span>
         <span style="color: #3dbbb0;">(</span>dev-id 0<span style="color: #3dbbb0;">)</span>
         <span style="color: #3dbbb0;">(</span>block-size 16<span style="color: #3dbbb0;">)</span> <span style="color: #cf9f7f; font-style: italic;">;; </span><span style="color: #cf9f7f; font-style: italic;">actual size is squared, this is more of a "length"</span>
         <span style="color: #3dbbb0;">(</span>threads-per-block <span style="color: #df8f6f;">(</span>expt block-size 2<span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span>
         <span style="color: #3dbbb0;">(</span>blocks-per-grid <span style="color: #df8f6f;">(</span>/ out-size threads-per-block<span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span>
    <span style="color: #ef656a;">(</span>cl-cuda:with-cuda <span style="color: #3dbbb0;">(</span>dev-id<span style="color: #3dbbb0;">)</span>
      <span style="color: #3dbbb0;">(</span>cl-cuda:with-memory-blocks <span style="color: #df8f6f;">(</span><span style="color: #379cf6;">(</span>c-arr1 'float <span style="color: #ff7a7f;">(</span>array-size arr1<span style="color: #ff7a7f;">)</span><span style="color: #379cf6;">)</span>
                                   <span style="color: #379cf6;">(</span>c-arr2 'float <span style="color: #ff7a7f;">(</span>array-size arr2<span style="color: #ff7a7f;">)</span><span style="color: #379cf6;">)</span>
                                   <span style="color: #379cf6;">(</span>c-out 'float out-size<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span>random-init c-arr1 <span style="color: #379cf6;">(</span>array-size arr1<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span>random-init c-arr2 <span style="color: #379cf6;">(</span>array-size arr2<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span>cl-cuda:sync-memory-block c-arr1 <span style="color: #ff7a7f; font-weight: bold;">:host-to-device</span><span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span>cl-cuda:sync-memory-block c-arr2 <span style="color: #ff7a7f; font-weight: bold;">:host-to-device</span><span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span>matrix-mul-kernel
         c-arr1 c-arr2 c-out
         <span style="color: #379cf6;">(</span>array-rows arr1<span style="color: #379cf6;">)</span> <span style="color: #379cf6;">(</span>array-cols arr2<span style="color: #379cf6;">)</span> <span style="color: #379cf6;">(</span>array-rows arr2<span style="color: #379cf6;">)</span>
         <span style="color: #ff7a7f; font-weight: bold;">:grid-dim</span> <span style="color: #379cf6;">(</span>list blocks-per-grid 1 1<span style="color: #379cf6;">)</span>
         <span style="color: #ff7a7f; font-weight: bold;">:block-dim</span> <span style="color: #379cf6;">(</span>list block-size block-size 1<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span>
        <span style="color: #df8f6f;">(</span>cl-cuda:sync-memory-block c-out <span style="color: #ff7a7f; font-weight: bold;">:device-to-host</span><span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span><span style="color: #64aa0f;">)</span><span style="color: #d0730f;">)</span>

<span style="color: #d0730f;">(</span><span style="color: #c48702; font-weight: bold;">defun</span> <span style="color: #3dbbb0;">example-cuda-matrix-mul</span> <span style="color: #64aa0f;">()</span>
  <span style="color: #64aa0f;">(</span><span style="color: #c48702; font-weight: bold;">let</span> <span style="color: #ef656a;">(</span><span style="color: #3dbbb0;">(</span>size <span style="color: #df8f6f;">(</span>expt 16 3<span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span>
    <span style="color: #ef656a;">(</span><span style="color: #c48702; font-weight: bold;">let</span> <span style="color: #3dbbb0;">(</span><span style="color: #df8f6f;">(</span>mat1 <span style="color: #379cf6;">(</span>make-array <span style="color: #ff7a7f;">(</span>list size size<span style="color: #ff7a7f;">)</span> <span style="color: #ff7a7f; font-weight: bold;">:initial-element</span> 10<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span>
          <span style="color: #df8f6f;">(</span>mat2 <span style="color: #379cf6;">(</span>make-array <span style="color: #ff7a7f;">(</span>list size size<span style="color: #ff7a7f;">)</span> <span style="color: #ff7a7f; font-weight: bold;">:initial-element</span> 20<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span>
      <span style="color: #3dbbb0;">(</span>cuda-matrix-mul mat1 mat2<span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span><span style="color: #64aa0f;">)</span><span style="color: #d0730f;">)</span>
</pre>
</div>
<p>
that took some effort, i had to hard-code the block size because as it compiles to C the variable isnt defined as a constant which it has to be because i'd get this error if i were to replace the number 16 with the variable <code>block-size</code>:
</p>
<div class="org-src-container">
<pre class="src src-C">nvcc exits with <span style="color: #2fa526;">code</span>: 1
/tmp/cl-cuda.EjtQeE.cu<span style="color: #d0730f;">(</span>60<span style="color: #d0730f;">)</span>: error: expression must have a constant value
                        <span style="color: #c48702; font-weight: bold;">__attribute__</span><span style="color: #d0730f;">(</span><span style="color: #64aa0f;">(</span>shared<span style="color: #64aa0f;">)</span><span style="color: #d0730f;">)</span> <span style="color: #2fa526;">float</span> <span style="color: #6fafff;">arr1_sub</span><span style="color: #d0730f;">[</span>block_size<span style="color: #d0730f;">][</span>block_size<span style="color: #d0730f;">]</span>;
</pre>
</div>
<p>
kernels are very limtied in terms of functionality, not everything can be used in kernels, almost everything is translated to C.
</p>

<p>
now a small benchmark to test this against my previous cpu solution:
</p>
<div class="org-src-container">
<pre class="src src-lisp"><span style="color: #d0730f;">(</span><span style="color: #c48702; font-weight: bold;">defun</span> <span style="color: #3dbbb0;">example-cuda-matrix-mul</span> <span style="color: #64aa0f;">()</span>
  <span style="color: #64aa0f;">(</span><span style="color: #c48702; font-weight: bold;">let</span> <span style="color: #ef656a;">(</span><span style="color: #3dbbb0;">(</span>size <span style="color: #df8f6f;">(</span>expt 16 3<span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span>
    <span style="color: #ef656a;">(</span><span style="color: #c48702; font-weight: bold;">let</span> <span style="color: #3dbbb0;">(</span><span style="color: #df8f6f;">(</span>mat1 <span style="color: #379cf6;">(</span>make-array <span style="color: #ff7a7f;">(</span>list size size<span style="color: #ff7a7f;">)</span> <span style="color: #ff7a7f; font-weight: bold;">:initial-element</span> 10<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span>
          <span style="color: #df8f6f;">(</span>mat2 <span style="color: #379cf6;">(</span>make-array <span style="color: #ff7a7f;">(</span>list size size<span style="color: #ff7a7f;">)</span> <span style="color: #ff7a7f; font-weight: bold;">:initial-element</span> 20<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span>
      <span style="color: #3dbbb0;">(</span>cuda-matrix-mul mat1 mat2<span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span><span style="color: #64aa0f;">)</span><span style="color: #d0730f;">)</span>

<span style="color: #d0730f;">(</span><span style="color: #c48702; font-weight: bold;">defun</span> <span style="color: #3dbbb0;">example-cpu-matrix-mul</span> <span style="color: #64aa0f;">()</span>
  <span style="color: #64aa0f;">(</span><span style="color: #c48702; font-weight: bold;">let</span> <span style="color: #ef656a;">(</span><span style="color: #3dbbb0;">(</span>size 768<span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span>
    <span style="color: #ef656a;">(</span><span style="color: #c48702; font-weight: bold;">let</span> <span style="color: #3dbbb0;">(</span><span style="color: #df8f6f;">(</span>mat1 <span style="color: #379cf6;">(</span>make-array <span style="color: #ff7a7f;">(</span>list size size<span style="color: #ff7a7f;">)</span> <span style="color: #ff7a7f; font-weight: bold;">:initial-element</span> 10<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span>
          <span style="color: #df8f6f;">(</span>mat2 <span style="color: #379cf6;">(</span>make-array <span style="color: #ff7a7f;">(</span>list size size<span style="color: #ff7a7f;">)</span> <span style="color: #ff7a7f; font-weight: bold;">:initial-element</span> 20<span style="color: #379cf6;">)</span><span style="color: #df8f6f;">)</span><span style="color: #3dbbb0;">)</span>
      <span style="color: #3dbbb0;">(</span>matrix-mul mat1 mat2<span style="color: #3dbbb0;">)</span><span style="color: #ef656a;">)</span><span style="color: #64aa0f;">)</span><span style="color: #d0730f;">)</span>

<span style="color: #d0730f;">(</span>time <span style="color: #64aa0f;">(</span>example-cuda-matrix-mul<span style="color: #64aa0f;">)</span><span style="color: #d0730f;">)</span>
<span style="color: #cf9f7f; font-style: italic;">;; </span><span style="color: #cf9f7f; font-style: italic;">Evaluation took:</span>
<span style="color: #cf9f7f; font-style: italic;">;;   </span><span style="color: #cf9f7f; font-style: italic;">2.776 seconds of real time</span>
<span style="color: #cf9f7f; font-style: italic;">;;   </span><span style="color: #cf9f7f; font-style: italic;">2.771769 seconds of total run time (2.600099 user, 0.171670 system)</span>
<span style="color: #cf9f7f; font-style: italic;">;;   </span><span style="color: #cf9f7f; font-style: italic;">[ Run times consist of 0.032 seconds GC time, and 2.740 seconds non-GC time. ]</span>
<span style="color: #cf9f7f; font-style: italic;">;;   </span><span style="color: #cf9f7f; font-style: italic;">99.86% CPU</span>
<span style="color: #cf9f7f; font-style: italic;">;;   </span><span style="color: #cf9f7f; font-style: italic;">7,192,807,350 processor cycles</span>
<span style="color: #cf9f7f; font-style: italic;">;;   </span><span style="color: #cf9f7f; font-style: italic;">268,435,488 bytes consed</span>

<span style="color: #d0730f;">(</span>time <span style="color: #64aa0f;">(</span>example-cpu-matrix-mul<span style="color: #64aa0f;">)</span><span style="color: #d0730f;">)</span>
<span style="color: #cf9f7f; font-style: italic;">;; </span><span style="color: #cf9f7f; font-style: italic;">Evaluation took:</span>
<span style="color: #cf9f7f; font-style: italic;">;;   </span><span style="color: #cf9f7f; font-style: italic;">6.533 seconds of real time</span>
<span style="color: #cf9f7f; font-style: italic;">;;   </span><span style="color: #cf9f7f; font-style: italic;">6.530840 seconds of total run time (6.530840 user, 0.000000 system)</span>
<span style="color: #cf9f7f; font-style: italic;">;;   </span><span style="color: #cf9f7f; font-style: italic;">99.97% CPU</span>
<span style="color: #cf9f7f; font-style: italic;">;;   </span><span style="color: #cf9f7f; font-style: italic;">16,934,213,216 processor cycles</span>
<span style="color: #cf9f7f; font-style: italic;">;;   </span><span style="color: #cf9f7f; font-style: italic;">14,155,824 bytes consed</span>
</pre>
</div>
<p>
the values dont even scale linearly, the gpu version is much faster, a benchmark figure here would be a good idea but im too lazy to bother with that atm.</p>
</div>
</div>
</div>
</div>
</body>
</html>
