<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="mahmood" />
<meta name="description" content="studying and building neural networks from scratch" />
<meta name="generator" content="Org Mode" />
<title>feedforward neural network</title><!-- lambda icon, frail attempt -->
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%221em%22 font-size=%22100%22 color=%22red%22>λ</text></svg>">
<!-- not-so-awesome awesome font -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

<link rel="stylesheet" href="/main.css">
</head>
<body>
<div id="preamble" class="status">
<div class="navbar">
  <a href='/'>home</a>
  <a href='/blog.html'>blog</a>
  <a href='/archive.html'>archive</a>
  <a href='/about.html'>about</a>
</div><h1 class="main-title">feedforward neural network</h1><span class="desc"></span>
</div>
<div id="content" class="content">
<div class="math-block note" data-blocktype="note" id="org92da588">
<p>
feedforward neural networks is a wider term than discussed here, whats discussed here really is a specific type we call a <b>simple multilayer perceptron</b>, but this serves as an introductory to other types of more complex networks
</p>

</div>
<p>
a <b>feedforward network</b> has connections only in one direction. each node computes a function of its inputs and passes the result to its successors in the network. information flows through the network from the input nodes to the output nodes, and there are no loops.
</p>

<p>
a unit calculates the weighted sum of the inputs from predecessor nodes and then applies a nonlinear function to produce its output. let <img src="ltx/ba9c61d2c49.svg" alt="\(a_j\)" style="height: 0.9012em; vertical-align: -0.4590em; display: inline-block" class="org-latex org-latex-inline" /> denote the output of unit <img src="ltx/53c0c910d3c.svg" alt="\(j\)" style="height: 0.8559em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" /> and let <img src="ltx/6e64afd33cd.svg" alt="\(w_{i,j}\)" style="height: 0.9051em; vertical-align: -0.4590em; display: inline-block" class="org-latex org-latex-inline" /> be the weight attached to the link from unit <img src="ltx/b7a09af24c5.svg" alt="\(i\)" style="height: 0.6746em; vertical-align: -0.0786em; display: inline-block" class="org-latex org-latex-inline" /> to unit <img src="ltx/53c0c910d3c.svg" alt="\(j\)" style="height: 0.8559em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" />; then we have
</p>

<div id="orgf688f72" class="equation-container">
<span class="equation">
<img src="ltx/c3dad5ce876.svg" alt="\begin{equation}
  a_j = g_j\left(\sum_{i} w_{i,j}a_i\right) = g_j(in_j)
\end{equation}
" style="height: 3.1128em; vertical-align: -0.5392em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
where <img src="ltx/fae91e282ef.svg" alt="\(g_j\)" style="height: 0.9022em; vertical-align: -0.4590em; display: inline-block" class="org-latex org-latex-inline" /> is a nonlinear activation function associated with unit <img src="ltx/53c0c910d3c.svg" alt="\(j\)" style="height: 0.8559em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" /> and <img src="ltx/cd98a59ca9a.svg" alt="\(in_j\)" style="height: 1.0550em; vertical-align: -0.4590em; display: inline-block" class="org-latex org-latex-inline" /> is the weighted sum of the inputs to unit <img src="ltx/53c0c910d3c.svg" alt="\(j\)" style="height: 0.8559em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" />.
</p>

<p>
each unit has an extra input from a dummy unit 0 that is fixed to +1 and a weight <img src="ltx/c9988f4765a.svg" alt="\(w_{0,j}\)" style="height: 0.9051em; vertical-align: -0.4590em; display: inline-block" class="org-latex org-latex-inline" /> for that input. this allows the total weighted input <img src="ltx/cd98a59ca9a.svg" alt="\(in_j\)" style="height: 1.0550em; vertical-align: -0.4590em; display: inline-block" class="org-latex org-latex-inline" /> to unit <img src="ltx/53c0c910d3c.svg" alt="\(j\)" style="height: 0.8559em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" /> to be nonzero even when the outputs of the preceding layer are all zero. with this convention, we can write the preceding equation in vector form:
<img src="ltx/9372726f0e2.svg" alt="\[ a_j = g_j(\brm{w}^T\brm{x}) \]" style="height: 1.6842em; display: block" class="org-latex org-latex-block" />
where <img src="ltx/f652ff9ab54.svg" alt="\(\brm{w}\)" style="height: 0.4874em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> is the vector of weights leading into unit <img src="ltx/53c0c910d3c.svg" alt="\(j\)" style="height: 0.8559em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" /> (including <img src="ltx/c9988f4765a.svg" alt="\(w_{0,j}\)" style="height: 0.9051em; vertical-align: -0.4590em; display: inline-block" class="org-latex org-latex-inline" />) and <img src="ltx/1dcc6f283b4.svg" alt="\(\brm{x}\)" style="height: 0.4874em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> is the vector of inputs to unit <img src="ltx/53c0c910d3c.svg" alt="\(j\)" style="height: 0.8559em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" /> (including the +1)
training a neural network consists of modifying the network’s parameters so as to minimize the loss function on the training set. in principle, any kind of optimization algorithm could be used. in practice, modern neural networks are almost always trained with some variant of stochastic gradient descent.
</p>

<p>
here, the goal is to minimize the loss <img src="ltx/09bb45f3c96.svg" alt="\(L(\brm{w})\)" style="height: 0.9627em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" />, where <img src="ltx/f652ff9ab54.svg" alt="\(\brm{w}\)" style="height: 0.4874em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> represents all of the parameters of the network (all the weights). each update step in the gradient descent process looks like this:
<img src="ltx/de16fe3beea.svg" alt="\[ \brm{w} \gets \brm{w} - \alpha\nabla_{\brm{w}}L(\brm{w}) \]" style="height: 1.5164em; display: block" class="org-latex org-latex-block" />
</p>

<p>
where <img src="ltx/7d4b8a169de.svg" alt="\(\alpha\)" style="height: 0.4992em; vertical-align: -0.0551em; display: inline-block" class="org-latex org-latex-inline" /> is the learning rate. for standard gradient descent, the loss function <img src="ltx/910da899660.svg" alt="\(L\)" style="height: 0.7109em; vertical-align: -0.0511em; display: inline-block" class="org-latex org-latex-inline" /> is defined with respect to the entire training set. for stochastic gradient descent, it is defined with respect to a minibatch of <img src="ltx/15162f97f7a.svg" alt="\(m\)" style="height: 0.4992em; vertical-align: -0.0541em; display: inline-block" class="org-latex org-latex-inline" /> examples chosen randomly at each step.
</p>

<p>
the change in a single weight is defined as:
<img src="ltx/9ba6f698268.svg" alt="\[ w \gets w - \alpha \frac{\partial L}{\partial w} \]" style="height: 1.9947em; display: block" class="org-latex org-latex-block" />
given a single training example <img src="ltx/00ac6761568.svg" alt="\((\brm{x},y)\)" style="height: 0.9627em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" />, let <img src="ltx/baa02072cf9.svg" alt="\(\hat y=a_k\)" style="height: 0.9898em; vertical-align: -0.3007em; display: inline-block" class="org-latex org-latex-inline" /> of the output layer. <img src="ltx/1f7f41d3590.svg" alt="\(\hat y\)" style="height: 0.9510em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" /> and <img src="ltx/e4f3ae35206.svg" alt="\(y\)" style="height: 0.7109em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" /> are both constant throughout the back-propagation process (independent of other variables), as we're really just trying to get the output <img src="ltx/1f7f41d3590.svg" alt="\(\hat y\)" style="height: 0.9510em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" /> to be as close to <img src="ltx/e4f3ae35206.svg" alt="\(y\)" style="height: 0.7109em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" /> as possible on future feedforwarding runs.
</p>

<p>
denoting the output layer by <img src="ltx/2bb7fff56f4.svg" alt="\(q\)" style="height: 0.7050em; vertical-align: -0.2579em; display: inline-block" class="org-latex org-latex-inline" />, the output of an aribtrary unit <img src="ltx/bb71ee7d056.svg" alt="\(k\)" style="height: 0.6991em; vertical-align: -0.0609em; display: inline-block" class="org-latex org-latex-inline" /> in the output layer is defined as:
<img src="ltx/ff0b19c09d5.svg" alt="\[ \hat y = g^q(in_k^q) \]" style="height: 1.5746em; display: block" class="org-latex org-latex-block" />
such that a superscript denotes the number of the layer that an expression is associated with.
</p>

<p>
assuming the squared cost function (which we would usually denote by <img src="ltx/3b99baaa22c.svg" alt="\(L_2\)" style="height: 0.9541em; vertical-align: -0.2944em; display: inline-block" class="org-latex org-latex-inline" /> but here im using <img src="ltx/4eef58905b2.svg" alt="\(L_k\)" style="height: 0.9604em; vertical-align: -0.3007em; display: inline-block" class="org-latex org-latex-inline" /> for something else), the output of the cost function for the <img src="ltx/bb71ee7d056.svg" alt="\(k\)" style="height: 0.6991em; vertical-align: -0.0609em; display: inline-block" class="org-latex org-latex-inline" />-th unit in the output layer would be:
<img src="ltx/658981e550e.svg" alt="\[ L_k = (y - \hat y)^2 \]" style="height: 1.5258em; display: block" class="org-latex org-latex-block" />
after the first application of the derivative chain rule to the cost function with respect to the weight that connects an arbitrary unit <img src="ltx/53c0c910d3c.svg" alt="\(j\)" style="height: 0.8559em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" /> in the last hidden layer to an arbitrary unit <img src="ltx/bb71ee7d056.svg" alt="\(k\)" style="height: 0.6991em; vertical-align: -0.0609em; display: inline-block" class="org-latex org-latex-inline" /> in the output layer we get:
<img src="ltx/865a8de6ef1.svg" alt="\[ \frac{\partial L}{\partial w^q_{jk}} = -2(y-\hat y) \frac{\partial {g^q(in_k^q)}}{\partial w^q_{jk}} \]" style="height: 2.8657em; display: block" class="org-latex org-latex-block" />
</p>
<div class="math-block note" data-blocktype="note" id="org24c59fa">
<p>
a weight that is between a layer <img src="ltx/f89624a23f7.svg" alt="\(l-1\)" style="height: 0.7246em; vertical-align: -0.0649em; display: inline-block" class="org-latex org-latex-inline" /> and <img src="ltx/da14e46a009.svg" alt="\(l\)" style="height: 0.7060em; vertical-align: -0.0649em; display: inline-block" class="org-latex org-latex-inline" /> is denoted by <img src="ltx/0a568cd7a47.svg" alt="\(w^l_x\)" style="height: 1.1873em; vertical-align: -0.2991em; display: inline-block" class="org-latex org-latex-inline" /> and not <img src="ltx/93ce8a49b83.svg" alt="\(w^{l-1}_x\)" style="height: 1.2022em; vertical-align: -0.2991em; display: inline-block" class="org-latex org-latex-inline" /> because it actually belongs to the <img src="ltx/da14e46a009.svg" alt="\(l\)" style="height: 0.7060em; vertical-align: -0.0649em; display: inline-block" class="org-latex org-latex-inline" />th layer, even if it comes out of the <img src="ltx/f89624a23f7.svg" alt="\(l-1\)" style="height: 0.7246em; vertical-align: -0.0649em; display: inline-block" class="org-latex org-latex-inline" />th layer, this means that weights with a superscript of 1, e.g. <img src="ltx/d227a94ba5a.svg" alt="\(w^1_x\)" style="height: 1.2022em; vertical-align: -0.2991em; display: inline-block" class="org-latex org-latex-inline" /> arent defined, because the first layer (input layer) has no weights.
</p>

</div>
<p>
applying the chain rule again we get:
<img src="ltx/3fa92a7551a.svg" alt="\[ \frac{\partial L}{\partial w^q_{jk}} = -2(y-\hat y) {g^q}'(in_k^q) \frac{\partial in_k^q}{w^q_{jk}} \]" style="height: 2.8657em; display: block" class="org-latex org-latex-block" />
we apply the chain rule again to get the final gradient formula for <img src="ltx/eb2508f379b.svg" alt="\(w^q_{jk}\)" style="height: 1.3794em; vertical-align: -0.5078em; display: inline-block" class="org-latex org-latex-inline" />:
</p>

<div id="orgaafa6b2" class="equation-container">
<span class="equation">
<img src="ltx/ed7750c29cc.svg" alt="\begin{equation}
  \frac{\partial L}{\partial w^q_{jk}} = \underbrace{-2(y-\hat y){g^q}'(in_k^q)}_{\text{perceived error, } \Delta_k^q} a_j^{q-1}
\end{equation}
" style="height: 3.2616em; vertical-align: -0.5392em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
during backpropagation, a part of a weights gradient, which we define as <b>perceived error</b> or a <b>delta</b>, is back-propagated to preceding layers in the network because it appears in the formulas of the partial derivatives for weights that exist in these previous layers, we denote by <img src="ltx/c23bb008ff8.svg" alt="\(\Delta_k^l\)" style="height: 1.2218em; vertical-align: -0.3336em; display: inline-block" class="org-latex org-latex-inline" /> the perceived error of the <img src="ltx/bb71ee7d056.svg" alt="\(k\)" style="height: 0.6991em; vertical-align: -0.0609em; display: inline-block" class="org-latex org-latex-inline" />-th unit in the <img src="ltx/da14e46a009.svg" alt="\(l\)" style="height: 0.7060em; vertical-align: -0.0649em; display: inline-block" class="org-latex org-latex-inline" />-th layer. in the case of a unit in the output layer, its a simple formula:
<img src="ltx/313d3af8f5e.svg" alt="\[ \Delta_k^q = -2(y-\hat y){g^q}'(in_k^q) \]" style="height: 1.5746em; display: block" class="org-latex org-latex-block" />
which is, as you might've noticed, just a part of the equation <a href="/feedforward_neural_network.html">eq-fnn-w-derivative</a>, the beginning of the expression, <img src="ltx/9cefc284300.svg" alt="\(-2(y-\hat y)\)" style="height: 0.9637em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" />, doesnt change, but as we move backwards through the layers, the pattern in the second part of the equation <img src="ltx/9912f556f2e.svg" alt="\({g^l}'(in_k^l)\)" style="height: 1.3955em; vertical-align: -0.3336em; display: inline-block" class="org-latex org-latex-inline" /> repeats over and over again (demonstrated in <a href="/feedforward_neural_network.html">exa-fnn-1</a>), which is why the concept of a perceived error is quite helpful. with this in mind, the perceived error of a unit <img src="ltx/53c0c910d3c.svg" alt="\(j\)" style="height: 0.8559em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" /> in the last hidden layer <img src="ltx/1ca556b7efe.svg" alt="\(q-1\)" style="height: 0.9176em; vertical-align: -0.2579em; display: inline-block" class="org-latex org-latex-inline" /> would be:
<img src="ltx/c3180993cc9.svg" alt="\[ \Delta_j^{q-1} = \Delta_k^qw^q_{jk}{g^{q-1}}'(in_j^{q-1}) = -2(y-\hat y){g^q}'(in_k^q)w_{jk}^q{g^{q-1}}'(in_j^{q-1}) \]" style="height: 1.7330em; display: block" class="org-latex org-latex-block" />
where <img src="ltx/bb71ee7d056.svg" alt="\(k\)" style="height: 0.6991em; vertical-align: -0.0609em; display: inline-block" class="org-latex org-latex-inline" /> refers to the number of the unit in the succeeding layer that the back-propagated gradient message originated from.
</p>

<p>
and in general form, the perceived error for an arbitrary unit <img src="ltx/bb71ee7d056.svg" alt="\(k\)" style="height: 0.6991em; vertical-align: -0.0609em; display: inline-block" class="org-latex org-latex-inline" /> in an arbitrary hidden layer <img src="ltx/da14e46a009.svg" alt="\(l\)" style="height: 0.7060em; vertical-align: -0.0649em; display: inline-block" class="org-latex org-latex-inline" />, is defined as:
</p>

<div id="orgef3f5cc" class="equation-container">
<span class="equation">
<img src="ltx/cb7585723d9.svg" alt="\begin{equation}
  \Delta_k^l = \Delta_c^{l+1}w^{l+1}_{kc}{g^l}'(in_k^l) 
\end{equation}
" style="height: 1.5746em; vertical-align: -0.3495em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
where <img src="ltx/93b8fb15026.svg" alt="\(c\)" style="height: 0.5002em; vertical-align: -0.0560em; display: inline-block" class="org-latex org-latex-inline" /> is some arbitrary unit in <img src="ltx/f0c56fb1491.svg" alt="\(l+1\)" style="height: 0.8020em; vertical-align: -0.1423em; display: inline-block" class="org-latex org-latex-inline" /> that the message was propagated back from.
</p>

<p>
for a weight that connects the unit <img src="ltx/53c0c910d3c.svg" alt="\(j\)" style="height: 0.8559em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" /> of the hidden layer <img src="ltx/f89624a23f7.svg" alt="\(l-1\)" style="height: 0.7246em; vertical-align: -0.0649em; display: inline-block" class="org-latex org-latex-inline" /> to the unit <img src="ltx/bb71ee7d056.svg" alt="\(k\)" style="height: 0.6991em; vertical-align: -0.0609em; display: inline-block" class="org-latex org-latex-inline" /> of the next hidden layer <img src="ltx/da14e46a009.svg" alt="\(l\)" style="height: 0.7060em; vertical-align: -0.0649em; display: inline-block" class="org-latex org-latex-inline" />, the formula is defined in terms of the perceived error of the unit <img src="ltx/d0956363fa8.svg" alt="\(k^l\)" style="height: 0.9492em; vertical-align: -0.0609em; display: inline-block" class="org-latex org-latex-inline" /> (weight originates from unit <img src="ltx/c506a04f73e.svg" alt="\(j^{l-1}\)" style="height: 1.1621em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" /> and connects to unit <img src="ltx/d0956363fa8.svg" alt="\(k^l\)" style="height: 0.9492em; vertical-align: -0.0609em; display: inline-block" class="org-latex org-latex-inline" />, so it belongs to <img src="ltx/d0956363fa8.svg" alt="\(k^l\)" style="height: 0.9492em; vertical-align: -0.0609em; display: inline-block" class="org-latex org-latex-inline" />):
<img src="ltx/f2114fbc8f9.svg" alt="\[ \frac{\partial L}{\partial w^l_{jk}} = \Delta_k^l a_j^{l-1} \]" style="height: 2.5832em; display: block" class="org-latex org-latex-block" />
the back-propagation process passes messages back along each link in the network. at each node, the incoming messages are collected and new messages are calculated to pass back to the next layer.
</p>

<p>
overall, the process of learning the weights of the network is usually one that exhibits diminishing returns. we run until it is no longer practical to decrease the test error by running longer. usually this does not mean we have reached a global or even a local minimum of the loss function. instead, it means we would have to make an impractically large number of very small steps to continue reducing the cost, or that additional steps would only cause overfitting, or that estimates of the gradient are too inaccurate to make further progress.
</p>

<div class="math-block my_example" data-blocktype="example" id="org25c7fd4">
<p>
citation: (Peter Norvig, Stuart J. Russell, 2020)
consider the following network
</p>
<p>
<img src="/network-graph.png" />
coupling multiple units together into a network creates a complex function that is a composition of the algebraic expressions represented by the individual units. for example, the network above represents a function <img src="ltx/17ce36ad200.svg" alt="\(h_{\brm{w}}(\brm{x})\)" style="height: 0.9794em; vertical-align: -0.2912em; display: inline-block" class="org-latex org-latex-inline" />, parameterized by weights <img src="ltx/f652ff9ab54.svg" alt="\(\brm{w}\)" style="height: 0.4874em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />, that maps a two-element input vector <img src="ltx/1dcc6f283b4.svg" alt="\(\brm{x}\)" style="height: 0.4874em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> to a scalar output value <img src="ltx/1f7f41d3590.svg" alt="\(\hat y\)" style="height: 0.9510em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" />. the internal structure of the function mirrors the structure of the network. for example we can write an expression for the output <img src="ltx/1f7f41d3590.svg" alt="\(\hat y\)" style="height: 0.9510em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" /> as follows:
</p>

<div id="org8fc2602" class="equation-container">
<span class="equation">
<img src="ltx/1a3fcf2a5e3.svg" alt="\begin{align*}
  \hat y &amp;amp;= g_5(in_5) = g_5(w_{0,5} + w_{3,5}a_3 + w_{4,5}a_4)\\
  &amp;amp;= g_5(w_{0,5} + w_{3,5}g_3(in_3) + w_{4,5}g_4(in_4))\\
  &amp;amp;= g_5(w_{0,5} + w_{3,5}g_3(w_{0,3} + w_{1,3}x_1 + w_{2,3}x_2) + w_{4,5}g_4(w_{0,4} + w_{1,4}x_1 + w_{2,4}x_2))
\end{align*}
" style="height: 4.5672em; vertical-align: -0.4020em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
thus, we have the output <img src="ltx/1f7f41d3590.svg" alt="\(\hat y\)" style="height: 0.9510em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" /> expressed as a function <img src="ltx/17ce36ad200.svg" alt="\(h_{\brm{w}}(\brm{x})\)" style="height: 0.9794em; vertical-align: -0.2912em; display: inline-block" class="org-latex org-latex-inline" /> of the inputs and the weights.
</p>

<p>
we generally use <img src="ltx/1be81b70afd.svg" alt="\(\brm{W}\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> to denote a weight matrix; <img src="ltx/2e1315ddee4.svg" alt="\(\brm{W}^{(i)}\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> denotes the weights in the i'th layer, let <img src="ltx/6a45fe55d0b.svg" alt="\(\brm{g}^{(i)}\)" style="height: 1.1858em; vertical-align: -0.2599em; display: inline-block" class="org-latex org-latex-inline" /> denote the activation functions in the i'th layer, then, for example, an entire network of 1 input layer, 1 hidden layer, and an output node can be written as follows:
<img src="ltx/d6fec6a217a.svg" alt="\[ h_{\brm{w}}(\brm{x}) = \brm{g}^{(2)}(\brm{W}^{(2)}\brm{g}^{(1)}(\brm{W}^{(1)}\brm{x})) \]" style="height: 1.5164em; display: block" class="org-latex org-latex-block" />
for the network to learn, we need to gradually adjust the weights to fit the learning data, using the gradient descent algorithm
first we apply the loss function, for simplicity the squared loss function is used here, we will calculate the gradient for the network we <a href="#org8d107bb">proposed</a> with respect to a single training example <img src="ltx/00ac6761568.svg" alt="\((\brm{x},y)\)" style="height: 0.9627em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" />. (for multiple examples, the gradient is just the sum of the gradients for the individual examples.) the network outputs a prediction <img src="ltx/c930c9d01ea.svg" alt="\(\hat y = h_{\brm{w}}(\brm{x})\)" style="height: 0.9804em; vertical-align: -0.2912em; display: inline-block" class="org-latex org-latex-inline" /> and the true value is <img src="ltx/e4f3ae35206.svg" alt="\(y\)" style="height: 0.7109em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" />, so we have
<img src="ltx/a0b27112260.svg" alt="\[ Loss(h_{\brm{w}}) = L_2(y,h_{\brm{x}}(\brm{x})) = ||y-h_{\brm{w}}(\brm{x})||^2 = (y-\hat y)^2 \]" style="height: 1.5196em; display: block" class="org-latex org-latex-block" />
we compute the gradient of the loss with respect to the weights, we use the chain rule, we'll start with the easy case: a weight such as <img src="ltx/e69b03d05cc.svg" alt="\(w_{3,5}\)" style="height: 0.8259em; vertical-align: -0.3798em; display: inline-block" class="org-latex org-latex-inline" /> that is connected to the output unit. we operate directly on the <a href="/feedforward_neural_network.html">network-defining expressions</a>:
</p>

<div class="equation-container">
<span class="equation">
<img src="ltx/b4c0b440c2d.svg" alt="\begin{align*}
  \frac{\partial}{\partial w_{3,5}}Loss(h_{\brm{w}}) &amp;amp;= \frac{\partial}{\partial w_{3,5}}(y-\hat y)^2 = -2(y-\hat y)\frac{\partial \hat y}{\partial w_{3,5}}\\
  &amp;amp;= -2(y-\hat y)\frac{\partial}{\partial w_{3,5}}g_5(in_5) = -2(y-\hat y)g'_5(in_5)\frac{\partial}{\partial w_{3,5}}in_5\\
  &amp;amp;= -2(y-\hat y)g'_5(in_5)\frac{\partial}{\partial w_{3,5}}(w_{0,5}+w_{3,5}a_3+w_{4,5}a_4)\\
  &amp;amp;= -2(y-\hat y)g'_5(in_5)a_3
\end{align*}
" style="height: 8.9249em; vertical-align: -0.4020em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
the slightly more difficult case involves a weight such as <img src="ltx/caf67e30f12.svg" alt="\(w_{1,3}\)" style="height: 0.8259em; vertical-align: -0.3798em; display: inline-block" class="org-latex org-latex-inline" /> that is not directly connected to the output unit. here, we have to apply the chain rule one more time. the first few steps are identical, so we omit them:
</p>

<div class="equation-container">
<span class="equation">
<img src="ltx/82b1f6eb411.svg" alt="\begin{align*}
  \frac{\partial}{\partial w_{1,3}}Loss(h_{\brm{w}}) &amp;amp;= -2(y-\hat y)g'_5(in_5)\frac{\partial}{\partial w_{1,3}}(w_{0,5}+w_{3,5}a_3+w_{4,5}a_4)\\
  &amp;amp;= -2(y-\hat y)g'_5(in_5)w_{3,5}\frac{\partial}{\partial w_{1,3}}a_3\\
  &amp;amp;= -2(y-\hat y)g'_5(in_5)w_{3,5}\frac{\partial}{\partial w_{1,3}}g_3(in_3)\\
  &amp;amp;= -2(y-\hat y)g'_5(in_5)w_{3,5}g'_3(in_3)\frac{\partial}{\partial w_{1,3}}in_3\\
  &amp;amp;= -2(y-\hat y)g'_5(in_5)w_{3,5}g'_3(in_3)\frac{\partial}{\partial w_{1,3}}(w_{0,3}+w_{1,3}x_1+w_{2,3}x_2)\\
  &amp;amp;= -2(y-\hat y)g'_5(in_5)w_{3,5}g'_3(in_3)x_1
\end{align*}
" style="height: 13.9379em; vertical-align: -0.4020em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
here, <img src="ltx/94fcd5e8e5b.svg" alt="\(\Delta_5=2(\hat y-y)g'_5(in_5)\)" style="height: 1.1654em; vertical-align: -0.3320em; display: inline-block" class="org-latex org-latex-inline" /> is the perceived error unit 5, and the gradient with respect to <img src="ltx/e69b03d05cc.svg" alt="\(w_{3,5}\)" style="height: 0.8259em; vertical-align: -0.3798em; display: inline-block" class="org-latex org-latex-inline" /> is just <img src="ltx/fce16bfe11a.svg" alt="\(\Delta_5a_3\)" style="height: 0.9694em; vertical-align: -0.2999em; display: inline-block" class="org-latex org-latex-inline" />. this makes perfect sense: if <img src="ltx/444fc3d7961.svg" alt="\(\Delta_5\)" style="height: 0.9639em; vertical-align: -0.2944em; display: inline-block" class="org-latex org-latex-inline" /> is positive, that means <img src="ltx/1f7f41d3590.svg" alt="\(\hat y\)" style="height: 0.9510em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" /> is too big (<img src="ltx/788aa587dad.svg" alt="\(g'\)" style="height: 1.0933em; vertical-align: -0.2599em; display: inline-block" class="org-latex org-latex-inline" /> is always nonnegative); if <img src="ltx/9b6a7cbd760.svg" alt="\(a_3\)" style="height: 0.7420em; vertical-align: -0.2999em; display: inline-block" class="org-latex org-latex-inline" /> is also positive, then increasing <img src="ltx/e69b03d05cc.svg" alt="\(w_{3,5}\)" style="height: 0.8259em; vertical-align: -0.3798em; display: inline-block" class="org-latex org-latex-inline" /> will only make things worse, whereas if <img src="ltx/9b6a7cbd760.svg" alt="\(a_3\)" style="height: 0.7420em; vertical-align: -0.2999em; display: inline-block" class="org-latex org-latex-inline" /> is negative, then increasing <img src="ltx/e69b03d05cc.svg" alt="\(w_{3,5}\)" style="height: 0.8259em; vertical-align: -0.3798em; display: inline-block" class="org-latex org-latex-inline" /> will reduce the error. the magnitude of <img src="ltx/9b6a7cbd760.svg" alt="\(a_3\)" style="height: 0.7420em; vertical-align: -0.2999em; display: inline-block" class="org-latex org-latex-inline" /> also matters: if <img src="ltx/9b6a7cbd760.svg" alt="\(a_3\)" style="height: 0.7420em; vertical-align: -0.2999em; display: inline-block" class="org-latex org-latex-inline" /> is small for this training example, then<img src="ltx/e69b03d05cc.svg" alt="\(w_{3,5}\)" style="height: 0.8259em; vertical-align: -0.3798em; display: inline-block" class="org-latex org-latex-inline" /> didnt play a major role in producing the error and doesnt need to be changed much.
</p>

<p>
we also know <img src="ltx/c3063af370e.svg" alt="\(\Delta_3=\Delta_5w_{3,5}g'_3(in_3)\)" style="height: 1.2132em; vertical-align: -0.3798em; display: inline-block" class="org-latex org-latex-inline" />, and the gradient for <img src="ltx/caf67e30f12.svg" alt="\(w_{1,3}\)" style="height: 0.8259em; vertical-align: -0.3798em; display: inline-block" class="org-latex org-latex-inline" /> becomes just <img src="ltx/dc906dd446a.svg" alt="\(\Delta_3x_1\)" style="height: 0.9694em; vertical-align: -0.2999em; display: inline-block" class="org-latex org-latex-inline" />. thus, the perceived error at the input to unit 3 is the perceived error at the input to unit 5, multiplied by information along the path from 5 back to 3. this phenomenon is completely general, and gives rise to the term back-propagation for the way that the error at the output is passed back through the network.
</p>

<p>
another important characteristic of these gradient expressions is that they have as factors the local derivatives <img src="ltx/282cfd664c9.svg" alt="\(g'_j(in_j)\)" style="height: 1.2924em; vertical-align: -0.4590em; display: inline-block" class="org-latex org-latex-inline" />. as noted earlier, these derivatives are always nonnegative, but they can be very close to zero (in the case of the sigmoid, softplus, and tanh functions) or exactly zero (in the case of ReLUs), if the inputs from the training example in question happen to put unit <img src="ltx/53c0c910d3c.svg" alt="\(j\)" style="height: 0.8559em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" /> in the flat operating region. if the derivative <img src="ltx/f351be3f529.svg" alt="\(g'_j\)" style="height: 1.2924em; vertical-align: -0.4590em; display: inline-block" class="org-latex org-latex-inline" /> is small or zero, that means that changing the weights leading into unit <img src="ltx/53c0c910d3c.svg" alt="\(j\)" style="height: 0.8559em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" /> will have a negligible effect on its output. as a result, deep networks with many layers may suffer from a vanishing gradient&#x2013;the error signals are extinguished altogether as they are propagated back through the network.
</p>

</div>
<p>
but how do we pass the data into the input layer? and what do the values coming out of the output layer mean? the following paragraphs explain how we <b>encode</b> data to pass it into the input layer, and how we treat the values resulting from the output layer.
</p>

<p>
the encoding of input data is usually straightforward, at least for the case of factored data where each training example contains values for <img src="ltx/ddcbd3a127e.svg" alt="\(n\)" style="height: 0.4982em; vertical-align: -0.0541em; display: inline-block" class="org-latex org-latex-inline" /> input attributes. if the attributes are boolean, we have <img src="ltx/ddcbd3a127e.svg" alt="\(n\)" style="height: 0.4982em; vertical-align: -0.0541em; display: inline-block" class="org-latex org-latex-inline" /> input nodes; usually false is mapped to an input of 0 and true is mapped to 1, although sometimes -1 and +1 are used. numeric attributes, whether integer or real-valued, are typically used as is, although they may be scaled to fit within a fixed range; if the magnitudes for different examples vary enormously, the values can be mapped onto a log scale.
</p>

<p>
images do not quite fit into the category of factored data; although an RGB image of size <img src="ltx/e1922613a90.svg" alt="\(X \times Y\)" style="height: 0.7412em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" /> pixels can be thought of as <img src="ltx/adf0f0b1c49.svg" alt="\(3XY\)" style="height: 0.7246em; vertical-align: -0.0600em; display: inline-block" class="org-latex org-latex-inline" /> integer-valued attributes (typically with values in the range <img src="ltx/3c9fd08f3e1.svg" alt="\(\{0,\dots,255\}\)" style="height: 1.0078em; vertical-align: -0.3001em; display: inline-block" class="org-latex org-latex-inline" />), this would ignore the fact that the RGB triplets belong to the same pixel in the image and the fact that pixel adjacency really matters. of course, we can map adjacent pixels onto adjacent input nodes in the network, but the meaning of adjacency is completely lost if the internal layers of the network are fully connected. in practice, networks used with image data have array-like internal structures that aim to reflect the semantics of adjacency.
</p>

<p>
categorical attributes with more than two values are usually encoded using the so-called <b>one-hot encoding</b>. an attribute with <img src="ltx/286c5bd9a61.svg" alt="\(d\)" style="height: 0.6971em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /> possible values is represented by <img src="ltx/286c5bd9a61.svg" alt="\(d\)" style="height: 0.6971em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /> separate input bits. for any given value, the corresponding input bit is set to 1 and all the others are set to 0. this generally works better than mapping the values to integers.
</p>

<p>
on the output side of the network, the problem of encoding the raw data values into actual values <img src="ltx/e4f3ae35206.svg" alt="\(y\)" style="height: 0.7109em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" /> for the output nodes of the graph is much the same as the input encoding problem. for example, if the network is trying to predict a variable named weather, which has values {sun,rain,cloud,snow}, we would use a one-hot encoding with four bits.
</p>

<p>
so much for the data values <img src="ltx/a6bde9c6ad7.svg" alt="\(\brm{y}\)" style="height: 0.7011em; vertical-align: -0.2628em; display: inline-block" class="org-latex org-latex-inline" />. what about the prediction <img src="ltx/cc5f7caed8f.svg" alt="\(\hat  y\)" style="height: 0.9510em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" />? ideally, it would exactly match the desired value <img src="ltx/a6bde9c6ad7.svg" alt="\(\brm{y}\)" style="height: 0.7011em; vertical-align: -0.2628em; display: inline-block" class="org-latex org-latex-inline" />. and the loss would be zero, and we'd be done. in practice, this seldom happens&#x2013;especially before we have started the process of adjusting the weights. thus, we need to think about what an incorrect output value means, and how to measure the loss. in deriving the gradients, we began with the squared-error loss function. this keeps the algebra simple, but it is not the only possibility. in fact, for most deep learning applications, it is more common to interpret the output values <img src="ltx/1f7f41d3590.svg" alt="\(\hat y\)" style="height: 0.9510em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" /> as probabilities and to use the negative log likelihood as the loss function.
</p>

<p>
maximum likelihood learning finds the value of <img src="ltx/f652ff9ab54.svg" alt="\(\brm{w}\)" style="height: 0.4874em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> that maximizes the probability of the observed data. and because the log function is monotonic, this is equivalent to maximizing the log likelihood of the data, which is equivalent in turn to minimizing a loss function defined as the negative log likelihood. (taking logs turns products of probabilities into sums, which are more amenable for taking derivatives.) in other words, we are looking for <img src="ltx/73826f22224.svg" alt="\(\brm{w}^*\)" style="height: 0.7893em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> that minimizes the sum of negative log probabilities of the <img src="ltx/83ddf1cc0cb.svg" alt="\(N\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> examples:
</p>
<p>
<img src="ltx/0e54eb0ba3d.svg" alt="\[ \brm{w}^* = \argmin_w - \sum_{j=1}^{N} \log P_{\brm{w}}(\brm{y}_j|\brm{x}_j) \]" style="height: 2.9649em; display: block" class="org-latex org-latex-block" />
in the deep learning literature, it is common to talk about minimizing the cross-entropy. we typically use the definition of cross-entropy with <img src="ltx/6cd2992f682.svg" alt="\(P\)" style="height: 0.7079em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> being the true distribution over training examples, <img src="ltx/cf45f3a6ddf.svg" alt="\(P^*(\brm{x},\brm{y})\)" style="height: 1.0147em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" />, and <img src="ltx/786a45d9f74.svg" alt="\(Q\)" style="height: 0.8490em; vertical-align: -0.1805em; display: inline-block" class="org-latex org-latex-inline" /> being the predictive hypothesis <img src="ltx/2f225a2c354.svg" alt="\(P_{\brm{w}}(\brm{y} \mid \brm{x})\)" style="height: 0.9951em; vertical-align: -0.2932em; display: inline-block" class="org-latex org-latex-inline" />. minimizing the cross-entropy <img src="ltx/f742bda70ed.svg" alt="\(H(P^*(\brm{x},\brm{y}), P_{\brm{w}}(\brm{y} \mid \brm{x}))\)" style="height: 1.0333em; vertical-align: -0.2932em; display: inline-block" class="org-latex org-latex-inline" /> by adjusting <img src="ltx/f652ff9ab54.svg" alt="\(\brm{w}\)" style="height: 0.4874em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> makes the hypothesis agree as closely as possible with the true distribution. in reality, we cannot minimize this cross-entropy because we do not have access to the true data distribution <img src="ltx/cf45f3a6ddf.svg" alt="\(P^*(\brm{x},\brm{y})\)" style="height: 1.0147em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" />; but we do have access to samples from <img src="ltx/cf45f3a6ddf.svg" alt="\(P^*(\brm{x},\brm{y})\)" style="height: 1.0147em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" />, so the sum over the actual data in <a href="/feedforward_neural_network.html">this equation</a> approximates the expectation in the equation of cross-entropy.
</p>

<p>
to minimize the negative log likelihood (or the cross-entropy), we need to be able to interpret the output of the network as a probability. for example, if the network has one output unit with a sigmoid activation function and is learning a boolean classification, we can interpret the output value directly as the probability that the example belongs to the positive class. thus, for boolean classification problems, we commonly use a sigmoid output layer.
</p>

<p>
multiclass classification problems are very common in machine learning. for example, classifiers used for object recognition often need to recognize thousands of distinct categories of objects. natural language models that try to predict the next word in a sentence may have to choose among tens of thousands of possible words. for this kind of prediction, we need the network to output a categorical distribution&#x2013;that is, if there are <img src="ltx/286c5bd9a61.svg" alt="\(d\)" style="height: 0.6971em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /> possible answers, we need <img src="ltx/286c5bd9a61.svg" alt="\(d\)" style="height: 0.6971em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /> output nodes that represent probabilities summing to 1.
</p>

<p>
To achieve this, we use a softmax layer, which outputs a vector of <img src="ltx/286c5bd9a61.svg" alt="\(d\)" style="height: 0.6971em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /> values given a vector of input values <img src="ltx/eddf0293ad3.svg" alt="\(\brm{in}=(in_1,\dots,in_d)\)" style="height: 0.9872em; vertical-align: -0.2991em; display: inline-block" class="org-latex org-latex-inline" />. The kth element of that output vector is given by
<img src="ltx/3a9bae284d0.svg" alt="\[ \mathrm{softmax}(\brm{in})_k = \frac{e^{in_k}}{\sum_{k'=1}^{d} e^{in_{k'}}} \]" style="height: 2.8152em; display: block" class="org-latex org-latex-block" />
for a regression problem, where the target value <img src="ltx/e4f3ae35206.svg" alt="\(y\)" style="height: 0.7109em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" /> is continuous, it is common to use a linear output layer&#x2013;in other words, <img src="ltx/fe0f9eb30ad.svg" alt="\(\hat y_j=in_j\)" style="height: 1.1481em; vertical-align: -0.4590em; display: inline-block" class="org-latex org-latex-inline" />, without any activation function <img src="ltx/4294823b7db.svg" alt="\(g\)" style="height: 0.7030em; vertical-align: -0.2599em; display: inline-block" class="org-latex org-latex-inline" />.
</p>

<p>
many other output layers are possible.
</p>
<div id="outline-container-orge822f79" class="outline-2">
<h2 id="orge822f79">initial common lisp implementation</h2>
<div class="outline-text-2" id="text-orge822f79">
<p>
this code uses common lisp (aswell as code/functions from the actual link itself, click it for those)
</p>
<div class="org-src-container">
<pre class="src src-lisp"><span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">neural network class</span>
(<span style="font-weight: bold;">defclass</span> <span style="font-weight: bold; text-decoration: underline;">network</span> ()
  ((weights <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:weights</span> <span style="font-weight: bold;">:initform</span> nil <span style="font-weight: bold;">:accessor</span> network-weights)
   (input-layer-size <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:input-layer-size</span> <span style="font-weight: bold;">:accessor</span> network-input-layer-size)
   (output-layer-size <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:output-layer-size</span> <span style="font-weight: bold;">:accessor</span> network-output-layer-size)
   (hidden-layer-sizes <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:hidden-layer-sizes</span> <span style="font-weight: bold;">:accessor</span> network-hidden-layer-sizes)
   (learning-rate <span style="font-weight: bold;">:initform</span> 0.0005 <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:learning-rate</span> <span style="font-weight: bold;">:accessor</span> network-learning-rate)))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">network-feedforward</span> ((nw network) x)
  <span style="font-style: italic;">"pass the vector x into the network, x is treated as the input layer, the activations of the entire network are returned"</span>
  (<span style="font-weight: bold;">let</span> ((previous-layer-activations x)
        (network-activations (list x))
        (network-unsquashed-activations (list x)))
    (<span style="font-weight: bold;">loop</span> for weights-index from 0 below (length (network-weights nw))
          do (<span style="font-weight: bold;">let*</span> ((weights (elt (network-weights nw) weights-index))
                    (layer-activations (list-&gt;vector (make-list (length (elt weights 0)) <span style="font-weight: bold;">:initial-element</span> 0))))
               (<span style="font-weight: bold;">loop</span> for i from 0 below (length previous-layer-activations)
                     do (<span style="font-weight: bold;">let</span> ((previous-unit-activation (elt previous-layer-activations i))
                              (previous-unit-weights (elt weights i)))
                          (setf
                           layer-activations
                           (map
                            'list
                            (<span style="font-weight: bold;">lambda</span> (weight activation)
                              (+ activation (* previous-unit-activation weight)))
                            previous-unit-weights layer-activations))))
               (setf previous-layer-activations (map 'list #'sigmoid layer-activations))
               (setf network-activations
                     (append network-activations
                             (list previous-layer-activations)))
               (setf network-unsquashed-activations
                     (append network-unsquashed-activations
                             (list layer-activations)))))
    (values (list-&gt;vector network-activations) (list-&gt;vector network-unsquashed-activations))))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">network-generate-weights</span> ((nw network))
  <span style="font-style: italic;">"generate random weights based on the sizes of the layers"</span>
  (setf
   (network-weights nw)
   (<span style="font-weight: bold;">loop</span> for i from 0 below (1+ (length (network-hidden-layer-sizes nw)))
         collect (<span style="font-weight: bold;">let*</span> ((layer-sizes (append
                                      (list (network-input-layer-size nw))
                                      (network-hidden-layer-sizes nw)
                                      (list (network-output-layer-size nw))))
                        (layer-size (elt layer-sizes i))
                        (next-layer-size (elt layer-sizes (1+ i))))
                   (<span style="font-weight: bold;">loop</span> for j from 0 below layer-size
                         collect (<span style="font-weight: bold;">loop</span> for k from 0 below next-layer-size
                                       collect (generate-random-weight))))))
  <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">convert weights from nested lists to nested vector for fast access</span>
  (setf (network-weights nw) (list-&gt;vector (network-weights nw))))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">print-object</span> ((nw network) stream)
  (print-unreadable-object (nw stream <span style="font-weight: bold;">:type</span> t)
    (format stream <span style="font-style: italic;">"total weights: ~a"</span>
            (reduce
             #'*
             (append (list (network-input-layer-size nw) (network-output-layer-size nw))
                     (network-hidden-layer-sizes nw))))))

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">generate-random-weight</span> ()
  (/ (- (random 2000) 1000) 1000))

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">make-network</span> (<span style="font-weight: bold; text-decoration: underline;">&amp;key</span> input-layer-size hidden-layer-sizes output-layer-size
                       learning-rate)
  (<span style="font-weight: bold;">let</span> ((nw (make-instance 'network
                           <span style="font-weight: bold;">:input-layer-size</span> input-layer-size
                           <span style="font-weight: bold;">:hidden-layer-sizes</span> hidden-layer-sizes
                           <span style="font-weight: bold;">:output-layer-size</span> output-layer-size
                           <span style="font-weight: bold;">:learning-rate</span> learning-rate)))
    (network-generate-weights nw)
    nw))
</pre>
</div>
<p>
while implementing backpropagation i faced a challenge(s), can i simply iterate through the layers one by one like i did in <code>network-feedforward</code> and just handle them each at a time? at first i thought i couldnt, because i need to go through each possible path from the output layer to the input layer and update each weight, each weight's gradient depends on the gradient for the weight preceding it in the path, but after alot of thinking i realized that i could just store the gradients of each matrix of weights between 2 layers and update the gradient matrix as we go, this would probably work i think, but im more curious about how i would go about finding all paths and operating on them in the way i described.
</p>

<p>
we have a variable number of layers <img src="ltx/ddcbd3a127e.svg" alt="\(n\)" style="height: 0.4982em; vertical-align: -0.0541em; display: inline-block" class="org-latex org-latex-inline" />, we definitely could iterate through these layers but on each iteration we only have access to the current layers units, and to construct a path from the input layer to the output layer we need to find <img src="ltx/ddcbd3a127e.svg" alt="\(n\)" style="height: 0.4982em; vertical-align: -0.0541em; display: inline-block" class="org-latex org-latex-inline" /> units, one from each layer, so this cannot be done by a simple for loop that iterates through the layers one at a time, what about nested loops (one for each layer)? but how are we supposed to nest <img src="ltx/ddcbd3a127e.svg" alt="\(n\)" style="height: 0.4982em; vertical-align: -0.0541em; display: inline-block" class="org-latex org-latex-inline" /> loops? this isnt possible, so maybe recursion could help?
say we did use recursion, and on each recurrence we hopped backwards into the previous layer in the network, how are we supposed to return the different units from a layer to a function call? well we dont need to, we could just call the function on the many units we have in that previous layer and pass it an index to say which unit we're referring to.
</p>

<div class="org-src-container">
<pre class="src src-lisp">(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">network-train</span> ((nw network) xs ys)
  <span style="font-style: italic;">"train on the given data, xs is a list of vectors, each vector is treated as an input layer to the network, and ys is a list of vectors, each vector representing the output layer corresponding to the vector in xs thats at the same index"</span>
  (<span style="font-weight: bold;">loop</span> for i from 0 below (length xs)
        do (<span style="font-weight: bold;">let*</span>
               ((x (elt xs i))
                (y (elt ys i))
                (layer-index (1+ (length (network-hidden-layer-sizes nw)))))
             (<span style="font-weight: bold;">multiple-value-bind</span> (activations unsquashed-activations)
                 (network-feedforward nw x)
               (<span style="font-weight: bold;">loop</span>
                 for unit-index
                 from 0
                   below (network-output-layer-size nw)
                 do (network-back-propagate
                     nw
                     layer-index
                     unit-index
                     (* 2 (- (elt (elt activations layer-index) unit-index)
                             (elt y unit-index)))
                     activations
                     unsquashed-activations))))))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">network-back-propagate</span> ((nw network) layer-index unit-index gradient activations unsquashed-activations)
  <span style="font-style: italic;">"backpropagate the error through the network, each layers gradients depend on those of the layer succeeding it in the network"</span>
  (<span style="font-weight: bold;">let*</span>
      ((predecessor-layer-weights (elt (network-weights nw) (1- layer-index)))
       (weights-into-unit
         (map
          'vector
          (<span style="font-weight: bold;">lambda</span> (predecessor-unit-weights)
            (elt predecessor-unit-weights unit-index))
          predecessor-layer-weights))
       (unit-activation (elt (elt activations layer-index) unit-index))
       (unit-input (elt (elt unsquashed-activations layer-index) unit-index)))
    (<span style="font-weight: bold;">loop</span> for predecessor-unit-index from 0 below (length predecessor-layer-weights)
          do (<span style="font-weight: bold;">let*</span>
                 ((weight (elt weights-into-unit predecessor-unit-index))
                  (predecessor-unit-activation
                    (elt (elt activations (1- layer-index))
                         predecessor-unit-index))
                  (gradient-to-back-propagate
                    (* gradient
                       (sigmoid-derivative unit-input)
                       weight))
                  (actual-gradient (* gradient
                                      (sigmoid-derivative unit-input)
                                      predecessor-unit-activation))
                  (weight-change (* (network-learning-rate nw)
                                    actual-gradient)))
               (<span style="font-weight: bold;">if</span> (&gt; layer-index 1)
                   (network-back-propagate
                    nw
                    (1- layer-index)
                    predecessor-unit-index
                    gradient-to-back-propagate
                    activations
                    unsquashed-activations))
               (setf
                (elt (elt (elt (network-weights nw) (1- layer-index))
                          predecessor-unit-index)
                     unit-index)
                (- weight weight-change))))))
</pre>
</div>
<p>
lets try training on some simple data
</p>
<div class="org-src-container">
<pre class="src src-lisp">(<span style="font-weight: bold;">defparameter</span> <span style="font-weight: bold; font-style: italic;">nw</span> (make-network
                  <span style="font-weight: bold;">:input-layer-size</span> 3
                  <span style="font-weight: bold;">:hidden-layer-sizes</span> '(2)
                  <span style="font-weight: bold;">:output-layer-size</span> 1
                  <span style="font-weight: bold;">:learning-rate</span> 0.01))
(<span style="font-weight: bold;">loop</span> for i from 0 below 10000
      do (network-train nw '((0 0 1) (1 1 1) (1 0 1) (0 1 1)) '((0) (1) (1) (0))))
(<span style="font-weight: bold;">multiple-value-bind</span> (activations unsquashed-activations)
    (network-feedforward nw '(1 1 0))
  (print activations))
(<span style="font-weight: bold;">multiple-value-bind</span> (activations unsquashed-activations)
    (network-feedforward nw '(0 1 0))
  (print activations))
</pre>
</div>

<pre class="example">

#(#(1 1 0) #(0.84486073 0.013950213) #(0.9093194)) 
#(#(0 1 0) #(0.59563345 0.5919403) #(0.2278899)) 
</pre>


<p>
it predicts correctly that the first number is the output number
using this code, i was getting 125k weight updates in 0.01 seconds (only 1 thread, no gpu), which i think is as far as i can get in terms of efficiency without some more advanced matrix algorithms, i tried python and got 1m (simple addition) operations in 0.04 seconds:
</p>
<div class="org-src-container">
<pre class="src src-bash">~ &#955; time python -c <span style="font-style: italic;">'</span>
<span style="font-style: italic;">i=1</span>
<span style="font-style: italic;">for i in range(999999):</span>
<span style="font-style: italic;">  i = i + 1</span>
<span style="font-style: italic;">print(i)'</span>
999999
python -c <span style="font-style: italic;">' i=1; for i in range(999999):   i = i + 1; print(i)'</span>  0.04s user 0.00s system 98% cpu 0.045 total
</pre>
</div>
<p>
we need  a method to keep to keep track of loss, aswell as visual feedback would be nice, see common lisp graphics which ill be also using here
</p>
<div class="org-src-container">
<pre class="src src-lisp">(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">network-test</span> ((nw network) xs ys)
  <span style="font-style: italic;">"test the given data (collection of vectors for input/output layers), return the total loss"</span>
  (<span style="font-weight: bold;">let</span> ((loss 0))
    (<span style="font-weight: bold;">loop</span> for i from 0 below (length xs)
          do (<span style="font-weight: bold;">let*</span>
                 ((x (elt xs i))
                  (y (elt ys i)))
               (<span style="font-weight: bold;">multiple-value-bind</span> (activations unsquashed-activations)
                   (network-feedforward nw x)
                 (<span style="font-weight: bold;">let*</span> ((output-layer (elt activations (1- (length activations))))
                        (example-loss (expt (vector-sum (vector-sub y output-layer)) 2)))
                   (incf loss example-loss)))))
    loss))
</pre>
</div>
<p>
we can use this function to plot the loss function after each epoch:
</p>
<div class="org-src-container">
<pre class="src src-lisp">(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">feedforward-network-first-test</span> ()
  (<span style="font-weight: bold;">defparameter</span> <span style="font-weight: bold; font-style: italic;">*train-in*</span> '((0 0 1) (1 1 1) (1 0 1) (0 1 1)))
  (<span style="font-weight: bold;">defparameter</span> <span style="font-weight: bold; font-style: italic;">*train-out*</span> '((0) (1) (1) (0)))
  (<span style="font-weight: bold;">defparameter</span> <span style="font-weight: bold; font-style: italic;">*win*</span> (make-instance 'window <span style="font-weight: bold;">:width</span> 800 <span style="font-weight: bold;">:height</span> 800))
  (<span style="font-weight: bold;">defparameter</span> <span style="font-weight: bold; font-style: italic;">*nw*</span> (make-network
                      <span style="font-weight: bold;">:input-layer-size</span> 3
                      <span style="font-weight: bold;">:hidden-layer-sizes</span> '(2)
                      <span style="font-weight: bold;">:output-layer-size</span> 1
                      <span style="font-weight: bold;">:learning-rate</span> 0.01))
  (<span style="font-weight: bold;">let*</span> ((loss-history '())
         (epochs 10000)
         (axis (make-axis
                <span style="font-weight: bold;">:min-x</span> (/ (- epochs) 10)
                <span style="font-weight: bold;">:max-x</span> epochs
                <span style="font-weight: bold;">:min-y</span> -0.1
                <span style="font-weight: bold;">:pos</span> (make-point-2d <span style="font-weight: bold;">:x</span> 0 <span style="font-weight: bold;">:y</span> 0)
                <span style="font-weight: bold;">:width</span> (window-width *win*)
                <span style="font-weight: bold;">:height</span> (window-height *win*))))
    (<span style="font-weight: bold;">loop</span> for i from 0 below epochs
          do (network-train *nw* *train-in* *train-out*)
             (<span style="font-weight: bold;">let</span> ((loss (network-test *nw* *train-in* *train-out*)))
               (setf loss-history (append loss-history (list loss)))
               (<span style="font-weight: bold;">if</span> (&gt; loss (axis-max-y axis))
                   (setf (axis-max-y axis) (* loss 1.1)))))
    <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">(setf (axis-min-y axis) (* loss 0.1)))))</span>
    (<span style="font-weight: bold;">let*</span> ((loss-plot (make-discrete-plot
                       <span style="font-weight: bold;">:points</span> (map
                                'vector
                                (<span style="font-weight: bold;">lambda</span> (loss epoch) (make-point-2d <span style="font-weight: bold;">:x</span> epoch <span style="font-weight: bold;">:y</span> (elt loss-history epoch)))
                                loss-history
                                (<span style="font-weight: bold;">loop</span> for i from 0 below epochs by 100 collect i))))) <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">by 100 to reduce number of points to plot</span>
      (axis-add-renderable axis loss-plot)
      (window-add-renderable *win* axis)
      (window-run *win*))))
</pre>
</div>
<p>
although notice that here we're testing the network on the same data we trained it with, which generally isnt a good measure of the performance of the network, but this is just an example of how we can keep track of a property
</p>
</div>
</div>
<div id="outline-container-orga4a0597" class="outline-2">
<h2 id="orga4a0597">making use of gpu</h2>
<div class="outline-text-2" id="text-orga4a0597">
<p>
before implementing this i started experimenting with <a href="/cuda_programming_in_common_lisp.html">cuda in common lisp</a>, one unfortunate thing is that <code>cl-cuda</code> doesnt allow using arbitrary data types like you would have in <code>C</code> with structs, we're gonna have to live with arrays of floats on the gpu
at first i was gonna implement the most straightforward approach, which is to run every backpropagation/feedforward operation by a single gpu thread, so that if we have 256 gpu threads, we'd be doing 256 backpropagation operations in parallel whereas on the cpu (with a single thread) we'd be doing only 1, i thought this would definitely work, until i actually thought about it and then realized, how the hell can this be a thread-safe process? each thread is modifying every weight in the network and many threads are running at once, it would be a nightmare to actually use such an approach, and a miracle if it even works as intended, so i had to think of something else, something thread-safe.</p>
</div>
</div>
</div>
</body>
</html>
