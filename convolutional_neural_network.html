<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="mahmood" />
<meta name="generator" content="Org Mode" />
<title>convolutional neural network</title><!-- lambda icon, frail attempt -->
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%221em%22 font-size=%22100%22 color=%22red%22>λ</text></svg>">
<!-- not-so-awesome awesome font -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

<link rel="stylesheet" href="/main.css">
</head>
<body>
<div id="preamble" class="status">
<div class="navbar">
  <a href='/'>home</a>
  <a href='/blog.html'>blog</a>
  <a href='/archive.html'>archive</a>
  <a href='/about.html'>about</a>
</div><h1 class="main-title">convolutional neural network</h1>
</div>
<div id="content" class="content">
<div id="outline-container-org14aa339" class="outline-2">
<h2 id="org14aa339">citations</h2>
<div class="outline-text-2" id="text-org14aa339">
<ul class="org-ul">
<li>(Peter Norvig, Stuart J. Russell, 2020)</li>
<li>(Jianxin Wu, 2017)</li>
<li>(Zhifei Zhang, 2016)</li>
</ul>
<p>
and countless other resources that helped me form a good understanding of the topics related to conv networks
</p>
</div>
</div>
<div id="outline-container-org4aec26d" class="outline-2">
<h2 id="org4aec26d">motivation</h2>
<div class="outline-text-2" id="text-org4aec26d">
<p>
an image cannot be thought of as a simple vector of input pixel values, primarily because <b>adjacency of pixels in an image matters</b>. simple <a href="/feedforward_neural_network.html">multilayer perceptrons</a> arent suited for images because adjacent neurons in the same layer arent really connected or affected by each other, which isnt the case in images.
</p>

<p>
consider an input image with <img src="ltx/ddcbd3a127e.svg" alt="\(n\)" style="height: 0.4982em; vertical-align: -0.0541em; display: inline-block" class="org-latex org-latex-inline" /> pixels, if the input and the first hidden layer are fully connected, that means we'd have <img src="ltx/fab8a54310b.svg" alt="\(n^2\)" style="height: 0.9580em; vertical-align: -0.0541em; display: inline-block" class="org-latex org-latex-inline" /> weights; for a typical RGB image, thats 9 trillion weights, which is an unreasonable amount of parameters, all the more reason we need to ditch multilayer perceptrons.
</p>

<p>
these considerations suggest that we should construct the first hidden layer so that <b>each hidden unit receives input from only a small, local region of the image</b>. this kills two birds with one stone. first, it respects adjacency, at least locally. second, it cuts down the number of weights.
</p>

<p>
so far, so good. but we are missing another important property of images: roughly speaking, anything that is detectable in one small, local region of the image&#x2013;perhaps an eye or a blade of grass&#x2013;would look the same if it appeared in another small, local region of the image. in other words, we expect image data to exhibit approximate <b>spatial invariance</b>, at least at small to moderate scales. we don’t necessarily expect the top halves of photos to look like bottom halves, so there is a scale beyond which spatial invariance no longer holds.
</p>

<p>
local spatial invariance can be achieved by constraining the <img src="ltx/da14e46a009.svg" alt="\(l\)" style="height: 0.7060em; vertical-align: -0.0649em; display: inline-block" class="org-latex org-latex-inline" /> weights connecting a local region to a unit in the hidden layer to be the same for each hidden unit. (that is, for hidden units <img src="ltx/b7a09af24c5.svg" alt="\(i\)" style="height: 0.6746em; vertical-align: -0.0786em; display: inline-block" class="org-latex org-latex-inline" /> and <img src="ltx/53c0c910d3c.svg" alt="\(j\)" style="height: 0.8559em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" />, the weights <img src="ltx/d3b374bc8a3.svg" alt="\(w_{1,j},\dots,w_{l,j}\)" style="height: 0.9051em; vertical-align: -0.4590em; display: inline-block" class="org-latex org-latex-inline" /> are the same as <img src="ltx/d3b374bc8a3.svg" alt="\(w_{1,j},\dots,w_{l,j}\)" style="height: 0.9051em; vertical-align: -0.4590em; display: inline-block" class="org-latex org-latex-inline" />) this makes the hidden units into <b>feature detectors</b> that detect the same feature wherever it appear in the image. typically, we want the first hidden layer to detect many kinds of features, not just one; so for each local image region we might have <img src="ltx/286c5bd9a61.svg" alt="\(d\)" style="height: 0.6971em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /> hidden units with <img src="ltx/286c5bd9a61.svg" alt="\(d\)" style="height: 0.6971em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /> distinct sets of weights. this means that there are <img src="ltx/9e1c5853d71.svg" alt="\(dl\)" style="height: 0.7060em; vertical-align: -0.0649em; display: inline-block" class="org-latex org-latex-inline" /> weights in all&#x2013;a number that is not only far smaller than <img src="ltx/fab8a54310b.svg" alt="\(n^2\)" style="height: 0.9580em; vertical-align: -0.0541em; display: inline-block" class="org-latex org-latex-inline" />, but is actually independent of <img src="ltx/ddcbd3a127e.svg" alt="\(n\)" style="height: 0.4982em; vertical-align: -0.0541em; display: inline-block" class="org-latex org-latex-inline" />, the image size. thus, by injecting some prior knowledge&#x2013;namely, knowledge of adjacency and spatial invariance&#x2013;we can develop models that have far fewer parameters and can learn much more quickly.
</p>

<p>
usually an image is stored in memory as a 2d array (or matrix), each cell containing 3 different values for RGB (red,green,blue), which forms a 3 dimensional array or a tensor of order 3, which means the network needs to accept tensors as inputs.
</p>
</div>
</div>
<div id="outline-container-org7cd5fe6" class="outline-2">
<h2 id="org7cd5fe6">convolutional layers</h2>
<div class="outline-text-2" id="text-org7cd5fe6">
<p>
a convolutional neural network is one that contains spatially local connections, at least in the early layers which we call <b>convolutional layers</b>, and has patterns of weights that are replicated across the units in each layer. a pattern of weights that is replicated across multiple local regions is called a <b>kernel</b> which is applied with a convolution.
</p>

<p>
in practice, multiple kernels are learned at each convolutional layer, suppose the input to the <img src="ltx/b1c9236fece.svg" alt="\(\ell\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" />-th layer is an order 3 tensor with size <img src="ltx/5b83a7227f3.svg" alt="\(D^\ell \times H^\ell \times W^\ell\)" style="height: 1.0074em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" />, a kernel would also be an order 3 tensor with size <img src="ltx/6f90cd02d91.svg" alt="\(D^\ell \times H \times W\)" style="height: 1.0074em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" /> (<img src="ltx/0dbc0c2e856.svg" alt="\(W\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> and <img src="ltx/5b452e7595e.svg" alt="\(H\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> are the dimensions of the kernel and they differ from <img src="ltx/8707bd34260.svg" alt="\(W^\ell,H^\ell\)" style="height: 1.0858em; vertical-align: -0.1599em; display: inline-block" class="org-latex org-latex-inline" />, but the depth of both the image and tensor should be equal for the result of the convolution to be of lower dimensionality), assuming <img src="ltx/2aac24b1aeb.svg" alt="\(K\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> kernels are used (in other words, <img src="ltx/2aac24b1aeb.svg" alt="\(K\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> feature maps), the tensor of kernels (or weights) would be an order 4 tensor in <img src="ltx/66a5d97186f.svg" alt="\(\mathbb{R}^{K \times D^\ell \times H \times W}\)" style="height: 1.2036em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />, assuming a stride of 1, the output <img src="ltx/a6bde9c6ad7.svg" alt="\(\brm{y}\)" style="height: 0.7011em; vertical-align: -0.2628em; display: inline-block" class="org-latex org-latex-inline" /> would be a tensor of the third order and of size <img src="ltx/260a0b11fa3.svg" alt="\(K \times (H^\ell-(H-1)) \times (W^\ell-(W-1)))\)" style="height: 1.2005em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" /> which is composed of <img src="ltx/2aac24b1aeb.svg" alt="\(K\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> feature maps.
</p>

<div class="math-block note" data-before="note" data-after="" id="org0306305">
<p>
its important to maintain the order of dimensions as depth X height X width, because thats the order i use everywhere
</p>

</div>
<p>
<img src="/conv_depth-1.webp" />
at the first layer, the image is stored as a tensor of order 3 of <b>depth</b> <img src="ltx/804697bef82.svg" alt="\(D^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />, each layer of this tensor is basically a matrix we call a <b>feature map</b> or <b>channel</b>, so at each layer, the image is stored as nested layers of feature maps, it might be easier to take the input to the first layer as an example, which can be thought of as a set of 3 feature maps of RGB values.
</p>
</div>
</div>
<div id="outline-container-orgc58f3cf" class="outline-2">
<h2 id="orgc58f3cf">pooling layers and downsampling</h2>
<div class="outline-text-2" id="text-orgc58f3cf">
<p>
a pooling layer in a neural network summarizes a set of adjacent units from the preceding pooling layer with a single value. pooling works just like a convolution layer, with a kernel size l and stride <img src="ltx/fa6d4a870fd.svg" alt="\(s\)" style="height: 0.5011em; vertical-align: -0.0541em; display: inline-block" class="org-latex org-latex-inline" />, but the operation that is applied is fixed rather than learned.
</p>

<p>
typically, no activation function is associated with the pooling layer. there are two common forms of pooling:
</p>
<ul class="org-ul">
<li><b>average-pooling</b> computes the average value of its <img src="ltx/da14e46a009.svg" alt="\(l\)" style="height: 0.7060em; vertical-align: -0.0649em; display: inline-block" class="org-latex org-latex-inline" /> inputs. this is identical to concvolution with a uniform kernel vector <img src="ltx/bd82be2338b.svg" alt="\(k=[1/l,\dots,1/l]\)" style="height: 1.0264em; vertical-align: -0.2814em; display: inline-block" class="org-latex org-latex-inline" />. if we set <img src="ltx/4c132cc1085.svg" alt="\(l=s\)" style="height: 0.7060em; vertical-align: -0.0649em; display: inline-block" class="org-latex org-latex-inline" />, the effect is to coarsen the resolution of the image&#x2013;to downsample it&#x2013;by a factor of <img src="ltx/fa6d4a870fd.svg" alt="\(s\)" style="height: 0.5011em; vertical-align: -0.0541em; display: inline-block" class="org-latex org-latex-inline" />. an object that occupied, say, <img src="ltx/c6edc9cae81.svg" alt="\(10s\)" style="height: 0.7197em; vertical-align: -0.0541em; display: inline-block" class="org-latex org-latex-inline" /> pixels, would now occupy only 10 pixels after pooling. the same learned classifier that would be able to recognize the object at a size of 10 pixels in the original image would now be able to recognize that object in the pooled image, even if it was too big to recognize in the original image. in other words, average-pooling facilitates <b>multiscale recognition</b>. it also reduces the number of weights required in subsequent layers, leading to lower computational cost and possibly faster learning.</li>

<li><b>max-pooling</b> computes the maximum value of its <img src="ltx/da14e46a009.svg" alt="\(l\)" style="height: 0.7060em; vertical-align: -0.0649em; display: inline-block" class="org-latex org-latex-inline" /> inputs. it can also be used purely for downsampling, but it has a somewhat different semantics. suppose we applied max-pooling to a layer with the values [5,9,4]: the result would be a 9, indicating that somewhere in the input image there is a darker dot that is detected by the kernel. in other words, max-pooling acts as a kind of logical disjunction, saying that a feature exists somewhere in the unit's receptive field.</li>
</ul>

<p>
if the goal is to classify the image into one of <img src="ltx/93b8fb15026.svg" alt="\(c\)" style="height: 0.5002em; vertical-align: -0.0560em; display: inline-block" class="org-latex org-latex-inline" /> categories, then the final layer of the network will be a softmax with <img src="ltx/93b8fb15026.svg" alt="\(c\)" style="height: 0.5002em; vertical-align: -0.0560em; display: inline-block" class="org-latex org-latex-inline" /> output units. the early layers of the CNN are image-sized, so somewhere in between there must be significant reductions in layer size. convolution layers and pooling layers with stride larger than 1 all serve to reduce the layer size. it's also possible to reduce the layer size simply by having a fully connected layer with fewer units than the preceding layer. CNNs often have one or two such layers preceding the final softmax layer.
</p>

<p>
let <img src="ltx/07b79fa0b6d.svg" alt="\(\brm{x}^\ell \in \mathbb{R}^{H^\ell \times W^\ell \times D^\ell}\)" style="height: 1.2673em; vertical-align: -0.1129em; display: inline-block" class="org-latex org-latex-inline" /> be the input to the <img src="ltx/da14e46a009.svg" alt="\(l\)" style="height: 0.7060em; vertical-align: -0.0649em; display: inline-block" class="org-latex org-latex-inline" />-th pooling layer, the pooling operation requires no parameters (the matrix of weights is nulled), hence parameter learning is not needed for this layer, let the spatial extent (dimensions) of the pooling be <img src="ltx/1e6c459d67e.svg" alt="\(H \times W\)" style="height: 0.7412em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" />, assume that <img src="ltx/5b452e7595e.svg" alt="\(H\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> divides <img src="ltx/6285d6349a9.svg" alt="\(H^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> and <img src="ltx/0dbc0c2e856.svg" alt="\(W\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> divides <img src="ltx/58e9e9afc8c.svg" alt="\(W^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />, the output of pooling (<img src="ltx/a6bde9c6ad7.svg" alt="\(\brm{y}\)" style="height: 0.7011em; vertical-align: -0.2628em; display: inline-block" class="org-latex org-latex-inline" /> or equivalently <img src="ltx/e29f5a32d3a.svg" alt="\(\brm{x}^{\ell+1}\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />) will be an order 3 tensor of size <img src="ltx/96607db978b.svg" alt="\(H^{\ell+1} \times W^{\ell+1} \times D^{\ell+1}\)" style="height: 1.0074em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" />, with
</p>

<div class="equation-container">
<span class="equation">
<img src="ltx/ff413d174f2.svg" alt="\begin{equation} \label{eq-convnet-pooling-dims}
  H^{\ell+1}=\frac{H^\ell}{H}, \quad W^{\ell+1}=\frac{W^\ell}{W}, \quad D^{\ell+1}=D^\ell
\end{equation}
" style="height: 2.2344em; vertical-align: -0.5392em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
a pooling layer operates upon <img src="ltx/7cd7cc44bf5.svg" alt="\(\brm{x}^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> channel by channel independently. within each channel, the matrix with <img src="ltx/6ad4d111d73.svg" alt="\(H^\ell \times W^\ell\)" style="height: 1.0074em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" /> elements are divided into <img src="ltx/87d86ddd5db.svg" alt="\(H^{\ell+1}\times W^{\ell+1}\)" style="height: 1.0074em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" /> nonoverlapping subregions, each subregion being <img src="ltx/1e6c459d67e.svg" alt="\(H \times W\)" style="height: 0.7412em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" /> in size. the pooling operator then maps a subregion into a single number.
</p>


<div class="equation-container">
<span class="equation">
<img src="ltx/9fa4d325ebe.svg" alt="\begin{align}
  \text{max :} &amp;amp;&amp;amp; y_{i^{\ell+1}},j^{\ell+1},d = \max_{0 \le i &amp;lt; H,0 \le j \le W} x^\ell_{i^{\ell+1} \times H+i,j^{\ell+1} \times W+j,d}\\
  \text{average :} &amp;amp;&amp;amp; y_{i^{\ell+1},j^{\ell+1},d} = \frac{1}{HW} \sum_{0 \le i&amp;lt;H,0 \le j&amp;lt;W} x^\ell_{i^{\ell+1}\times H+i,j^{\ell+1}\times W+j,d}
\end{align}
" style="height: 4.9494em; vertical-align: -0.5392em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
where <img src="ltx/8602d1101b1.svg" alt="\(0 \le i^{\ell+1}&amp;lt;H^{\ell+1},0 \le j^{\ell+1}&amp;lt;W^{\ell+1}\)" style="height: 1.1848em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" />, and <img src="ltx/8fa4e1d18ca.svg" alt="\(0 \le d&amp;lt;D^{\ell+1}=D^\ell\)" style="height: 1.1485em; vertical-align: -0.2226em; display: inline-block" class="org-latex org-latex-inline" />.
</p>

<p>
since pooling layers have no weights they arent directly connected to their preceding layer, so during backpropagation, we cant directly tell what subregion of the previous layer each pixel in <img src="ltx/a6bde9c6ad7.svg" alt="\(\brm{y}\)" style="height: 0.7011em; vertical-align: -0.2628em; display: inline-block" class="org-latex org-latex-inline" /> corresponds to, but we can reverse the pooling operation to "decode" the values from <img src="ltx/a6bde9c6ad7.svg" alt="\(\brm{y}\)" style="height: 0.7011em; vertical-align: -0.2628em; display: inline-block" class="org-latex org-latex-inline" /> back to <img src="ltx/7cd7cc44bf5.svg" alt="\(\brm{x}^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />. assuming a max-pooling layer, we need a triplet <img src="ltx/fbbf1d16717.svg" alt="\((i^\ell,j^\ell,d^\ell)\)" style="height: 1.2005em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" /> to pinpoint one element in the input <img src="ltx/7cd7cc44bf5.svg" alt="\(\brm{x}^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />, and another triplet <img src="ltx/3993a6a544a.svg" alt="\((i^{\ell+1},j^{\ell+1},d^{\ell+1})\)" style="height: 1.2005em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" /> to locate one element in <img src="ltx/a6bde9c6ad7.svg" alt="\(\brm{y}\)" style="height: 0.7011em; vertical-align: -0.2628em; display: inline-block" class="org-latex org-latex-inline" />. assuming a max-pooling layer (process is the same for average-pooling), the pooling output <img src="ltx/260c98b6355.svg" alt="\(y_{i^{\ell+1},j^{\ell+1},d^{\ell+1}}\)" style="height: 1.0027em; vertical-align: -0.5392em; display: inline-block" class="org-latex org-latex-inline" /> comes from <img src="ltx/6ec44f08550.svg" alt="\(x^\ell_{i^\ell,j^\ell,d^\ell}\)" style="height: 1.6158em; vertical-align: -0.5392em; display: inline-block" class="org-latex org-latex-inline" />, iff the following conditions are met:
</p>
<ul class="org-ul">
<li>they are in the same channel</li>
<li>the <img src="ltx/fe891574f7f.svg" alt="\((i^\ell,j^\ell)\)" style="height: 1.2005em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" />-th spatial entry belongs to the <img src="ltx/a6ceeffe1ba.svg" alt="\((i^{\ell+1},j^{\ell+1})\)" style="height: 1.2005em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" />-th subregion</li>
<li>the <img src="ltx/fe891574f7f.svg" alt="\((i^\ell,j^\ell)\)" style="height: 1.2005em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" />-th spatial entry is the largest one in that subregion</li>
</ul>
<p>
translating these conditions into equations, we get
</p>

<div class="equation-container">
<span class="equation">
<img src="ltx/0af0e37f74b.svg" alt="\begin{gather}
  d^{\ell+1} = d^\ell\\
  \floor*{\frac{i^\ell}{H}} = i^{\ell+1}, \floor*{\frac{j^\ell}{W}} = j^{\ell+1}\\
  x^\ell_{i^\ell,j^\ell,d^\ell} \geq y_{i+i^{\ell+1}\times H,j+j^{\ell+1}\times W,d^t}, \forall 0 \le i &amp;lt; H, 0 \le j&amp;lt;W
\end{gather}
" style="height: 6.7392em; vertical-align: -0.5392em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
</div>
</div>
<div id="outline-container-org79a3e73" class="outline-2">
<h2 id="org79a3e73">flatten layer</h2>
<div class="outline-text-2" id="text-org79a3e73">
<p>
a layer that takes a tensor and vectorizes or <b>flattens</b> it, turning it into a 1d vector, it doesnt need any parameters.
</p>
</div>
</div>
<div id="outline-container-org044414f" class="outline-2">
<h2 id="org044414f">fully connected convolutional layers</h2>
<div class="outline-text-2" id="text-org044414f">
<p>
suppose the input of a layer <img src="ltx/7cd7cc44bf5.svg" alt="\(\brm{x}^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> has size <img src="ltx/5b83a7227f3.svg" alt="\(D^\ell \times H^\ell \times W^\ell\)" style="height: 1.0074em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" />. if we use convolution kernels whose size is <img src="ltx/5b83a7227f3.svg" alt="\(D^\ell \times H^\ell \times W^\ell\)" style="height: 1.0074em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" />, then <img src="ltx/9f774ae7c31.svg" alt="\(K^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> such kernels form an order 4 tensor in <img src="ltx/a303e8c9a99.svg" alt="\(K^\ell \times D^\ell \times H^\ell \times W^\ell\)" style="height: 1.0074em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" />. the output would be <img src="ltx/3f31beb82da.svg" alt="\(\brm{y} \in \mathbb{R}^{K^\ell}\)" style="height: 1.4172em; vertical-align: -0.2628em; display: inline-block" class="org-latex org-latex-inline" />, which is why these layers are sometimes used as flatten layers.
</p>

<p>
it is obvious that to compute any element in <img src="ltx/a6bde9c6ad7.svg" alt="\(\brm{y}\)" style="height: 0.7011em; vertical-align: -0.2628em; display: inline-block" class="org-latex org-latex-inline" />, we need to use all elements in the input <img src="ltx/7cd7cc44bf5.svg" alt="\(\brm{x}^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />. hence, this layer is a fully connected layer, but can be implemented as a convolution layer. hence, we do not need to derive learning rules for a fully connected layer separately.
</p>

<p>
if the data has already been flattened in a preceding layer, then the input size would be <img src="ltx/81d15e09f91.svg" alt="\(\brm{x}^\ell \in \mathbb{R}^{K^{\ell-1}}\)" style="height: 1.2673em; vertical-align: -0.1129em; display: inline-block" class="org-latex org-latex-inline" />, we use <img src="ltx/9f774ae7c31.svg" alt="\(K^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> kernels of size <img src="ltx/db4e3dd7802.svg" alt="\(K^{\ell-1}\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />, which form a 2d matrix we can call <img src="ltx/ac5c1e800f4.svg" alt="\(M\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> of size <img src="ltx/a73d7e76f3b.svg" alt="\(K^\ell \times K^{\ell-1}\)" style="height: 1.0074em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" />. the convolution <img src="ltx/b2a5d65c33c.svg" alt="\(M * \brm{x}^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />, is exactly the same as the matrix multiplication <img src="ltx/9edc8e211d4.svg" alt="\(M_{K \times K^{\ell-1}} \times {{\brm{x}^\ell}^{T}}_{K^{\ell-1} \times 1}\)" style="height: 1.5811em; vertical-align: -0.4118em; display: inline-block" class="org-latex org-latex-inline" /> (<img src="ltx/ac5c1e800f4.svg" alt="\(M\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> multiplied by the transpose of the vector <img src="ltx/7cd7cc44bf5.svg" alt="\(\brm{x}^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />), which results in a matrix of size <img src="ltx/67f15670529.svg" alt="\(K \times 1\)" style="height: 0.7412em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" />, again meaning <img src="ltx/a6bde9c6ad7.svg" alt="\(\brm{y}\)" style="height: 0.7011em; vertical-align: -0.2628em; display: inline-block" class="org-latex org-latex-inline" /> is just a vector of size <img src="ltx/9f774ae7c31.svg" alt="\(K^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />, <img src="ltx/3f31beb82da.svg" alt="\(\brm{y} \in \mathbb{R}^{K^\ell}\)" style="height: 1.4172em; vertical-align: -0.2628em; display: inline-block" class="org-latex org-latex-inline" />.
</p>

<p>
because the convolution operation in this layer is equal to a matrix multiplication, the fully connected layers here behave in the same manner as in simple <a href="/feedforward_neural_network.html">multilayer perceptron</a>s.
</p>

<p>
notice that here <img src="ltx/db4e3dd7802.svg" alt="\(K^{\ell-1}\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> also equals the number of units in the <img src="ltx/da14e46a009.svg" alt="\(l\)" style="height: 0.7060em; vertical-align: -0.0649em; display: inline-block" class="org-latex org-latex-inline" />-th layer.
</p>
</div>
</div>
<div id="outline-container-orgbaea14e" class="outline-2">
<h2 id="orgbaea14e">full example</h2>
<div class="outline-text-2" id="text-orgbaea14e">
<p>
consider the following network:
<img src="/net.png" />
let <img src="ltx/95083719d34.svg" alt="\(K^\ell_k\)" style="height: 1.2548em; vertical-align: -0.3289em; display: inline-block" class="org-latex org-latex-inline" /> denote the <img src="ltx/bb71ee7d056.svg" alt="\(k\)" style="height: 0.6991em; vertical-align: -0.0609em; display: inline-block" class="org-latex org-latex-inline" />th kernel of the <img src="ltx/b1c9236fece.svg" alt="\(\ell\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" />th layer whose dimensions are denoted by <img src="ltx/fda47a78901.svg" alt="\(D_K^\ell \times H_K^\ell \times W_K^\ell\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" /> (depth,height and width of the kernel tensor, respectively, although depth is irrelevant here), where <img src="ltx/149a0d4174c.svg" alt="\(1 \le k \le t^\ell\)" style="height: 1.1485em; vertical-align: -0.2226em; display: inline-block" class="org-latex org-latex-inline" />, such that <img src="ltx/c9c9f3b801c.svg" alt="\(t^\ell\)" style="height: 0.9996em; vertical-align: -0.0737em; display: inline-block" class="org-latex org-latex-inline" /> is the number of kernels in the <img src="ltx/b1c9236fece.svg" alt="\(\ell\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" />th layer, let <img src="ltx/7682e71d836.svg" alt="\(B^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> denote the tensor of weights of layer <img src="ltx/b1c9236fece.svg" alt="\(\ell\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" />, let <img src="ltx/2fb480524b0.svg" alt="\(I^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> denote the input to the <img src="ltx/b1c9236fece.svg" alt="\(\ell\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" />th layer, let <img src="ltx/7c52db4d64b.svg" alt="\(\hat Y^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> denote the output of the <img src="ltx/b1c9236fece.svg" alt="\(\ell\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" />th layer, let <img src="ltx/c370623764d.svg" alt="\(S^\ell\)" style="height: 0.9849em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /> denote the output before the activation function
(im using a different notation than in the image but it should be obvious which is which)
the parameters are
</p>
<ul class="org-ul">
<li><img src="ltx/9ec3adb7339.svg" alt="\(C1\)" style="height: 0.7324em; vertical-align: -0.0600em; display: inline-block" class="org-latex org-latex-inline" /> or <img src="ltx/e362cd58053.svg" alt="\(\ell=2\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" />, <img src="ltx/17f278cde0e.svg" alt="\(K_k^2\)" style="height: 1.2281em; vertical-align: -0.3242em; display: inline-block" class="org-latex org-latex-inline" /> of size <img src="ltx/75a50d5be00.svg" alt="\(5 \times 5\)" style="height: 0.7412em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" /> and <img src="ltx/752bd4f93e0.svg" alt="\(B^2\)" style="height: 0.9531em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> of size 6, <img src="ltx/bc986a2512b.svg" alt="\(1 \le k \le 6\)" style="height: 0.9000em; vertical-align: -0.2226em; display: inline-block" class="org-latex org-latex-inline" /></li>
<li><img src="ltx/8d246e695dc.svg" alt="\(C2\)" style="height: 0.7324em; vertical-align: -0.0600em; display: inline-block" class="org-latex org-latex-inline" /> or <img src="ltx/552eaa650f4.svg" alt="\(\ell=4\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" />, <img src="ltx/c99dec0172d.svg" alt="\(K_i^4\)" style="height: 1.2156em; vertical-align: -0.3148em; display: inline-block" class="org-latex org-latex-inline" /> of size <img src="ltx/0d05873a1cb.svg" alt="\(6 \times 5 \times 5\)" style="height: 0.7471em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" /> and <img src="ltx/1a32e64d1b1.svg" alt="\(B^4\)" style="height: 0.9500em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> of size 2, <img src="ltx/8e23a1d9f2f.svg" alt="\(1 \le k \le 12\)" style="height: 0.9000em; vertical-align: -0.2226em; display: inline-block" class="org-latex org-latex-inline" /> (size of the kernel is different from in the original figure)</li>
<li><img src="ltx/00b7d2f5770.svg" alt="\(FC\)" style="height: 0.7324em; vertical-align: -0.0600em; display: inline-block" class="org-latex org-latex-inline" /> or <img src="ltx/23ddaabdb96.svg" alt="\(\ell=6\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" />, <img src="ltx/0dbc0c2e856.svg" alt="\(W\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> of size <img src="ltx/c6c294f2174.svg" alt="\(10 \times 192\)" style="height: 0.7471em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" /> and <img src="ltx/2ca5eea3403.svg" alt="\(B^6\)" style="height: 0.9570em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> of size <img src="ltx/adaf1c6442e.svg" alt="\(10 \times 1\)" style="height: 0.7471em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" /></li>
</ul>
<p>
<b>feedforwarding for convolutional layer <img src="ltx/9ec3adb7339.svg" alt="\(C1\)" style="height: 0.7324em; vertical-align: -0.0600em; display: inline-block" class="org-latex org-latex-inline" /> or <img src="ltx/e362cd58053.svg" alt="\(\ell=2\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /></b>:
</p>

<div class="equation-container">
<span class="equation">
<img src="ltx/cf6ae37e815.svg" alt="\begin{align*}
  \hat Y_k^\ell &amp;amp;= \phi(I^\ell * K^\ell_k + B^\ell[k])\\
  \hat Y_k^\ell[i,j] &amp;amp;= \phi\left(\sum_{y=0}^{H_K^\ell-1} \sum_{x=0}^{W_K^\ell-1}I^\ell[y+i,x+j]K^\ell_k[H^\ell_K-y-1,W^\ell_K-x-1]+B^\ell[k]\right)
\end{align*}
" style="height: 6.1352em; vertical-align: -0.5392em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
where <img src="ltx/e362cd58053.svg" alt="\(\ell=2\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /> (i think its better to keep general notation than write 2 explicitly), <img src="ltx/bc986a2512b.svg" alt="\(1 \le k \le 6\)" style="height: 0.9000em; vertical-align: -0.2226em; display: inline-block" class="org-latex org-latex-inline" />, <img src="ltx/71f8e85a441.svg" alt="\(1 \le y \le H_I^\ell-H_K^\ell+1,1 \le x \le W_I^\ell-W_K^\ell+1\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" />, taken from this convolution example
here <img src="ltx/e2bb3056bcf.svg" alt="\(H_I^\ell-H_K^\ell+1=24,W_I^\ell-W_K^\ell+1=24\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" />, the dimensions of the output <img src="ltx/7c52db4d64b.svg" alt="\(\hat Y^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> are <img src="ltx/e9e312b5d60.svg" alt="\(6 \times 24 \times 24\)" style="height: 0.7471em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" />
<b>feedforwarding for average pooling layer <img src="ltx/ca7e40c8b7b.svg" alt="\(S1\)" style="height: 0.7305em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /> or <img src="ltx/c3a5537b743.svg" alt="\(\ell=3\)" style="height: 0.7481em; vertical-align: -0.0600em; display: inline-block" class="org-latex org-latex-inline" /></b>:
<img src="ltx/07d87034c71.svg" alt="\[ \hat Y^\ell_k[i,j] = \frac{1}{W_K^\ell H_K^\ell}\sum_{x=iW_K^\ell}^{(i+1)W_K^\ell}\sum_{y=jH_K^\ell}^{(j+1)H_K^\ell} I_k^\ell[x,y] \]" style="height: 3.8341em; display: block" class="org-latex org-latex-block" />
in words, we iterate through each block of size <img src="ltx/b315303e245.svg" alt="\(W_K^\ell H_K^\ell\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" /> (in each kernel) and pool it into a single pixel, here the size of the pooling kernels is <img src="ltx/c2d28aeb12e.svg" alt="\(2 \times 2\)" style="height: 0.7422em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" /> and the result is of dimensions <img src="ltx/c70ed74c891.svg" alt="\(6 \times 12 \times 12\)" style="height: 0.7471em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" />
<b>feedforwarding for convolutional layer <img src="ltx/8d246e695dc.svg" alt="\(C2\)" style="height: 0.7324em; vertical-align: -0.0600em; display: inline-block" class="org-latex org-latex-inline" /> or <img src="ltx/552eaa650f4.svg" alt="\(\ell=4\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /></b>:
</p>

<div class="equation-container">
<span class="equation">
<img src="ltx/7ff4fde4539.svg" alt="\begin{align*}
  \hat Y_k^\ell &amp;amp;= \phi(I^\ell*K_k^\ell+B^\ell[k])\\
  \hat Y_k^\ell[i,j] &amp;amp;= \phi\left(\sum_{z=0}^{D_K^\ell-1}\sum_{y=0}^{H_K^\ell-1}\sum_{x=0}^{W_K^\ell-1}I^\ell[z,y+i,x+j]K^\ell_k[D_K^\ell-z-1,H_K^\ell-y-1,W_K^\ell-x-1]+B^\ell[k]\right)
\end{align*}
" style="height: 6.1352em; vertical-align: -0.4020em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
where <img src="ltx/5a085348b41.svg" alt="\(1 \le i \le 8,1 \le j \le 8,1 \le k \le 12\)" style="height: 0.9363em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" />, output is of dimensions <img src="ltx/423ed2498fc.svg" alt="\(12 \times 8 \times 8\)" style="height: 0.7442em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" />, we use <img src="ltx/f67193b3c45.svg" alt="\(z+1\)" style="height: 0.8275em; vertical-align: -0.1678em; display: inline-block" class="org-latex org-latex-inline" /> for the depth of the image because in truth what we're finding is <img src="ltx/a7f85ce233b.svg" alt="\(\hat Y_K^\ell[1,i,j]\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" /> which is equal to <img src="ltx/b1c1cbbaebe.svg" alt="\(\hat Y_k^\ell[i,j]\)" style="height: 1.2548em; vertical-align: -0.3289em; display: inline-block" class="org-latex org-latex-inline" /> but we omit an index of <img src="ltx/3b61f59503f.svg" alt="\(1\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> because its "implied" when its to the left
</p>
<div class="math-block note" data-before="note" data-after="" id="orgb6356dd">
<p>
we use <img src="ltx/9891a5097f1.svg" alt="\(z\)" style="height: 0.6217em; vertical-align: -0.1678em; display: inline-block" class="org-latex org-latex-inline" /> as the index to <img src="ltx/2fb480524b0.svg" alt="\(I^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> because originally it would've been some variable index, but the result of the convolution is a tensor of order 2 (because the operands of the convolution have the same depth), but originally its meant ot be of order 3 (operands are of order 3), so we are "redimensioning" the result implicitly, if we were to use a variable, e.g. <img src="ltx/32022fcd4b7.svg" alt="\(\hat Y^\ell_k[r,i,j]\)" style="height: 1.2548em; vertical-align: -0.3289em; display: inline-block" class="org-latex org-latex-inline" />, <img src="ltx/c7b35775fb5.svg" alt="\(r\)" style="height: 0.4953em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> would turn out to equal just 0, because substituting <img src="ltx/0d6bf72131d.svg" alt="\(r&amp;gt;0\)" style="height: 0.7628em; vertical-align: -0.0972em; display: inline-block" class="org-latex org-latex-inline" /> would result in an "overflow" in the summation, as when <img src="ltx/2e2457dbbbc.svg" alt="\(z=D_k^\ell-1\)" style="height: 1.2548em; vertical-align: -0.3289em; display: inline-block" class="org-latex org-latex-inline" />, we get <img src="ltx/c0054233176.svg" alt="\(I^\ell[z+r,y+i,x+j]=\vec{v}[D_K^\ell,y+i,x+j]\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" />, which isnt defined (max index is <img src="ltx/ee2d79d03a6.svg" alt="\(D_K^\ell-1\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" /> not <img src="ltx/4301000aad1.svg" alt="\(D_K^\ell\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" />), therefore <img src="ltx/776c99f2e95.svg" alt="\(Y_k^\ell[1,i,j]\)" style="height: 1.2548em; vertical-align: -0.3289em; display: inline-block" class="org-latex org-latex-inline" /> isnt defined and <img src="ltx/ef832d06693.svg" alt="\(r=0\)" style="height: 0.7197em; vertical-align: -0.0541em; display: inline-block" class="org-latex org-latex-inline" /> in all cases, so we might aswell just write 0 or not consider it at all since adding 0 wouldnt affect a number.
</p>

</div>
<p>
<b>feedforwarding for average pooling layer <img src="ltx/7aba9a30be6.svg" alt="\(S2\)" style="height: 0.7305em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /> or <img src="ltx/5d83d2253fd.svg" alt="\(\ell=5\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /></b>:
<img src="ltx/07d87034c71.svg" alt="\[ \hat Y^\ell_k[i,j] = \frac{1}{W_K^\ell H_K^\ell}\sum_{x=iW_K^\ell}^{(i+1)W_K^\ell}\sum_{y=jH_K^\ell}^{(j+1)H_K^\ell} I_k^\ell[x,y] \]" style="height: 3.8341em; display: block" class="org-latex org-latex-block" />
here the size of the kernels is <img src="ltx/c2d28aeb12e.svg" alt="\(2 \times 2\)" style="height: 0.7422em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" /> aswell, the output is of dimensions <img src="ltx/28bfe374f7a.svg" alt="\(12 \times 4 \times 4\)" style="height: 0.7422em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" />
<b>vectorization before fully connected layer <img src="ltx/23ddaabdb96.svg" alt="\(\ell=6\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /></b> (may have its own layer):
<img src="ltx/4db75d0c3e0.svg" alt="\[ \vec{v} = (I^\ell[0,0,0], I^\ell[0,0,1], \dots, I^\ell[0,0,W_I^\ell], I^\ell[0,1,0], I^\ell[0,1,1], \dots, I^\ell[D_I^\ell,H_I^\ell,W_I^\ell])^\intercal \]" style="height: 1.5619em; display: block" class="org-latex org-latex-block" />
the output is of size <img src="ltx/6dfa269a722.svg" alt="\(12 \times 4 \times 4 = 192\)" style="height: 0.7422em; vertical-align: -0.0815em; display: inline-block" class="org-latex org-latex-inline" />
<b>feedforwarding for fully connected layer <img src="ltx/00b7d2f5770.svg" alt="\(FC\)" style="height: 0.7324em; vertical-align: -0.0600em; display: inline-block" class="org-latex org-latex-inline" /> or <img src="ltx/23ddaabdb96.svg" alt="\(\ell=6\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /></b>:
</p>
<div class="math-block note" data-before="note" data-after="" id="org89820f7">
<p>
in the image, the output layer and the last fully connected layer are treated as separate layers, this makes little sense because that way the fully connected layers would have no parameters, here we treat them as a single fully connected layer
</p>

</div>

<div class="equation-container">
<span class="equation">
<img src="ltx/247396f1e2b.svg" alt="\begin{align*}
  \hat Y^\ell[k] &amp;amp;= \phi(\vec{v}*K_k^\ell+B^\ell[k])\\
  &amp;amp;= \phi\left(\sum_{x=0}^{W_K^\ell-1}\vec{v}[x]K_k^\ell[W_K^\ell-x-1]+B^\ell[k]\right)
\end{align*}
" style="height: 6.1352em; vertical-align: -0.5392em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
see <a href="/convolutional_neural_network.html">not-convnet-I-drop-index</a> as to why i wrote <img src="ltx/b5a4e634d54.svg" alt="\(x+1\)" style="height: 0.8020em; vertical-align: -0.1423em; display: inline-block" class="org-latex org-latex-inline" />
let <img src="ltx/e7f4dc52977.svg" alt="\(flip(A)\)" style="height: 0.9627em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" /> denote the flipping of a matrix <img src="ltx/683dde0b47f.svg" alt="\(A\)" style="height: 0.7187em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> over both axes so that the indicies may be reversed, i.e. <img src="ltx/b6a0c26ea44.svg" alt="\(flip(A[i,j])=A[-i,-j]\)" style="height: 0.9813em; vertical-align: -0.2814em; display: inline-block" class="org-latex org-latex-inline" /> (a negative index implies an index that is subtracted from the size of the dimension along with the value 1). we can show that this layer resembles the usual fully connected layer from simple multilayer perceptrons by turning the convolution into matrix multiplication:
</p>

<div class="equation-container">
<span class="equation">
<img src="ltx/c847b544498.svg" alt="\begin{align*}
  \hat Y^\ell[k] &amp;amp;= \phi\left(\sum_{x=0}^{W_K^\ell-1}\vec{v}[x]K_k^\ell[W_K^\ell-x-1]+B^\ell[k]\right)\\
  &amp;amp;= \phi\left(\sum_{x=0}^{W_K^\ell-1}K_k^\ell[W_K^\ell-x-1]\vec{v}[x]+B^\ell[k]\right)\\
  &amp;amp;= \phi\left(\sum_{x=1}^{W_K^\ell}K_k^\ell[W_K^\ell-x]\vec{v}[x-1]+B^\ell[k]\right)\\
  &amp;amp;= \phi\left(\sum_{x=1}^{W_K^\ell}flip(K^\ell)[k,x-1]\vec{v}[x-1]+B^\ell[k]\right)
\end{align*}
" style="height: 17.8364em; vertical-align: -0.5392em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
where <img src="ltx/fbd414f5681.svg" alt="\(1 \le k &amp;lt; 10\)" style="height: 0.9000em; vertical-align: -0.2226em; display: inline-block" class="org-latex org-latex-inline" /> (layer contains 10 fully connected kernels), the output is of size 10, here <img src="ltx/85bd990149f.svg" alt="\(W_I^\ell=192\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" /> and the size of each kernel is <img src="ltx/4f54d28408e.svg" alt="\(192\)" style="height: 0.7207em; vertical-align: -0.0600em; display: inline-block" class="org-latex org-latex-inline" /> (<img src="ltx/0cc8e11d69e.svg" alt="\(W_K^\ell=192,H_K^\ell=1,D_K^\ell=1\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" />)
in vector notation, we write (notice that <img src="ltx/f9c88eb42fb.svg" alt="\(flip(K^\ell)\vec{v}\)" style="height: 1.2005em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" /> results in a vector):
<img src="ltx/c1d06814d9c.svg" alt="\[ \hat Y^\ell = \phi(flip(K^\ell)\vec{v}+B^\ell) \]" style="height: 1.4998em; display: block" class="org-latex org-latex-block" />
at this point we'd have arrived at the output layer and the output of the neural network would be <img src="ltx/322c38551d8.svg" alt="\(\hat Y^6\)" style="height: 0.9570em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> or simply <img src="ltx/c4b1fb057e8.svg" alt="\(\hat Y\)" style="height: 0.9078em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> (the output of the last fully connected layer which itself is the output layer), assuming the quadratic loss function, and assuming the desired output is <img src="ltx/6030eebe277.svg" alt="\(\vec{y}\)" style="height: 0.9451em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" />, the loss function would be defined as:
<img src="ltx/2b4e3b5dd38.svg" alt="\[ L = \frac{1}{2}||\hat Y-\vec{y}||^2 = \frac{1}{2}\sum_{i=1}^{10} (\hat Y[i]-\vec{y}[i])^2 \]" style="height: 2.8292em; display: block" class="org-latex org-latex-block" />
now we can begin to backpropagate the error, <img src="ltx/98d854999f7.svg" alt="\(\Delta\)" style="height: 0.7187em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> followed by an object denotes the error at that specific object or point:
<b>backpropagation for fully connected layer <img src="ltx/00b7d2f5770.svg" alt="\(FC\)" style="height: 0.7324em; vertical-align: -0.0600em; display: inline-block" class="org-latex org-latex-inline" /> or <img src="ltx/23ddaabdb96.svg" alt="\(\ell=6\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /></b>:
</p>

<div class="equation-container">
<span class="equation">
<img src="ltx/9bc2ba31dc2.svg" alt="\begin{align*}
  \Delta K^\ell_k[i] &amp;amp;= \frac{\partial L}{\partial K^\ell_k[i]}\\
  &amp;amp;= \frac{\partial L}{\partial \hat Y^\ell[k]}\frac{\partial \vec{y}[k]}{\partial K^\ell_k[i]}\\
  &amp;amp;= (\hat Y^\ell[k]-\vec{y}[k])\frac{\partial}{\partial K^\ell_k[i]}\phi\left(\sum_{x=0}^{W_K^\ell-1}\vec{v}[x]K_k^\ell[W_K^\ell-x-1]+B^\ell[k]\right)\\
  &amp;amp;= (\hat Y^\ell[k]-\vec{y}[k])\phi'\left(\sum_{x=0}^{W_K^\ell-1}\vec{v}[x]K_k^\ell[W_K^\ell-x-1]+B^\ell[k]\right)\vec{v}[W_K^\ell-i-1]\\
  &amp;amp;= (\hat Y^\ell[k]-\vec{y}[k])\phi'(S^\ell[k])\vec{v}[W_K^\ell-i-1]
\end{align*}
" style="height: 15.7922em; vertical-align: -0.4020em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
where <img src="ltx/39578ab3e68.svg" alt="\(1 \le i &amp;lt; W_K^\ell, 1 \le k &amp;lt; t^\ell\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" />, here <img src="ltx/03cca2ba8f1.svg" alt="\(W_K^\ell=120,t^\ell=10\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" />, notice that <img src="ltx/774f1c51de0.svg" alt="\(\vec{v}[k]=\phi(S^\ell[k])\)" style="height: 1.2073em; vertical-align: -0.2814em; display: inline-block" class="org-latex org-latex-inline" />
let <img src="ltx/3974612cff1.svg" alt="\(\Delta S^\ell[k]=(\hat Y^\ell[k]-\vec{y}[k])\phi'(S[k])\)" style="height: 1.2073em; vertical-align: -0.2814em; display: inline-block" class="org-latex org-latex-inline" />, whose size would be 10, then:
</p>

<div class="equation-container">
<span class="equation">
<img src="ltx/780aa038d3b.svg" alt="\begin{align*}
  \Delta K^\ell_k[i] &amp;amp;= \Delta S^\ell[k]\vec{v}[W_K^\ell-i-1]\\
  &amp;amp;= \Delta S^\ell[k]flip(\vec{v})[i]\\
  &amp;amp;= \Delta S^\ell[k]flip(\vec{v})[i]\\
  \implies \Delta K^\ell &amp;amp;= \Delta S^\ell \times flip(\vec{v})^T
\end{align*}
" style="height: 6.4917em; vertical-align: -0.4020em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
we arrived at an equation in matrix notation which is convenient (<img src="ltx/0c0e30cc3f5.svg" alt="\(\vec{v}\)" style="height: 0.7324em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> was a column vector initially)
<b>backpropagation through flatten layer before <img src="ltx/00b7d2f5770.svg" alt="\(FC\)" style="height: 0.7324em; vertical-align: -0.0600em; display: inline-block" class="org-latex org-latex-inline" /></b>
recall that during feedforwarding, the flatten layer turned the tensor of feature maps which is of the third order into a single vector, the deltas defined by the layer during backpropagation is a vector aswell, which we need to turn them into a tensor of the third order that resembles the delta at each entry in the tensor of the feature maps
first we find the delta at each entry in the vector:
</p>

<div class="equation-container">
<span class="equation">
<img src="ltx/e4621209388.svg" alt="\begin{align*}
  \Delta\vec{v}[j] &amp;amp;= \frac{\partial L}{\partial \vec{v}[j]}\\
  &amp;amp;= \sum_{k=0}^{t^\ell} \frac{\partial L}{\partial \vec{y}[k]}\frac{\partial \vec{y}[k]}{\partial \vec{v}[j]}\\
  &amp;amp;= \sum_{k=0}^{t^\ell} (\hat Y^\ell[k]-\vec{y}[k])\frac{\partial}{\partial\vec{v}[j]}\phi\left(\sum_{x=0}^{W_K^\ell-1}\vec{v}[x]K_k^\ell[W_K^\ell-x-1]+B^\ell[k]\right)\\
  &amp;amp;= \sum_{k=0}^{t^\ell} (\hat Y^\ell[k]-\vec{y}[k])\phi'(S^\ell[k])\frac{\partial}{\partial\vec{v}[j]}\sum_{x=0}^{W_K^\ell-1}\vec{v}[x]K_k^\ell[W_K^\ell-x-1]+B^\ell[k]\\
  &amp;amp;= \sum_{k=0}^{t^\ell} (\hat Y^\ell[k]-\vec{y}[k])\phi'(S^\ell[k])K^\ell_k[W_K^\ell-j-1]\\
  &amp;amp;= \sum_{k=0}^{t^\ell} \Delta S^\ell[k]K^\ell_k[W_K^\ell-j-1]\\
  &amp;amp;= \sum_{k=0}^{t^\ell} \Delta S^\ell[k]flip(K^\ell_k)[j]\\
  \implies \Delta \vec{v} &amp;amp;= \Delta S^\ell \times flip(K_k^\ell)^T
\end{align*}
" style="height: 24.9848em; vertical-align: -0.4020em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
notice that <img src="ltx/85bd990149f.svg" alt="\(W_I^\ell=192\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" />
where <img src="ltx/e2c3c42417b.svg" alt="\(0 \le j &amp;lt; 192\)" style="height: 0.9363em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" /> because 192 is the total size of the tensor received from <img src="ltx/5d83d2253fd.svg" alt="\(\ell=5\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /> during feedforwarding
now the vector of deltas <img src="ltx/abd1c5835db.svg" alt="\(\Delta \vec{v}\)" style="height: 0.7324em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> needs to be reshaped into a tensor of the third order so that we may then send it to the next layer in the process, <img src="ltx/5d83d2253fd.svg" alt="\(\ell=5\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" />, which is a pooling layer
the reshaping should be the exact inverse of the row-major vectorization we did during feedforwarding
<img src="ltx/11293f47c51.svg" alt="\[ \Delta I^{\ell}\left[\floor*{(i+1)/D_I^\ell H_I^\ell W_I^\ell},\floor*{((i+1)\bmod D_I^\ell H_I^\ell W_I^\ell)/H_I^\ell W_I^\ell},(i+1) \bmod H_I^\ell W_I^\ell\right] = \Delta\vec{v}[i] \]" style="height: 2.5425em; display: block" class="org-latex org-latex-block" />
such that <img src="ltx/58fd283fc2d.svg" alt="\(0 \le i &amp;lt; W_I^\ell H_I^\ell D_I^\ell\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" />, here <img src="ltx/2999caf4602.svg" alt="\(W_I^\ell H_I^\ell D_I^\ell=192\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" />, note that <img src="ltx/6da9be703ba.svg" alt="\(\Delta\hat Y^{\ell-1}=\Delta I^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />
<b>backpropagation through average pooling layer <img src="ltx/5d83d2253fd.svg" alt="\(\ell=5\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /></b>:
recall that this layer downscaled the input given to it during feedforwarding from its preceding layer <img src="ltx/552eaa650f4.svg" alt="\(\ell=4\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" />, so during backpropagation, when it responds to the next layer which is the one it got the initial message from during feedforwarding it needs to scale its message back up to the dimensions of the input it got from that layer, that "message" would be the deltas:
<img src="ltx/2cd3be12c8b.svg" alt="\[ \Delta \hat Y^{\ell-1}[k,i,j] = \frac{1}{H_K^\ell W_K^\ell}\Delta \hat Y^\ell\left[k,\ceil*{(i+1)/H_K^\ell},\ceil*{(j+1)/W_K^\ell}\right] \quad 0 \le i &amp;lt; H_I^\ell,0 \le j &amp;lt; W_I^\ell,0 \le k &amp;lt; t^\ell \]" style="height: 2.6469em; display: block" class="org-latex org-latex-block" />
<b>backpropagation through convolutional layer <img src="ltx/552eaa650f4.svg" alt="\(\ell=4\)" style="height: 0.7471em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /></b>:
</p>

<div id="org1ede887" class="equation-container">
<span class="equation">
<img src="ltx/83ba40c54ce.svg" alt="\begin{align*}
  \Delta K^\ell_k[r,u,v] &amp;amp;= \frac{\partial L}{\partial K_k^\ell[r,u,v]}\\
  &amp;amp;= \sum_{i=0}^{H_I^{\ell+0}-1} \sum_{j=1}^{W_I^{\ell+0}-1} \frac{\partial L}{\partial \hat Y_k^\ell[i,j]}\frac{\partial \hat Y_k^\ell[i,j]}{\partial K_k^\ell[r,u,v]}\\
  &amp;amp;= \sum_{i=0}^{H_I^{\ell+1}-1} \sum_{j=0}^{W_I^{\ell+1}-1} \Delta\hat Y_k^\ell[i,j]\frac{\partial}{\partial K_k^\ell[r,u,v]}\phi\left(\sum_{z=0}^{D_K^\ell-1}\sum_{y=0}^{H_K^\ell-1}\sum_{x=0}^{W_K^\ell-1}I^\ell[z,y+i,x+j]K^\ell_k[D_K^\ell-z-1,H_K^\ell-y-1,W_K^\ell-x-1]+B^\ell[k]\right)\\
  &amp;amp;= \sum_{i=0}^{H_I^{\ell+1}-1} \sum_{j=0}^{W_I^{\ell+1}-1} \Delta\hat Y_k^\ell[i,j]\phi'(S^\ell_k[i,j])I^\ell[D_K^\ell-r-1,H_K^\ell-u-1+i,W_K^\ell-v-1+j]
\end{align*}
" style="height: 14.4451em; vertical-align: -0.4020em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
where <img src="ltx/b59f0db99d7.svg" alt="\(0 \le k &amp;lt; t^\ell,0 \le r &amp;lt; D_K^\ell,0 \le u &amp;lt; H_K^\ell,0 \le v &amp;lt; W_K^\ell\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" />
see <a href="/convolutional_neural_network.html">not-convnet-I-indicies</a> about the indicies of <img src="ltx/d59e21aa26f.svg" alt="\(I\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> (the note talks about the indicies of <img src="ltx/2aac24b1aeb.svg" alt="\(K\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> but the same fact applies to <img src="ltx/d59e21aa26f.svg" alt="\(I\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> here)
let <img src="ltx/308e79a5ca8.svg" alt="\(\Delta S_{k}^\ell\)" style="height: 1.2548em; vertical-align: -0.3289em; display: inline-block" class="org-latex org-latex-inline" /> be the error after (after in terms of backpropagation, before in terms of feedforwarding) the activation function:
<img src="ltx/290c019533a.svg" alt="\[ \Delta S_k^\ell[i,j] = \Delta \hat Y_k^\ell[i,j]\phi'(S_k^\ell[i,j]) \]" style="height: 1.5541em; display: block" class="org-latex org-latex-block" />
then:
<img src="ltx/8c4a4a180cd.svg" alt="\[ \Delta K^\ell_k[r,u,v] = \sum_{i=0}^{H_I^{\ell+1}-1} \sum_{j=0}^{W_I^{\ell+1}-1} \Delta S_k^\ell[i,j]I^\ell[D_K^\ell-r-1,H_K^\ell-u-1+i,W_K^\ell-v-1+j] \]" style="height: 3.5464em; display: block" class="org-latex org-latex-block" />
now for the bias:
</p>

<div class="equation-container">
<span class="equation">
<img src="ltx/e996c8f593a.svg" alt="\begin{align*}
  \Delta B^\ell[k] &amp;amp;= \frac{\partial L}{\partial B^\ell[k]}\\
  &amp;amp;= \sum_{i=0}^{H_I^{\ell+1}-1} \sum_{j=0}^{W_I^{\ell+1}-1} \frac{\partial L}{\partial \hat Y_k^\ell[i,j]}\frac{\partial \hat Y_k^\ell[i,j]}{\partial B^\ell[k]}\\
  &amp;amp;= \sum_{i=0}^{H_I^{\ell+1}-1} \sum_{j=0}^{W_I^{\ell+1}-1} \Delta\hat Y_k^\ell[i,j]\frac{\partial}{\partial B^\ell[k]}\phi\left(\sum_{z=0}^{D_K^\ell-1}\sum_{y=0}^{H_K^\ell-1}\sum_{x=0}^{W_K^\ell-1}I^\ell[z,y+i,x+j]K^\ell_k[D_K^\ell-z-1,H_K^\ell-y-1,W_K^\ell-x-1]+B^\ell[k]\right)\\
  &amp;amp;= \sum_{i=0}^{H_I^{\ell+1}-1} \sum_{j=0}^{W_I^{\ell+1}-1} \Delta\hat Y_k^\ell[i,j]\phi'(S_k^\ell[i,j])\\
  &amp;amp;= \sum_{i=0}^{H_I^{\ell+1}-1} \sum_{j=0}^{W_I^{\ell+1}-1} \Delta S^\ell_k[i,j]
\end{align*}
" style="height: 18.0621em; vertical-align: -0.4020em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
now we find the deltas at <img src="ltx/2fb480524b0.svg" alt="\(I^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> or <img src="ltx/0b3de44475e.svg" alt="\(\hat Y^{\ell-1}\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />, because thats what the next layer in backpropagation, <img src="ltx/c3a5537b743.svg" alt="\(\ell=3\)" style="height: 0.7481em; vertical-align: -0.0600em; display: inline-block" class="org-latex org-latex-inline" />, needs
</p>

<div id="orgba272b7" class="equation-container">
<span class="equation">
<img src="ltx/f1ba296ec5b.svg" alt="\begin{align}
  \Delta I^\ell[r,u,v] &amp;amp;= \frac{\partial L}{\partial I^\ell[r,u,v]}\\
  &amp;amp;= \sum_{k=0}^{D_I^{\ell+1}-1} \sum_{i=0}^{H_I^{\ell+1}-1} \sum_{j=0}^{W_I^{\ell+1}-1} \frac{\partial L}{\partial \hat Y_k^\ell[i,j]}\frac{\partial \hat Y_k^\ell[i,j]}{\partial I^\ell[r,u,v]}\\
  &amp;amp;= \sum_{k=0}^{D_I^{\ell+1}-1} \sum_{i=0}^{H_I^{\ell+1}-1} \sum_{j=0}^{W_I^{\ell+1}-1} \Delta\hat Y_k^\ell[i,j]\frac{\partial}{\partial I^\ell[r,u,v]}\phi\left(\sum_{z=0}^{D_K^\ell-1}\sum_{y=0}^{H_K^\ell-1}\sum_{x=0}^{W_K^\ell-1}I^\ell[z,y+i,x+j]K^\ell_k[D_K^\ell-z-1,H_K^\ell-y-1,W_K^\ell-x-1]+B^\ell[k]\right)\\
  &amp;amp;= \sum_{k=0}^{D_I^{\ell+1}-1} \sum_{i=0}^{H_I^{\ell+1}-1} \sum_{j=0}^{W_I^{\ell+1}-1} \Delta\hat Y_k^\ell[i,j]\phi'(S_k^\ell[i,j])K_k^\ell[D_K^\ell-r-1,H_K^\ell-u-1+i,W_K^\ell-v-1+j]\\
  &amp;amp;= \sum_{k=0}^{D_I^{\ell+1}-1} \sum_{i=0}^{H_I^{\ell+1}-1} \sum_{j=0}^{W_I^{\ell+1}-1} \Delta S_k^\ell[i,j] K_k^\ell[\underbrace{D_K^\ell-r-1,H_K^\ell-u-1+i,W_K^\ell-v-1+j}_{(D^\ell_K,H_K^\ell,W_K^\ell)-(r,u,v)-(1,1,1)+(0,i,j)}] \label{eq-convnet-1}
\end{align}
" style="height: 22.5451em; vertical-align: -0.4020em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
such that <img src="ltx/51af800d857.svg" alt="\(0 \le r &amp;lt; D_I^\ell, 0 \le u &amp;lt; H_I^\ell, 0 \le v &amp;lt; W_I^\ell\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" />. here, <img src="ltx/59b66a5b325.svg" alt="\(\Delta I^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> are the deltas that need to be passed onto the next layer <img src="ltx/c3a5537b743.svg" alt="\(\ell=3\)" style="height: 0.7481em; vertical-align: -0.0600em; display: inline-block" class="org-latex org-latex-inline" />
</p>
<div class="math-block note" data-before="note" data-after="" id="org5e38a8d">
<p>
its important to notice how in <a href="/convolutional_neural_network.html">eq-convnet-1</a> the indicies <img src="ltx/78320534b1d.svg" alt="\(r,u,v\)" style="height: 0.6060em; vertical-align: -0.1599em; display: inline-block" class="org-latex org-latex-inline" />, which are indexing <img src="ltx/2fb480524b0.svg" alt="\(I^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> are each subtracted from the dimensions of the kernels and then the indicies <img src="ltx/c3f9097fd94.svg" alt="\(0,i,j\)" style="height: 0.9245em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" /> which are indexing <img src="ltx/e44dab6c686.svg" alt="\(S^\ell_k\)" style="height: 1.2548em; vertical-align: -0.3289em; display: inline-block" class="org-latex org-latex-inline" /> are added element-wise aswell, this is important as its a property that is later exploited in the code for backpropagation
i should also note that the process of backpropagation in a convolutional layer, just like the feedforwarding process, resembles that of a convolutional one, i have yet to find out how to show that this follows from these math formulas
furthermore, here the expression <a href="/convolutional_neural_network.html">eq-convnet-1</a> would be correct only when the indicies of <img src="ltx/2aac24b1aeb.svg" alt="\(K\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> are in the correct range, so not less than 0 and not bigger than the size of the dimension theyre in, i think this happens because a single <img src="ltx/09f2be6a000.svg" alt="\(\Delta I[,,,]\)" style="height: 0.9813em; vertical-align: -0.2814em; display: inline-block" class="org-latex org-latex-inline" /> doesnt affect all entries in the output but we're differentiating with respect to all entries in the output, perhaps in the future i'll improve upon these formulas
</p>

</div>
</div>
</div>
<div id="outline-container-org53c4d58" class="outline-2">
<h2 id="org53c4d58">architecture</h2>
<div class="outline-text-2" id="text-org53c4d58">
</div>
<div id="outline-container-orgcb22607" class="outline-3">
<h3 id="orgcb22607">LeNet-5</h3>
<div class="outline-text-3" id="text-orgcb22607">
<p>
the <b>lenet-5</b> one of the earliest and most basic CNN architecture.
</p>

<p>
it consists of 7 layers. the first layer consists of an input image with dimensions of 32x32. it is convolved with 6 filters of size 5x5 resulting in dimension of 28x28x6. the second layer is a pooling operation with a filter of size 2×2 and stride of 2. hence the resulting image dimension will be 14x14x6.
</p>

<p>
similarly, the third layer also involves in a convolution operation with 16 filters of size 5x5 followed by a fourth pooling layer with similar filter size of 2x2 and stride of 2. thus, the resulting image dimension will be reduced to 5x5x16.
</p>

<p>
once the image dimension is reduced, the fifth layer is a fully connected convolutional layer with 120 filters each of size 5×5. in this layer, each of the 120 units in this layer will be connected to the 400 (5x5x16) units from the previous layers. the sixth layer is also a fully connected layer with 84 units.
</p>

<p>
the final seventh layer will be a softmax output layer with <img src="ltx/ddcbd3a127e.svg" alt="\(n\)" style="height: 0.4982em; vertical-align: -0.0541em; display: inline-block" class="org-latex org-latex-inline" /> possible classes depending upon the number of classes in the dataset.
</p>


<div id="org7786527" class="figure">
<p><img src="/lenet5.png" />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orga8e2315" class="outline-2">
<h2 id="orga8e2315">common lisp implementation</h2>
<div class="outline-text-2" id="text-orga8e2315">
</div>
<div id="outline-container-orgb274039" class="outline-3">
<h3 id="orgb274039">an abstract structure for networks</h3>
<div class="outline-text-3" id="text-orgb274039">
<p>
after having studied cnn's, i realied i need a more flexible data structure for networks than what i had in <a href="/feedforward_neural_network.html">simple feedforward networks</a>, because with simple perceptrons i only used one type of layer, with cnn's we need to be able to build networks with arbitrary layers, so i dropped my previous progress and started over:
</p>
<div class="org-src-container">
<pre class="src src-lisp" id="org0a2d09a">(<span style="font-weight: bold;">defgeneric</span> <span style="font-weight: bold;">feedforward</span> (layer x)
  (<span style="font-weight: bold;">:documentation</span> <span style="font-style: italic;">"feed-forward, return activations to pass to next layer"</span>))

(<span style="font-weight: bold;">defgeneric</span> <span style="font-weight: bold;">propbackward</span> (layer layer-x layer-y layer-y-unactivated propped-deltas learning-rate)
  (<span style="font-weight: bold;">:documentation</span> <span style="font-style: italic;">"backward-popagation, return gradients to pass to preceding layer"</span>))

(<span style="font-weight: bold;">defclass</span> <span style="font-weight: bold; text-decoration: underline;">layer</span> ()
  ((weights <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:weights</span> <span style="font-weight: bold;">:initform</span> nil <span style="font-weight: bold;">:accessor</span> layer-weights) <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">weights going into the layer's units</span>
   (biases <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:biases</span> <span style="font-weight: bold;">:initform</span> nil <span style="font-weight: bold;">:accessor</span> layer-biases)
   <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">per-layer activation functions (same activation function for all units in the layer)</span>
   (activation-function <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:activation-function</span> <span style="font-weight: bold;">:initform</span> #'relu <span style="font-weight: bold;">:accessor</span> layer-activation-function)
   (activation-function-derivative <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:activation-function-derivative</span> <span style="font-weight: bold;">:initform</span> #'relu-derivative <span style="font-weight: bold;">:accessor</span> layer-activation-function-derivative)
   <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">use tensor-activation-function and tensor-activation-function-derivative only if you want per-unit special activation functions, they take the indicies list (multidimensional index) of a unit and return a function to use for activation</span>
   (tensor-activation-function <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:tensor-activation-function</span> <span style="font-weight: bold;">:accessor</span> layer-tensor-activation-function)
   (tensor-activation-function-derivative <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:tensor-activation-function-derivative</span> <span style="font-weight: bold;">:accessor</span> layer-tensor-activation-function-derivative)))

(<span style="font-weight: bold;">defclass</span> <span style="font-weight: bold; text-decoration: underline;">network</span> ()
  ((layers <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:layers</span> <span style="font-weight: bold;">:initform</span> nil <span style="font-weight: bold;">:accessor</span> network-layers)
   (learning-rate <span style="font-weight: bold;">:initform</span> 0.0005 <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:learning-rate</span> <span style="font-weight: bold;">:accessor</span> network-learning-rate)))

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">make-network</span> (<span style="font-weight: bold; text-decoration: underline;">&amp;key</span> learning-rate layers)
  (make-instance
   'network
   <span style="font-weight: bold;">:layers</span> layers
   <span style="font-weight: bold;">:learning-rate</span> (or learning-rate 0.0005)))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">network-feedforward</span> ((n network) x)
  <span style="font-style: italic;">"x is a tensor, feedforward to the last layer and return its output"</span>
  (<span style="font-weight: bold;">with-slots</span> (layers) n
    <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">last-out is needed to keep the output of the lastly feedforwarded layer, the output of the layers is pushed onto the list which means they are stored in reverse order, to be later popped also in "reverse" order for backprop, this means that the first entry in out-list corresponds to the output layer, this should be better than storing them in normal order because we use a list and later we need to iterate from the output of the last layer to the first layer, each entry in the list is a cons with the first element as the activated output and second as the unactivated output</span>
    (<span style="font-weight: bold;">let</span> ((last-out x)
          (out-list nil))
      (<span style="font-weight: bold;">loop</span> for layer in layers
            do (<span style="font-weight: bold;">multiple-value-bind</span> (new-out new-out-unactivated)
                   (feedforward layer last-out)
                 (push (cons new-out new-out-unactivated) out-list)
                 (setf last-out new-out)))
      out-list)))
</pre>
</div>
</div>
</div>
<div id="outline-container-orgf805f80" class="outline-3">
<h3 id="orgf805f80">convolutional layer</h3>
<div class="outline-text-3" id="text-orgf805f80">
<p>
we use the relu by default to reduce the risk of a vanishing gradient
</p>
<div class="math-block note" data-before="note" data-after="" id="org94b0e69">
<p>
i should note that later on, during the writing of <code>propbackward</code> for convolutional layers, i realized that i needed some more data from <code>feedforward</code> than im currently returning, so <code>feedforward</code> functions needed some slight modifications which also meant i needed to rewrite the <code>network-feedforward</code> function
i needed more data like the unactivated output tensor of a layer, not just the final output of the layer, and the input to the layer
it also happened to me during the previous implementation in <a href="/feedforward_neural_network.html">feedforward neural network</a>, but it seems i didnt learn from my first encounter with that brickwall, lol.
</p>

</div>
<p>
but which weights should a <code>layer</code> object store? the weights that go into it, i.e. the layers that come out of the previous layer and land on <i>this</i> layer? or should it store the weights that are represented by the connections that come out of it and reach for the next layer in the network? at first it seemed the latter made more sense, but thats not the case, lets reconsider the equation for a neuron's activation <a href="/feedforward_neural_network.html">eq-neuron</a>, a neurons value is determined by the weights that go into it from the previous layer, not the weights going out of it into the next one, and since a layer is composed of units, and each unit depends on the weights coming into it, it already makes sense that we store the weights that go into the layer in its object, not the weights coming out of it, plus, if we were to use the second approach a layer would have to store the activation function of the next layer not the activation function of itself, which makes no sense.
</p>

<p>
i also went through the source code of <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer.py">tensorflow</a> and it seems this is what they're doing aswell. this will be the assumption in my code.
</p>

<p>
time to implement different types of layers, starting with convolutional layer:
</p>
<div class="org-src-container">
<pre class="src src-lisp" id="orga73e35d">(<span style="font-weight: bold;">defclass</span> <span style="font-weight: bold; text-decoration: underline;">convolutional-layer</span> (layer)
  ()
  (<span style="font-weight: bold;">:documentation</span> <span style="font-style: italic;">"a layer that arbitrary dimensions for its weights tensor, and convolves it with an arbitrary input tensor (convolution layer of arbitrary dimensions), the weights tensor should be 1 order higher that the input tensor, e.g. if the input tensor is a 3d tensor (image with feature maps), the shape of the weights tensor should be of 4 dimensions (4d tensor), the output tensor of the layer would be of the same order as the input tensor"</span>))

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">make-convolutional-layer</span> (<span style="font-weight: bold; text-decoration: underline;">&amp;key</span> dims activation-function
                                   activation-function-derivative
                                   tensor-activation-function
                                   tensor-activation-function-derivative)
  <span style="font-style: italic;">"consutrctor for convolutional layers"</span>
  (make-instance
   'convolutional-layer
   <span style="font-weight: bold;">:weights</span> (random-tensor dims)
   <span style="font-weight: bold;">:biases</span> (random-tensor (car dims))
   <span style="font-weight: bold;">:activation-function</span> activation-function
   <span style="font-weight: bold;">:activation-function-derivative</span> activation-function-derivative
   <span style="font-weight: bold;">:tensor-activation-function</span> tensor-activation-function
   <span style="font-weight: bold;">:tensor-activation-function-derivative</span> tensor-activation-function-derivative))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">feedforward</span> ((l convolutional-layer) x)
  <span style="font-style: italic;">"x is an arbitrary tensor"</span>
  (<span style="font-weight: bold;">with-slots</span> (weights biases activation-function tensor-activation-function) l
    (<span style="font-weight: bold;">let*</span> ((num-kernels (array-dimension weights 0))
           (convolution-out-size
             (mapcar
              (<span style="font-weight: bold;">lambda</span> (img-d ker-d) (- img-d (1- ker-d)))
              (array-dimensions x)
              (cdr (array-dimensions weights))))
           (out (make-array (append (list num-kernels) convolution-out-size))))
      (<span style="font-weight: bold;">loop</span> for kernel-idx from 0 below num-kernels
            do (set-array-nth
                out
                (array-map (<span style="font-weight: bold;">lambda</span> (cell) (+ cell (aref biases kernel-idx)))
                           (tensor-convolution
                            x
                            (array-nth weights kernel-idx)))
                kernel-idx))
      <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">we return 2 values, the output tensor and the unactivated output tensor</span>
      (<span style="font-weight: bold;">if</span> tensor-activation-function
          <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">apply per-unit activation functions</span>
          (values
           (array-map-indicies out tensor-activation-function)
           out)
          <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">apply single activation function to all units in the layer</span>
          (values (array-map activation-function out) out)))))

(<span style="font-weight: bold;">defclass</span> <span style="font-weight: bold; text-decoration: underline;">3d-convolutional-layer</span> (convolutional-layer)
  ()
  (<span style="font-weight: bold;">:documentation</span> <span style="font-style: italic;">"a convolutional layer with 4d weights tensor and 3d input/output tensors, the depths of the input and weight tensors should be the same, this is used for convolving images with feature maps (channels)</span>
<span style="font-style: italic;">to see the difference between this and the parent class consider the following examples:</span>
<span style="font-style: italic;">CL-USER&gt; (array-dimensions (let ((l (make-convolutional-layer :dims '(2 3 3 3))))</span>
<span style="font-style: italic;">                    (feedforward l (random-tensor '(3 6 6)))))</span>
<span style="font-style: italic;">=&gt; (2 1 4 4)</span>
<span style="font-style: italic;">CL-USER&gt; (array-dimensions (let ((l (make-3d-convolutional-layer-from-dims :dims '(2 3 3 3))))</span>
<span style="font-style: italic;">                    (feedforward l (random-tensor '(3 6 6)))))</span>
<span style="font-style: italic;">=&gt; (2 4 4)</span>
<span style="font-style: italic;">"</span>))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">feedforward</span> <span style="font-weight: bold;">:around</span> ((l 3d-convolutional-layer) x)
  <span style="font-style: italic;">"grab the output of the parent arbitrary-convolution class, reshape it and return it, as there is always redundant dimension in the 4d tensor, this happens because the tensors (input and weights) have the same depth when doing image convolution"</span>
  (<span style="font-weight: bold;">multiple-value-bind</span> (out unactivated-out) (call-next-method) <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">output of parent class' feedforward</span>
    (<span style="font-weight: bold;">let</span> ((actual-convolution-out-size
            (append (list (array-dimension out 0))
                    (cdr (cdr (array-dimensions out))))))
      (values (make-array actual-convolution-out-size <span style="font-weight: bold;">:displaced-to</span> out)
              (make-array actual-convolution-out-size <span style="font-weight: bold;">:displaced-to</span> unactivated-out)))))

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">make-3d-convolutional-layer-from-dims</span> (<span style="font-weight: bold; text-decoration: underline;">&amp;key</span> dims activation-function activation-function-derivative tensor-activation-function tensor-activation-function-derivative)
  <span style="font-style: italic;">"consutrctor for convolutional layers"</span>
  (make-instance
   '3d-convolutional-layer
   <span style="font-weight: bold;">:activation-function</span> activation-function
   <span style="font-weight: bold;">:activation-function-derivative</span> activation-function-derivative
   <span style="font-weight: bold;">:tensor-activation-function</span> tensor-activation-function
   <span style="font-weight: bold;">:tensor-activation-function-derivative</span> tensor-activation-function-derivative
   <span style="font-weight: bold;">:weights</span> (random-tensor dims)
   <span style="font-weight: bold;">:biases</span> (random-tensor (car dims))))

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">make-3d-convolutional-layer</span> (<span style="font-weight: bold; text-decoration: underline;">&amp;key</span> activation-function activation-function-derivative num-kernels kernel-depth kernel-height kernel-width tensor-activation-function tensor-activation-function-derivative)
  <span style="font-style: italic;">"consutrctor for convolutional layers"</span>
  (make-instance
   '3d-convolutional-layer
   <span style="font-weight: bold;">:activation-function</span> activation-function
   <span style="font-weight: bold;">:activation-function-derivative</span> activation-function-derivative
   <span style="font-weight: bold;">:tensor-activation-function</span> tensor-activation-function
   <span style="font-weight: bold;">:tensor-activation-function-derivative</span> tensor-activation-function-derivative
   <span style="font-weight: bold;">:weights</span> (random-tensor (list num-kernels kernel-depth kernel-height kernel-width))
   <span style="font-weight: bold;">:biases</span> (random-tensor num-kernels)))
</pre>
</div>
</div>
</div>
<div id="outline-container-orgdb1f9d5" class="outline-3">
<h3 id="orgdb1f9d5">pooling layer</h3>
<div class="outline-text-3" id="text-orgdb1f9d5">
<p>
pooling layer:
</p>
<div class="org-src-container">
<pre class="src src-lisp" id="org10450ac">(<span style="font-weight: bold;">defclass</span> <span style="font-weight: bold; text-decoration: underline;">pooling-layer</span> (layer)
  ((rows <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:rows</span> <span style="font-weight: bold;">:accessor</span> pooling-layer-rows)
   (cols <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:cols</span> <span style="font-weight: bold;">:accessor</span> pooling-layer-cols)
   (pooling-function <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:pooling-function</span> <span style="font-weight: bold;">:accessor</span> pooling-layer-function)
   (unpooling-function <span style="font-weight: bold;">:initarg</span> <span style="font-weight: bold;">:unpooling-function</span> <span style="font-weight: bold;">:accessor</span> pooling-layer-unpooling-function))) <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">unpooling-function will make sense when you read later on</span>

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">max-pooling-function</span> (myvec)
  (reduce #'max myvec))

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">average-pooling-function</span> (myvec)
  (/ (reduce #'+ myvec) (length myvec)))

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">make-pooling-layer</span> (<span style="font-weight: bold; text-decoration: underline;">&amp;key</span> rows cols pooling-function unpooling-function)
  (make-instance
   'pooling-layer
   <span style="font-weight: bold;">:rows</span> rows
   <span style="font-weight: bold;">:cols</span> cols
   <span style="font-weight: bold;">:pooling-function</span> (or pooling-function #'average-pooling-function)
   <span style="font-weight: bold;">:unpooling-function</span> (or unpooling-function #'average-unpooling-function)))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">feedforward</span> ((l pooling-layer) x)
  <span style="font-style: italic;">"x is a tensor of the third order, which in case of the first layer is the actual image"</span>
  (<span style="font-weight: bold;">with-slots</span> (rows cols pooling-function) l
    (<span style="font-weight: bold;">let*</span> ((num-channels (array-depth x))
           (img-rows (array-rows x))
           (img-cols (array-cols x))
           (out-rows (/ img-rows rows))
           (out-cols (/ img-cols cols))
           (out (make-array (list num-channels out-rows out-cols))))
      (<span style="font-weight: bold;">loop</span> for channel-idx from 0 below num-channels
            do (<span style="font-weight: bold;">loop</span> for img-row-idx from 0 below img-rows by rows
                     do (<span style="font-weight: bold;">loop</span> for img-col-idx from 0 below img-cols by cols
                              do (<span style="font-weight: bold;">let</span> ((out-row-idx (/ img-row-idx rows))
                                       (out-col-idx (/ img-col-idx cols)))
                                   (setf
                                    (aref out channel-idx out-row-idx out-col-idx)
                                    (funcall
                                     pooling-function
                                     (vectorize-array
                                      (subarray
                                       x
                                       (list channel-idx img-row-idx img-col-idx)
                                       (list rows cols)))))))))
      out)))
</pre>
</div>
</div>
</div>
<div id="outline-container-org747b76c" class="outline-3">
<h3 id="org747b76c">flatten layer</h3>
<div class="outline-text-3" id="text-org747b76c">
<p>
a flatten layer, which is used to reduce dimensionality to 1d (to vectorize the input tensor), and doesnt have any parameters:
</p>
<div class="org-src-container">
<pre class="src src-lisp" id="orgc3b2077">(<span style="font-weight: bold;">defclass</span> <span style="font-weight: bold; text-decoration: underline;">flatten-layer</span> (layer) ())

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">make-flatten-layer</span> () (make-instance 'flatten-layer))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">feedforward</span> ((l flatten-layer) x)
  (vectorize-array x))
</pre>
</div>
</div>
</div>
<div id="outline-container-orge1b6606" class="outline-3">
<h3 id="orge1b6606">dense layer</h3>
<div class="outline-text-3" id="text-orge1b6606">
<p>
a dense layer, or a fully connected layer:
</p>
<div class="org-src-container">
<pre class="src src-lisp" id="orgc380517">(<span style="font-weight: bold;">defclass</span> <span style="font-weight: bold; text-decoration: underline;">dense-layer</span> (convolutional-layer) ())

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">make-dense-layer</span> (<span style="font-weight: bold; text-decoration: underline;">&amp;key</span> num-units prev-layer-num-units
                           activation-function activation-function-derivative
                           tensor-activation-function tensor-activation-function-derivative)
  (make-instance
   'dense-layer
   <span style="font-weight: bold;">:activation-function</span> activation-function
   <span style="font-weight: bold;">:activation-function-derivative</span> activation-function-derivative
   <span style="font-weight: bold;">:tensor-activation-function</span> tensor-activation-function
   <span style="font-weight: bold;">:tensor-activation-function-derivative</span> tensor-activation-function-derivative
   <span style="font-weight: bold;">:weights</span> (random-tensor (list num-units prev-layer-num-units))
   <span style="font-weight: bold;">:biases</span> (random-tensor num-units)))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">feedforward</span> <span style="font-weight: bold;">:around</span> ((l dense-layer) x)
  <span style="font-style: italic;">"return the output of the convolution, properly reshaped"</span>
  (<span style="font-weight: bold;">multiple-value-bind</span> (out unactivated-out) (call-next-method)
    (values (make-array (list (array-dimension out 0)) <span style="font-weight: bold;">:displaced-to</span> out)
            (make-array (list (array-dimension out 0)) <span style="font-weight: bold;">:displaced-to</span> unactivated-out))))
</pre>
</div>
</div>
</div>
<div id="outline-container-org35b7d0e" class="outline-3">
<h3 id="org35b7d0e">test code</h3>
<div class="outline-text-3" id="text-org35b7d0e">
<p>
feedforwarding test in convolutional layers, with an (randomly generated) input image of depth 3 (3 channels), width 6 and height 6 (in the code columns and rows, respectively), like in <a href="/convolutional_neural_network.html">fig-convlayer</a>:
</p>
<div class="org-src-container">
<pre class="src src-lisp">(<span style="font-weight: bold;">let</span> ((l (make-convolutional-layer <span style="font-weight: bold;">:dims</span> '(5 3 4 3)
                                   <span style="font-weight: bold;">:activation-function</span> #'relu
                                   <span style="font-weight: bold;">:activation-function-derivative</span> #'relu-derivative)))
  (array-dimensions (feedforward l (random-tensor '(3 10 6)))))
</pre>
</div>

<pre class="example">
(5 1 7 4)
</pre>


<div class="org-src-container">
<pre class="src src-lisp">(<span style="font-weight: bold;">let</span> ((l (make-3d-convolutional-layer-from-dims
          <span style="font-weight: bold;">:dims</span> '(5 3 4 3)
          <span style="font-weight: bold;">:activation-function</span> #'relu
          <span style="font-weight: bold;">:activation-function-derivative</span> #'relu-derivative)))
  (array-dimensions (feedforward l (random-tensor '(3 10 6)))))
</pre>
</div>

<pre class="example">
(5 7 4)
</pre>


<p>
the code outputs a tensor of size 2x4x4, as expected and as seen in the figure.
</p>

<p>
feedforwarding test in pooling layers:
</p>
<div class="org-src-container">
<pre class="src src-lisp">(<span style="font-weight: bold;">let</span> ((l (make-pooling-layer <span style="font-weight: bold;">:rows</span> 16 <span style="font-weight: bold;">:cols</span> 16)))
  (feedforward l (random-tensor '(3 32 32))))
</pre>
</div>

<pre class="example">
#3A(((-0.09865912f0 0.0050482843f0) (0.033965267f0 0.009567747f0))
    ((-0.048119776f0 0.053631075f0) (0.0039087785f0 -0.045654505f0))
    ((0.011538963f0 0.012444877f0) (0.009892253f0 0.018327955f0)))
</pre>


<p>
feedforwarding test in dense layers:
</p>
<div class="org-src-container">
<pre class="src src-lisp">(<span style="font-weight: bold;">let</span> ((l (make-dense-layer <span style="font-weight: bold;">:num-units</span> 20 <span style="font-weight: bold;">:prev-layer-num-units</span> 30
                           <span style="font-weight: bold;">:activation-function</span> #'sigmoid
                           <span style="font-weight: bold;">:activation-function-derivative</span> #'sigmoid-derivative)))
  (feedforward l (random-tensor '(30))))
</pre>
</div>

<pre class="example">
#(0.07897233f0 0.6648786f0 0.5233108f0 0.16540164f0 0.5859843f0 0.3620081f0
  0.881382f0 0.6328014f0 0.9851791f0 0.54547656f0 0.2188674f0 0.46183386f0
  0.52967626f0 0.255327f0 0.5279645f0 0.70072246f0 0.022833973f0 0.043903537f0
  0.5002063f0 0.37229472f0)
#(-2.4563925f0 0.6851117f0 0.09331077f0 -1.6185739f0 0.34738904f0 -0.566659f0
  2.0055835f0 0.54425377f0 4.196786f0 0.18241026f0 -1.2722789f0 -0.15296215f0
  0.11884463f0 -1.0704002f0 0.11197477f0 0.8507405f0 -3.756407f0 -3.080864f0
  8.253455f-4 -0.52238494f0)
</pre>


<p>
feedforwarding test in flatten layers:
</p>
<div class="org-src-container">
<pre class="src src-lisp">(feedforward (make-flatten-layer) (random-tensor '(2 3 3)))
</pre>
</div>

<pre class="example">
#(0.7158413f0 -0.65535855f0 -0.95929027f0 0.8278158f0 -0.56471443f0
  -0.40639305f0 0.8319006f0 -0.039112806f0 -0.61821413f0 -0.8546965f0
  -0.8576379f0 -0.77950644f0 0.54494023f0 -0.5361223f0 -0.012234688f0
  0.868943f0 -0.96434f0 -0.61873984f0)
</pre>


<p>
now that we have implemented the necessary layers, we can start building convolutional networks, we start with <a href="/convolutional_neural_network.html">LeNet-5</a>:
</p>

<div class="org-src-container">
<pre class="src src-lisp" id="org49ab275">(<span style="font-weight: bold;">defparameter</span> <span style="font-weight: bold; font-style: italic;">*lenet5*</span>
  (make-network
   <span style="font-weight: bold;">:layers</span> (list
            (make-3d-convolutional-layer-from-dims
             <span style="font-weight: bold;">:dims</span> '(6 1 5 5)
             <span style="font-weight: bold;">:activation-function</span> #'relu
             <span style="font-weight: bold;">:activation-function-derivative</span> #'relu-derivative)
            (make-pooling-layer <span style="font-weight: bold;">:rows</span> 2 <span style="font-weight: bold;">:cols</span> 2)
            (make-3d-convolutional-layer-from-dims
             <span style="font-weight: bold;">:dims</span> '(16 6 5 5)
             <span style="font-weight: bold;">:activation-function</span> #'relu
             <span style="font-weight: bold;">:activation-function-derivative</span> #'relu-derivative)
            (make-pooling-layer <span style="font-weight: bold;">:rows</span> 2 <span style="font-weight: bold;">:cols</span> 2)
            (make-flatten-layer)
            (make-dense-layer <span style="font-weight: bold;">:num-units</span> 120 <span style="font-weight: bold;">:prev-layer-num-units</span> 400
                              <span style="font-weight: bold;">:activation-function</span> #'relu
                              <span style="font-weight: bold;">:activation-function-derivative</span> #'relu-derivative)
            (make-dense-layer <span style="font-weight: bold;">:num-units</span> 84 <span style="font-weight: bold;">:prev-layer-num-units</span> 120
                              <span style="font-weight: bold;">:activation-function</span> #'relu
                              <span style="font-weight: bold;">:activation-function-derivative</span> #'relu-derivative)
            (make-dense-layer <span style="font-weight: bold;">:num-units</span> 10 <span style="font-weight: bold;">:prev-layer-num-units</span> 84
                              <span style="font-weight: bold;">:activation-function</span> #'sigmoid
                              <span style="font-weight: bold;">:activation-function-derivative</span> #'sigmoid-derivative))))
</pre>
</div>
<p>
we can perform a simple feedforwarding test with a randomly permuted "image":
</p>
<div class="org-src-container">
<pre class="src src-lisp">(car (network-feedforward *lenet5* (random-tensor '(1 32 32))))
</pre>
</div>

<pre class="example">
(#(1.0 0 1.0 0 0 0 0 1.0 3.6099282e-22 1.0)
 . #(542.2019 -1379.9155 469.97314 -319.0183 -235.80972 -294.26898 -628.3136
     134.56656 -49.373184 454.5819))
</pre>


<p>
we got 10 values out, as expected (the size of the output layer is 10)
</p>
</div>
</div>
<div id="outline-container-org6c05869" class="outline-3">
<h3 id="org6c05869">convolutional layer backprop</h3>
<div class="outline-text-3" id="text-org6c05869">
<p>
its time to implement backpropagation!
a table of the variables in the code and their counterparts in the math formulas:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">code</th>
<th scope="col" class="org-left">math</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left"><code>layer-x</code></td>
<td class="org-left"><img src="ltx/2fb480524b0.svg" alt="\(I^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /></td>
</tr>

<tr>
<td class="org-left"><code>layer-y</code></td>
<td class="org-left"><img src="ltx/7c52db4d64b.svg" alt="\(\hat Y^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /></td>
</tr>

<tr>
<td class="org-left"><code>layer-y-unactivated</code></td>
<td class="org-left"><img src="ltx/c370623764d.svg" alt="\(S^\ell\)" style="height: 0.9849em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /></td>
</tr>

<tr>
<td class="org-left"><code>s-deltas</code></td>
<td class="org-left"><img src="ltx/23db621f97a.svg" alt="\(\Delta S^\ell\)" style="height: 0.9849em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" /></td>
</tr>

<tr>
<td class="org-left"><code>x-deltas</code></td>
<td class="org-left"><img src="ltx/2fb480524b0.svg" alt="\(I^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /></td>
</tr>

<tr>
<td class="org-left"><code>activation-function</code></td>
<td class="org-left"><img src="ltx/01d849759fb.svg" alt="\(\phi\)" style="height: 0.9010em; vertical-align: -0.2667em; display: inline-block" class="org-latex org-latex-inline" /></td>
</tr>

<tr>
<td class="org-left"><code>activation-function-derivative</code></td>
<td class="org-left"><img src="ltx/e2c4f273450.svg" alt="\(\phi'\)" style="height: 1.1001em; vertical-align: -0.2667em; display: inline-block" class="org-latex org-latex-inline" /></td>
</tr>

<tr>
<td class="org-left"><code>propped-deltas</code></td>
<td class="org-left"><img src="ltx/3d2843e509d.svg" alt="\(\Delta I^{\ell+1}\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /></td>
</tr>

<tr>
<td class="org-left"><code>learning-rate</code></td>
<td class="org-left"><img src="ltx/7d4b8a169de.svg" alt="\(\alpha\)" style="height: 0.4992em; vertical-align: -0.0551em; display: inline-block" class="org-latex org-latex-inline" /></td>
</tr>
</tbody>
</table>
<div class="org-src-container">
<pre class="src src-lisp" id="orgea20ff1">(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">propbackward</span> ((l convolutional-layer) layer-x layer-y layer-y-unactivated propped-deltas learning-rate)
  <span style="font-style: italic;">"compute the gradients of the layer, propped-deltas is a tensor of the errors or 'deltas' at the output nodes which is propagated back from the succeeding layer in the network, layer-x is the input image tensor that was passed to the layer during feedforwarding, layer-y is the output of the layers' feedforwarding (activation of nodes), the assumption here is that the last dimensions of weight,image tensors are equal so that the image tensor keeps its rank/order, notice that (for now) this function assumes the equality of the order of input and output tensors"</span>
  (<span style="font-weight: bold;">with-slots</span> (weights biases
               activation-function-derivative
               tensor-activation-function-derivative)
      l
    <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">here we restore the dropped dimension, if any (a dimension is dropped if the tensors convolved have the same depth, which happens in your standard 3d convolutions with images, im not even sure why im writing code for a more general case..), we do this because its then easier to apply the math, note that all three arrays layer-y,layer-y-unactivated,propped-deltas here have the same dimensions, also note that here reshaping the arrays by adding a dimension with size 1 doesnt affect the arrays actual sizes, only their dimensionality and order/rank</span>
    (<span style="font-weight: bold;">when</span> (not (eq (length (array-dimensions weights))
                   (length (array-dimensions layer-y))))
      (setf layer-y
            (make-array (append (list (array-dimension layer-y 0) 1)
                                (cdr (array-dimensions layer-y)))
                        <span style="font-weight: bold;">:displaced-to</span> layer-y))
      (setf layer-y-unactivated
            (make-array (append (list (array-dimension layer-y-unactivated 0) 1)
                                (cdr (array-dimensions layer-y-unactivated)))
                        <span style="font-weight: bold;">:displaced-to</span> layer-y-unactivated))
      (setf propped-deltas 
            (make-array (append (list (array-dimension propped-deltas 0) 1)
                                (cdr (array-dimensions propped-deltas)))
                        <span style="font-weight: bold;">:displaced-to</span> propped-deltas)))

    (<span style="font-weight: bold;">let</span> ((x-deltas (make-array (array-dimensions layer-x))) <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">\Delta I in the math section, the deltas at the inputs, these are to be backpropped</span>
          (s-deltas (make-array (array-dimensions layer-y)))) <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">\Delta S in the math, the deltas "after" the activation function</span>
      <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">compute s-deltas</span>
      (<span style="font-weight: bold;">loop</span> for layer-y-idx from 0 below (array-size layer-y) do
        (<span style="font-weight: bold;">let*</span> ((layer-y-indicies (array-index-row-major layer-y layer-y-idx))
               (layer-y-unactivated-entry (apply #'aref (append (list layer-y-unactivated) layer-y-indicies))) <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">S^\ell[k,i,j]</span>
               (propped-delta (apply #'aref (append (list propped-deltas) layer-y-indicies)))) <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">\Delta\hat Y[k,i,j]</span>
          (<span style="font-weight: bold;">if</span> tensor-activation-function-derivative
              (setf (apply #'aref (append (list s-deltas) layer-y-indicies)) (* propped-delta (funcall tensor-activation-function-derivative layer-y-unactivated layer-y-indicies)))
              (setf (apply #'aref (append (list s-deltas) layer-y-indicies)) (* propped-delta (funcall activation-function-derivative layer-y-unactivated-entry))))))

      <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">compute x-deltas, this was replaced with the next sexp, im not sure if it even works but i feel like keeping it here, it is the code for the math in [[blk:eq-convnet-I-delta-1]]</span>
      <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">(loop for layer-x-idx from 0 below (array-size layer-x) do</span>
      <span style="font-weight: bold; font-style: italic;">;;   </span><span style="font-weight: bold; font-style: italic;">(let ((layer-x-indicies (array-index-row-major layer-x layer-x-idx)))</span>
      <span style="font-weight: bold; font-style: italic;">;;     </span><span style="font-weight: bold; font-style: italic;">(loop for layer-y-idx from 0 below (array-size layer-y) do</span>
      <span style="font-weight: bold; font-style: italic;">;;       </span><span style="font-weight: bold; font-style: italic;">(let* ((layer-y-indicies (array-index-row-major layer-y layer-y-idx))</span>
      <span style="font-weight: bold; font-style: italic;">;;              </span><span style="font-weight: bold; font-style: italic;">(s-delta (apply #'aref (append (list s-deltas) layer-y-indicies)))</span>
      <span style="font-weight: bold; font-style: italic;">;;              </span><span style="font-weight: bold; font-style: italic;">;; see [[blk:not-convnet-I-indicies]], this is the pattern for the indicies of the weight to be multiplied by the entry in the input</span>
      <span style="font-weight: bold; font-style: italic;">;;              </span><span style="font-weight: bold; font-style: italic;">(weight-indicies (append (list (1- (array-dimension weights 0))) (mapcar #'+ (mapcar #'- (cdr (array-dimensions weights)) layer-x-indicies) (cdr layer-y-indicies) (make-list (length layer-x-indicies) :initial-element -1)))) ;; we add (..,-1,-1,-1) because in the math the indexing starts at 1 not 0</span>
      <span style="font-weight: bold; font-style: italic;">;;              </span><span style="font-weight: bold; font-style: italic;">(in-range t))</span>
      <span style="font-weight: bold; font-style: italic;">;;         </span><span style="font-weight: bold; font-style: italic;">;; use in-range to check whether weight indicies are within the range of the weights tensor, again refer to [[blk:not-convnet-I-indicies]]</span>
      <span style="font-weight: bold; font-style: italic;">;;         </span><span style="font-weight: bold; font-style: italic;">(loop for i from 0 below (length weight-indicies) do</span>
      <span style="font-weight: bold; font-style: italic;">;;           </span><span style="font-weight: bold; font-style: italic;">(if (or (not (&lt; (elt weight-indicies i) (array-dimension weights i)))</span>
      <span style="font-weight: bold; font-style: italic;">;;                   </span><span style="font-weight: bold; font-style: italic;">(&lt; (elt weight-indicies i) 0))</span>
      <span style="font-weight: bold; font-style: italic;">;;               </span><span style="font-weight: bold; font-style: italic;">(setf in-range nil)))</span>
      <span style="font-weight: bold; font-style: italic;">;;         </span><span style="font-weight: bold; font-style: italic;">(when in-range</span>
      <span style="font-weight: bold; font-style: italic;">;;           </span><span style="font-weight: bold; font-style: italic;">(let* ((weight (apply #'aref (append (list weights) weight-indicies)))</span>
      <span style="font-weight: bold; font-style: italic;">;;                  </span><span style="font-weight: bold; font-style: italic;">(x-delta-to-add (* s-delta weight)))</span>
      <span style="font-weight: bold; font-style: italic;">;;             </span><span style="font-weight: bold; font-style: italic;">;; update an x-delta</span>
      <span style="font-weight: bold; font-style: italic;">;;             </span><span style="font-weight: bold; font-style: italic;">(incf (apply #'aref (append (list x-deltas) layer-x-indicies)) x-delta-to-add)))))))</span>

      <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">an updated solution to compute x-deltas discussed in [[any:optimization1]], first attempt</span>
      <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">(loop for layer-x-idx from 0 below (array-size layer-x) do</span>
      <span style="font-weight: bold; font-style: italic;">;;   </span><span style="font-weight: bold; font-style: italic;">(let ((layer-x-indicies (array-index-row-major layer-x layer-x-idx)))</span>
      <span style="font-weight: bold; font-style: italic;">;;     </span><span style="font-weight: bold; font-style: italic;">(loop for weight-idx from 0 below (array-size weights) do</span>
      <span style="font-weight: bold; font-style: italic;">;;       </span><span style="font-weight: bold; font-style: italic;">(let* ((weight-indicies (array-index-row-major weights weight-idx))</span>
      <span style="font-weight: bold; font-style: italic;">;;              </span><span style="font-weight: bold; font-style: italic;">(s-delta-indicies (append</span>
      <span style="font-weight: bold; font-style: italic;">;;                                 </span><span style="font-weight: bold; font-style: italic;">(list (car weight-indicies))</span>
      <span style="font-weight: bold; font-style: italic;">;;                                 </span><span style="font-weight: bold; font-style: italic;">(mapcar #'-</span>
      <span style="font-weight: bold; font-style: italic;">;;                                         </span><span style="font-weight: bold; font-style: italic;">layer-x-indicies</span>
      <span style="font-weight: bold; font-style: italic;">;;                                         </span><span style="font-weight: bold; font-style: italic;">(cdr weight-indicies))))</span>
      <span style="font-weight: bold; font-style: italic;">;;              </span><span style="font-weight: bold; font-style: italic;">(desired-weight-indicies</span>
      <span style="font-weight: bold; font-style: italic;">;;                </span><span style="font-weight: bold; font-style: italic;">(append</span>
      <span style="font-weight: bold; font-style: italic;">;;                 </span><span style="font-weight: bold; font-style: italic;">(list (car weight-indicies))</span>
      <span style="font-weight: bold; font-style: italic;">;;                 </span><span style="font-weight: bold; font-style: italic;">(mapcar #'-</span>
      <span style="font-weight: bold; font-style: italic;">;;                         </span><span style="font-weight: bold; font-style: italic;">(cdr (array-dimensions weights))</span>
      <span style="font-weight: bold; font-style: italic;">;;                         </span><span style="font-weight: bold; font-style: italic;">(cdr weight-indicies)</span>
      <span style="font-weight: bold; font-style: italic;">;;                         </span><span style="font-weight: bold; font-style: italic;">(make-list (length (cdr weight-indicies))</span>
      <span style="font-weight: bold; font-style: italic;">;;                                    </span><span style="font-weight: bold; font-style: italic;">:initial-element 1))))</span>
      <span style="font-weight: bold; font-style: italic;">;;              </span><span style="font-weight: bold; font-style: italic;">(in-range t))</span>
      <span style="font-weight: bold; font-style: italic;">;;         </span><span style="font-weight: bold; font-style: italic;">(loop for i from 0 below (length desired-weight-indicies) do</span>
      <span style="font-weight: bold; font-style: italic;">;;           </span><span style="font-weight: bold; font-style: italic;">(if (or (not (&lt; (elt desired-weight-indicies i)</span>
      <span style="font-weight: bold; font-style: italic;">;;                           </span><span style="font-weight: bold; font-style: italic;">(array-dimension weights i)))</span>
      <span style="font-weight: bold; font-style: italic;">;;                   </span><span style="font-weight: bold; font-style: italic;">(&lt; (elt desired-weight-indicies i) 0))</span>
      <span style="font-weight: bold; font-style: italic;">;;               </span><span style="font-weight: bold; font-style: italic;">(setf in-range nil)))</span>
      <span style="font-weight: bold; font-style: italic;">;;         </span><span style="font-weight: bold; font-style: italic;">(loop for i from 0 below (length s-delta-indicies) do</span>
      <span style="font-weight: bold; font-style: italic;">;;           </span><span style="font-weight: bold; font-style: italic;">(if (or (not (&lt; (elt s-delta-indicies i)</span>
      <span style="font-weight: bold; font-style: italic;">;;                           </span><span style="font-weight: bold; font-style: italic;">(array-dimension s-deltas i)))</span>
      <span style="font-weight: bold; font-style: italic;">;;                   </span><span style="font-weight: bold; font-style: italic;">(&lt; (elt s-delta-indicies i) 0))</span>
      <span style="font-weight: bold; font-style: italic;">;;               </span><span style="font-weight: bold; font-style: italic;">(setf in-range nil)))</span>
      <span style="font-weight: bold; font-style: italic;">;;         </span><span style="font-weight: bold; font-style: italic;">(when in-range</span>
      <span style="font-weight: bold; font-style: italic;">;;           </span><span style="font-weight: bold; font-style: italic;">(let* ((weight (apply #'aref (append (list weights)</span>
      <span style="font-weight: bold; font-style: italic;">;;                                                </span><span style="font-weight: bold; font-style: italic;">desired-weight-indicies)))</span>
      <span style="font-weight: bold; font-style: italic;">;;                  </span><span style="font-weight: bold; font-style: italic;">(s-delta (apply #'aref (append (list s-deltas)</span>
      <span style="font-weight: bold; font-style: italic;">;;                                                 </span><span style="font-weight: bold; font-style: italic;">s-delta-indicies)))</span>
      <span style="font-weight: bold; font-style: italic;">;;                  </span><span style="font-weight: bold; font-style: italic;">(x-delta-to-add (* s-delta weight)))</span>
      <span style="font-weight: bold; font-style: italic;">;;             </span><span style="font-weight: bold; font-style: italic;">;; update an x-delta</span>
      <span style="font-weight: bold; font-style: italic;">;;             </span><span style="font-weight: bold; font-style: italic;">(incf (apply #'aref (append (list x-deltas) layer-x-indicies)) x-delta-to-add)))))))</span>

      <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">third attempt for [[any:optimization1]], here we're dropping the second-to-highest dimension because we dont need to iterate over it for every entry in layer-x, this saves us alot of iterations as it actually reduces the exponent of the time complexity (each dimension is basically another nested for loop), currently this doesnt support cases where D_K != D_I</span>
      (<span style="font-weight: bold;">let</span> ((needed-weight-dimensions (cons (car (array-dimensions weights))
                                            (cdr (cdr (array-dimensions weights))))))
        (<span style="font-weight: bold;">loop</span> for layer-x-idx from 0 below (array-size layer-x) do
          (<span style="font-weight: bold;">let</span> ((layer-x-indicies (array-index-row-major layer-x layer-x-idx)))
            (<span style="font-weight: bold;">loop</span> for weight-idx from 0 below (reduce #'* needed-weight-dimensions) do
              (<span style="font-weight: bold;">let*</span> ((weight-indicies (from-row-major needed-weight-dimensions weight-idx))
                     (s-delta-indicies (append
                                        (cons (car weight-indicies) (cons 0 nil))
                                        (mapcar #'-
                                                (cdr layer-x-indicies)
                                                (cdr weight-indicies))))
                     (desired-weight-indicies
                       (cons
                        (car weight-indicies)
                        (mapcar #'-
                                (cdr (array-dimensions weights))
                                (cons (car layer-x-indicies) (cdr weight-indicies))
                                (make-list (length weight-indicies)
                                           <span style="font-weight: bold;">:initial-element</span> 1))))
                     (in-range t))
                (<span style="font-weight: bold;">loop</span> for i from 0 below (length desired-weight-indicies) do
                  (<span style="font-weight: bold;">if</span> (or (not (&lt; (elt desired-weight-indicies i)
                                  (array-dimension weights i)))
                          (&lt; (elt desired-weight-indicies i) 0))
                      (setf in-range nil)))
                (<span style="font-weight: bold;">loop</span> for i from 0 below (length s-delta-indicies) do
                  (<span style="font-weight: bold;">if</span> (or (not (&lt; (elt s-delta-indicies i)
                                  (array-dimension s-deltas i)))
                          (&lt; (elt s-delta-indicies i) 0))
                      (setf in-range nil)))
                (<span style="font-weight: bold;">when</span> in-range
                  (<span style="font-weight: bold;">let*</span> ((weight (apply #'aref (append (list weights)
                                                       desired-weight-indicies)))
                         (s-delta (apply #'aref (append (list s-deltas)
                                                        s-delta-indicies)))
                         (x-delta-to-add (* s-delta weight)))
                    <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">update an x-delta</span>
                    (incf (apply #'aref (append (list x-deltas) layer-x-indicies)) x-delta-to-add))))))))

      <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">update the biases</span>
      <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">why are we iterating through biases as if its a multidimesional array/tensor? its just a vector, this is misleading, but im leaving it this way for now</span>
      (<span style="font-weight: bold;">loop</span> for bias-idx from 0 below (array-size biases) do
        (<span style="font-weight: bold;">let</span> ((bias-indicies (array-index-row-major biases bias-idx))
              (gradient 0)
              (needed-y-dimensions (cdr (array-dimensions layer-y))))
          (<span style="font-weight: bold;">loop</span> for layer-y-idx from 0 below (reduce #'* needed-y-dimensions) do
            (<span style="font-weight: bold;">let*</span> ((layer-y-indicies (cons (car bias-indicies)
                                           (from-row-major needed-y-dimensions layer-y-idx)))
                   (s-delta (apply #'aref (append (list s-deltas) layer-y-indicies))))
              (incf gradient s-delta)))
          <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">update bias</span>
          (decf (apply #'aref (cons biases bias-indicies)) (* learning-rate gradient))))

      <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">update the weights</span>
      (<span style="font-weight: bold;">loop</span> for weight-idx from 0 below (array-size weights) do
        (<span style="font-weight: bold;">let</span> ((weight-indicies (array-index-row-major weights weight-idx))
              (gradient 0)
              (needed-y-dimensions (cdr (array-dimensions layer-y))))
          <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">needed-y-dimensions are the dimensions we need to iterate through in the layer y, we dont need to iterate through the entire output as a weight is only connected to the output units that are connected to its kernel</span>
          (<span style="font-weight: bold;">loop</span> for layer-y-idx from 0 below (reduce #'* needed-y-dimensions) do
            <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">add the kernel index to the layer-y-indicies</span>
            (<span style="font-weight: bold;">let*</span> ((layer-y-indicies (cons (car weight-indicies) (from-row-major needed-y-dimensions layer-y-idx)))
                   (s-delta (apply #'aref (append (list s-deltas) layer-y-indicies)))
                   (i-indicies (mapcar #'+ (mapcar #'- (cdr (array-dimensions weights)) (cdr weight-indicies) (make-list (length (cdr weight-indicies)) <span style="font-weight: bold;">:initial-element</span> 1)) (cons 0 layer-y-indicies)))
                   (in-range t))
              <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">check if i-indicies are in the correct range</span>
              (<span style="font-weight: bold;">loop</span> for i from 0 below (length i-indicies) do
                (<span style="font-weight: bold;">if</span> (or (not (&lt; (elt i-indicies i) (array-dimension layer-x i)))
                        (&lt; (elt i-indicies i) 0))
                    (setf in-range nil)))
              (<span style="font-weight: bold;">when</span> in-range
                <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">(print (not (member (cons i-indicies layer-y-indicies) added :test #'equal)))</span>
                (<span style="font-weight: bold;">let</span> ((i (apply #'aref (append (list layer-x) i-indicies))))
                  (incf gradient (* s-delta i))))))
          <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">update weight</span>
          (decf (apply #'aref (append (list weights) weight-indicies)) (* learning-rate gradient))))
      x-deltas)))
</pre>
</div>
<p>
now we need to write the propbackward function for networks so we can conduct simple propbackward tests before continuing
</p>
<div class="org-src-container">
<pre class="src src-lisp" id="orgbc62baf">(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">network-propbackward</span> ((n network) network-x network-y feedforward-out)
  <span style="font-style: italic;">"feedforward-out is the result of the network-feedforward function, its a list of cons' of out and unactivated-out, network-x and network-y should be the input and the desired output to the network, respectively"</span>
  (<span style="font-weight: bold;">with-slots</span> (layers learning-rate) n
    (<span style="font-weight: bold;">let*</span> ((output-layer (car (car feedforward-out)))
           <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">initialize the propped deltas to (hat y - y), because we use squared error loss function</span>
           (propped-deltas (array-map #'- output-layer network-y)))
      <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">iterate through each layer</span>
      (<span style="font-weight: bold;">loop</span> for layer-index from (1- (length layers)) above -1 do
        <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">from the feedforward-out list, get the output of the current layer's feedforward (activated and non-activated), they are stored in reverse order so we use pop</span>
        (<span style="font-weight: bold;">let*</span> ((mycons (pop feedforward-out))
               (layer-out (car mycons))
               (layer-unactivated-out (cdr mycons))
               (layer (elt layers layer-index))
               <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">the input to this layer is the output of the next (or previous in feedforward terms) layer, except for the first layer which receives input from the input layer which isnt in the list because its not actually a layer</span>
               (layer-in (<span style="font-weight: bold;">if</span> (car feedforward-out) (car (car feedforward-out)) network-x)))
          <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">propbackward to the next layer, storing the deltas returned into propped-deltas to be passed onto the next layer</span>
          (setf propped-deltas (propbackward layer layer-in layer-out layer-unactivated-out propped-deltas learning-rate)))))))
</pre>
</div>
<p>
before implementing backpropagation for the pooling layer we need to think about how each type of pooling layer should handle the backpropagatted gradients, if we consider the average pooling layer, all pixels in a "pooled portion" of the image have an equal effect on the loss function, but in a max pooling layer, only the pixel with the maximum value has an effect and the rest of the pixels have no effect whatsoever because they dont affect the values of the pixels in succeeding layers
since i already had different functions for pooling, <code>average-pooling-function</code> and <code>max-pooling-function</code>, i decided to implement the concept of "unpooling function", which take a slice of the input image, one that corresponds to a pixel in the output of the layer and was reduced into that pixel during feedforward, it also takes the gradient at that pixel and splits it into a 2d layer of "subgradients" that corresponds to the slice of the image, for example in the "average unpooling function" <code>average-unpooling-function</code>, the gradient of the pixel in the output is divided equally to a grid of <img src="ltx/79e76a87c91.svg" alt="\(H_K^\ell \times W_K^\ell\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" /> which is then added to the tensor of deltas at the same position the 2d grid of pixels have in the input image tensor, this tensor of deltas is to be forwarded later to the next layer during backprop
i had to go back and edit the definition of the <code>pooling-layer</code> class to include the new <code>unpooling-function</code> slot and the function <code>make-pooling-layer</code> to provide a default unpooling function aswell
</p>
</div>
</div>
<div id="outline-container-org7713e38" class="outline-3">
<h3 id="org7713e38">pooling layer backprop</h3>
<div class="outline-text-3" id="text-org7713e38">
<p>
backprop for pooling layer:
</p>
<div class="org-src-container">
<pre class="src src-lisp" id="orgd6d7497">(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">propbackward</span> ((l pooling-layer) layer-x layer-y layer-y-unactivated propped-deltas learning-rate)
  <span style="font-style: italic;">"a pooling layer doesnt care about layer-y,layer-y-unactivated or learning-rate, it just needs to upscale the deltas and pass them on"</span>
  (<span style="font-weight: bold;">with-slots</span> (unpooling-function rows cols) l
    (<span style="font-weight: bold;">let</span> ((deltas (make-array (array-dimensions layer-x))))
      (<span style="font-weight: bold;">loop</span> for channel-idx from 0 below (array-depth deltas) do
        (<span style="font-weight: bold;">loop</span> for img-row-idx from 0 below (array-rows deltas) by rows do
          (<span style="font-weight: bold;">loop</span> for img-col-idx from 0 below (array-cols deltas) by cols do
            (<span style="font-weight: bold;">let*</span> ((delta-row-idx (/ img-row-idx rows))
                   (delta-col-idx (/ img-col-idx cols))
                   (img-subgrid (subarray layer-x
                                          (list channel-idx img-row-idx img-col-idx)
                                          (list rows cols)))
                   (gradient (aref propped-deltas channel-idx delta-row-idx delta-col-idx))
                   (delta-grid (funcall unpooling-function img-subgrid gradient)))
              (copy-into-array deltas delta-grid (list
                                                  channel-idx
                                                  img-row-idx
                                                  img-col-idx))))))
      deltas)))

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">average-unpooling-function</span> (img-subgrid gradient)
  <span style="font-style: italic;">"example usage: (progn (setf a (random-tensor '(4 4))) (average-unpooling-function a 0.7))"</span>
  (make-array
   (array-dimensions img-subgrid)
   <span style="font-weight: bold;">:initial-element</span> (/ gradient (array-size img-subgrid))))

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">max-unpooling-function</span> (img-subgrid gradient)
  <span style="font-style: italic;">"example usage: (progn (setf a (random-tensor '(4 4))) (max-unpooling-function a 0.7))"</span>
  (<span style="font-weight: bold;">let*</span> ((gradient-grid (make-array (array-dimensions img-subgrid)))
         (max-value (aref img-subgrid 0 0))
         (max-cell-indicies '(0 0)))
    (<span style="font-weight: bold;">loop</span> for row from 0 below (array-rows img-subgrid) do
      (<span style="font-weight: bold;">loop</span> for col from 0 below (array-cols img-subgrid) do
        (<span style="font-weight: bold;">let</span> ((val (aref img-subgrid row col)))
          (<span style="font-weight: bold;">if</span> (&gt; val max-value)
              (<span style="font-weight: bold;">progn</span>
                (setf max-value val)
                (setf max-cell-indicies (list row col)))))))
    (setf (apply #'aref (append (list gradient-grid) max-cell-indicies)) gradient)
    gradient-grid))
</pre>
</div>
</div>
</div>
<div id="outline-container-org8057e39" class="outline-3">
<h3 id="org8057e39">flatten layer backprop</h3>
<div class="outline-text-3" id="text-org8057e39">
<p>
backprop for flatten layers is probably the simplest, we only need to reshape the deltas and pass them on:
</p>
<div class="org-src-container">
<pre class="src src-lisp" id="org3a347b1">(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">propbackward</span> ((l flatten-layer) layer-x layer-y layer-y-unactivated propped-deltas learning-rate)
  <span style="font-style: italic;">"a pooling layer doesnt care about layer-y-unactivated,propped-deltas or learning-rate"</span>
  (make-array (array-dimensions layer-x) <span style="font-weight: bold;">:displaced-to</span> propped-deltas))
</pre>
</div>
</div>
</div>
<div id="outline-container-orgbbc131a" class="outline-3">
<h3 id="orgbbc131a">test code</h3>
<div class="outline-text-3" id="text-orgbbc131a">
<p>
time for a simple propbackward test:
</p>
<div class="org-src-container">
<pre class="src src-lisp">(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">lenet-test-1</span> ()
  (<span style="font-weight: bold;">let*</span> ((x (random-tensor '(1 32 32)))
         (y (make-array '(10)))
         (out (network-feedforward *lenet5* x)))
    (network-propbackward *lenet5* x y out)))
<span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">(time (lenet-test-1))</span>
</pre>
</div>
<p>
while this worked, it was really slow:
</p>
<pre class="example">
Evaluation took:
  3.116 seconds of real time
  3.117175 seconds of total run time (3.113856 user, 0.003319 system)
  [ Run times consist of 0.028 seconds GC time, and 3.090 seconds non-GC time. ]
  100.03% CPU
  7,783,051,200 processor cycles
  7,346,708,400 bytes consed
</pre>

<p>
3 seconds for a single feedforward,backprop pass is pretty horrible for a relatively small network
i used sbcl's <a href="http://www.lichteblau.com/sbcl/doc/manual/sbcl/Statistical-Profiler.html">sb-prof</a> for more insight:
</p>
<div class="org-src-container">
<pre class="src src-lisp">(<span style="font-weight: bold;">require</span> <span style="font-weight: bold; text-decoration: underline;">:sb-sprof</span>)
(sb-sprof:with-profiling (<span style="font-weight: bold;">:report</span> <span style="font-weight: bold;">:graph</span>)
  (lenet-test-1))
</pre>
</div>
<p>
but that didnt help much, as expected most of the time was taken by the backprop functions
although there are some things that can be improved (i think im doing unnecessary memory duplication with arrays) to make the single-threaded code run faster, i just decided it was time to take a multithreaded approach (perhaps even train on the gpu in the future with <a href="/cuda_programming_in_common_lisp.html">cl-cuda</a>)
</p>
</div>
</div>
<div id="outline-container-orgc1c84f2" class="outline-3">
<h3 id="orgc1c84f2">printing/formatting networks</h3>
<div class="outline-text-3" id="text-orgc1c84f2">
<p>
it would help to be able to inspect a network and see for example how many parameters it has so we implement a <code>print-object</code> function:
</p>
<div class="org-src-container">
<pre class="src src-lisp" id="org6395bba">(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">print-object</span> ((n network) stream)
  (print-unreadable-object (n stream <span style="font-weight: bold;">:type</span> t <span style="font-weight: bold;">:identity</span> t)
    (<span style="font-weight: bold;">let</span> ((total-weights 0))
      (<span style="font-weight: bold;">loop</span> for layer in (network-layers n) do
        (<span style="font-weight: bold;">when</span> (layer-weights layer)
          (incf total-weights (array-size (layer-weights layer))))
        (format stream <span style="font-style: italic;">"~%  ~a"</span> layer))
      (format stream
              <span style="font-style: italic;">"~&amp;total network weights: ~a, learning rate: ~a"</span>
              total-weights
              (network-learning-rate n)))))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">print-object</span> ((l pooling-layer) stream)
  (print-unreadable-object (l stream <span style="font-weight: bold;">:type</span> t)
    (format stream
            <span style="font-style: italic;">"rows: ~a, columns: ~a"</span>
            (pooling-layer-rows l)
            (pooling-layer-cols l))))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">print-object</span> ((l convolutional-layer) stream)
  (print-unreadable-object (l stream <span style="font-weight: bold;">:type</span> t)
    (format stream
            <span style="font-style: italic;">"weights: ~a, dimensions: ~a"</span>
            (array-size (layer-weights l))
            (array-dimensions (layer-weights l)))))
</pre>
</div>
<p>
example usage:
</p>
<div class="org-src-container">
<pre class="src src-lisp">CL-USER&gt; *lenet5*
#&lt;NETWORK 
  #&lt;3D-CONVOLUTIONAL-LAYER weights: 150, dimensions: (6 1 5 5)&gt;
  #&lt;POOLING-LAYER rows: 2, columns: 2&gt;
  #&lt;3D-CONVOLUTIONAL-LAYER weights: 2400, dimensions: (16 6 5 5)&gt;
  #&lt;POOLING-LAYER rows: 2, columns: 2&gt;
  #&lt;FLATTEN-LAYER {1013A58653}&gt;
  #&lt;DENSE-LAYER weights: 48000, dimensions: (120 400)&gt;
  #&lt;DENSE-LAYER weights: 10080, dimensions: (84 120)&gt;
  #&lt;DENSE-LAYER weights: 840, dimensions: (10 84)&gt;
total network weights: 61470&gt;
</pre>
</div>
</div>
</div>
<div id="outline-container-org99d8863" class="outline-3">
<h3 id="org99d8863">full pass propagation and training</h3>
<div class="outline-text-3" id="text-org99d8863">
<p>
now we need a function that takes a set of examples and trains a network
</p>
<div class="org-src-container">
<pre class="src src-lisp" id="orgbfe27ef">(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">network-full-pass</span> ((nw network) x y)
  <span style="font-style: italic;">"do a full pass in the network, feedforward and propbackward (backpropagation)"</span>
  (network-propbackward nw x y (network-feedforward nw x)))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">network-train</span> ((nw network) samples <span style="font-weight: bold; text-decoration: underline;">&amp;optional</span> (epochs 1))
  <span style="font-style: italic;">"train on the given data, xs[i],ys[i] represent the input,output of the ith example, xs,ys are lists, preferrably of the simple-vector type"</span>
  (<span style="font-weight: bold;">loop</span> for epoch from 0 below epochs do
    (<span style="font-weight: bold;">loop</span> for sample in samples
          do (<span style="font-weight: bold;">let*</span> ((x (car sample))
                    (y (cdr sample)))
               (network-full-pass nw x y)))))
</pre>
</div>
</div>
</div>
<div id="outline-container-orgbd608c1" class="outline-3">
<h3 id="orgbd608c1">test code</h3>
<div class="outline-text-3" id="text-orgbd608c1">
<p>
lets try a simple example:
</p>
<div class="org-src-container">
<pre class="src src-lisp">(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">cnn-test-1</span> ()
  (<span style="font-weight: bold;">let*</span> ((xs '(#(0 0 1) #(1 1 1) #(1 0 1) #(0 1 1)))
         (ys '(#(0) #(1) #(1) #(0)))
         (data-samples (mapcar #'cons xs ys))
         (nw (make-network
              <span style="font-weight: bold;">:learning-rate</span> 0.01
              <span style="font-weight: bold;">:layers</span> (list
                       <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">(make-dense-layer :num-units 5 :prev-layer-num-units 3)</span>
                       <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">(make-dense-layer :num-units 1 :prev-layer-num-units 5)))))</span>
                       (make-3d-convolutional-layer-from-dims
                        <span style="font-weight: bold;">:dims</span> '(4 3)
                        <span style="font-weight: bold;">:activation-function</span> #'sigmoid
                        <span style="font-weight: bold;">:activation-function-derivative</span> #'sigmoid-derivative)
                       (make-3d-convolutional-layer-from-dims
                        <span style="font-weight: bold;">:dims</span> '(1 4)
                        <span style="font-weight: bold;">:activation-function</span> #'sigmoid
                        <span style="font-weight: bold;">:activation-function-derivative</span> #'sigmoid-derivative)))))
    (print <span style="font-style: italic;">"doing 100k epochs"</span>)
    (network-train nw data-samples 100000)
    (format t <span style="font-style: italic;">"~%loss: ~A"</span> (network-test nw data-samples))
    (format t <span style="font-style: italic;">"~%this should equal 0: ~a"</span> (car (car (network-feedforward nw #(0 0 1)))))
    (format t <span style="font-style: italic;">"~%this should equal 1: ~a"</span> (car (car (network-feedforward nw #(1 1 1)))))
    (format t <span style="font-style: italic;">"~%this should equal 1: ~a"</span> (car (car (network-feedforward nw #(1 0 1)))))
    (format t <span style="font-style: italic;">"~%this should equal 0: ~a"</span> (car (car (network-feedforward nw #(0 0 0)))))
    (format t <span style="font-style: italic;">"~%this should equal 0: ~a"</span> (car (car (network-feedforward nw #(0 1 1)))))))
</pre>
</div>
<p>
test run:
</p>
<div class="org-src-container">
<pre class="src src-lisp">(cnn-test-1)
</pre>
</div>

<pre class="example">

"doing 100k epochs" 
loss: 0.07140527340555194
this should equal 0: #(0.0193214891516944)
this should equal 1: #(0.9785556472163439)
this should equal 1: #(0.9836150950875087)
this should equal 0: #(0.28466690862319854)
this should equal 0: #(0.014254526557710118)
</pre>


<p>
yes this the farthest thing from perfect, but the data isnt either, we'll see how it performs on real-world data later
i gave training on mnist with cnn a shot and tackled the issue of a vanishing gradient while at it, but again, it was really slow, we must tackle the issue of speed/efficiency
</p>
</div>
</div>
<div id="outline-container-orgd928f10" class="outline-3">
<h3 id="orgd928f10">backprop optimization</h3>
<div class="outline-text-3" id="text-orgd928f10">
<div class="any" id="org78fa65f">
<p>
if we reconsider the equations <a href="/convolutional_neural_network.html">eq-convnet-weight-delta</a>, we'll find that we're doing alot of redundant work, as mentioned in <a href="/convolutional_neural_network.html">not-convnet-I-indicies</a>, there are indicies we're iterating over that we have to ignore, so we could cut them out of the loop which will tremendously reduce the number of iterations
lets reconsider the equation:
<img src="ltx/248f80105ef.svg" alt="\[ \Delta K^\ell_k[r,u,v] = \sum_{i=1}^{H_I^{\ell+1}} \sum_{j=1}^{W_I^{\ell+1}} \frac{\partial L}{\partial \hat Y_k^\ell[i,j]}\frac{\partial \hat Y_k^\ell[i,j]}{\partial K_k^\ell[r,u,v]} \]" style="height: 3.5417em; display: block" class="org-latex org-latex-block" />
basically, here we're finding the derivative of the loss function with respect to a single weight in <img src="ltx/2aac24b1aeb.svg" alt="\(K\)" style="height: 0.7089em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> indexed by <img src="ltx/5519a950137.svg" alt="\(k,r,u,v\)" style="height: 0.7981em; vertical-align: -0.1599em; display: inline-block" class="org-latex org-latex-inline" />, using the all the errors from each output pixel in <img src="ltx/7c52db4d64b.svg" alt="\(\hat Y^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />, indexed by <img src="ltx/0726ae59ed9.svg" alt="\(k,i,j\)" style="height: 0.8971em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" />, now we for a single weight with indicies <img src="ltx/5519a950137.svg" alt="\(k,r,u,v\)" style="height: 0.7981em; vertical-align: -0.1599em; display: inline-block" class="org-latex org-latex-inline" /> dont need to iterate with <img src="ltx/bb71ee7d056.svg" alt="\(k\)" style="height: 0.6991em; vertical-align: -0.0609em; display: inline-block" class="org-latex org-latex-inline" /> through the number of layers as we only need to consider the feature map that the weight affects, as each kernel with its weights only affect a single feature map in the output, here, that map is <img src="ltx/bb71ee7d056.svg" alt="\(k\)" style="height: 0.6991em; vertical-align: -0.0609em; display: inline-block" class="org-latex org-latex-inline" /> which we consider a fixed constant when we're dealing with a single weight (which is why i preferred to write <img src="ltx/bb71ee7d056.svg" alt="\(k\)" style="height: 0.6991em; vertical-align: -0.0609em; display: inline-block" class="org-latex org-latex-inline" /> as a subscript), other pixels in feature maps other than <img src="ltx/bb71ee7d056.svg" alt="\(k\)" style="height: 0.6991em; vertical-align: -0.0609em; display: inline-block" class="org-latex org-latex-inline" /> arent affected by our weight and using the errors at those pixels wouldnt affect the gradient's value (they add 0). (there are still some pixels that we can drop depending on the place of the weight in the kernel, and the kernel's stride, but i dont think that can give us a noticable improvement in performance)
the real bottleneck for training is currently computing <img src="ltx/0802b72023a.svg" alt="\(\Delta I\)" style="height: 0.7187em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> for convolutional layers, which is what we need to work on, lets reconsider the following equation
<img src="ltx/8951f79333e.svg" alt="\[ \Delta I^\ell[r,u,v] = \sum_{k=1}^{D_I^{\ell+1}} \sum_{i=1}^{H_I^{\ell+1}} \sum_{j=1}^{W_I^{\ell+1}} \frac{\partial L}{\partial \hat Y_k^\ell[i,j]}\frac{\partial \hat Y_k^\ell[i,j]}{\partial I^\ell[r,u,v]} \]" style="height: 3.5417em; display: block" class="org-latex org-latex-block" />
here, we're considering the derivative of every pixel in the output image with respect to the entry in <img src="ltx/9e788d12d3a.svg" alt="\(I^\ell[r,u,v]\)" style="height: 1.2073em; vertical-align: -0.2814em; display: inline-block" class="org-latex org-latex-inline" /> that we wanna find, but in truth, each pixel in <img src="ltx/2fb480524b0.svg" alt="\(I^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> only affects <img src="ltx/9e695c2ba5f.svg" alt="\(t^\ell \times H_K^\ell \times W_K^\ell\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" /> output pixels <b>at most</b> (this is basically the size of the weight tensor <img src="ltx/9f774ae7c31.svg" alt="\(K^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />), intuitively, one may be able to imagine this to see each kernel sliding over the input <img src="ltx/2fb480524b0.svg" alt="\(I^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />, in this process each pixel gets multiplied by a subset of the sliding kernel, depending on whether it is close to the edge of the image and on the sliding stride, each time the image slides in some direction, our input pixel gets multiplied by a new weight, until the kernel slides away and isnt "hovering" over our pixel anymore, this process is illustrated in <a href="#org35bd435">3</a>
</p>
<p>
<a href="/sliding.gif">/home/mahmooz/brain/notes/data/d0/bddd39-2180-4e8d-895b-f64bcb6472ea/sliding.gif</a>
this can also be seen in the formula for the output pixels:
<img src="ltx/715f79831bc.svg" alt="\[ \hat Y_k^\ell[i,j] = \phi\left(\sum_{z=0}^{D_K^\ell-1}\sum_{y=0}^{H_K^\ell-1}\sum_{x=0}^{W_K^\ell-1}I^\ell[z,y+i,x+j]K^\ell_k[D_K^\ell-z-1,H_K^\ell-y-1,W_K^\ell-x-1]+B^\ell[k]\right) \]" style="height: 4.3124em; display: block" class="org-latex org-latex-block" />
for each combination of the indicies <img src="ltx/0726ae59ed9.svg" alt="\(k,i,j\)" style="height: 0.8971em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" />, we iterate through a block in <img src="ltx/2fb480524b0.svg" alt="\(I^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> (and in <img src="ltx/b140fc65934.svg" alt="\(K_k^\ell\)" style="height: 1.2548em; vertical-align: -0.3289em; display: inline-block" class="org-latex org-latex-inline" />) of size <img src="ltx/989da19db4a.svg" alt="\(D_K^\ell \times W_K^\ell \times H_K^\ell\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" />, but the size of <img src="ltx/2fb480524b0.svg" alt="\(I^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" /> is <img src="ltx/f90ecd7a137.svg" alt="\(D_I^\ell \times H_I^\ell \times W_I^\ell\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" />, and the latter is usually a much greater number, so not in every combination does the arbitrary entry <img src="ltx/9e788d12d3a.svg" alt="\(I^\ell[r,u,v]\)" style="height: 1.2073em; vertical-align: -0.2814em; display: inline-block" class="org-latex org-latex-inline" /> play a role in setting the value of <img src="ltx/f1643e1ed01.svg" alt="\(\hat Y^\ell_k[i,j]\)" style="height: 1.2548em; vertical-align: -0.3289em; display: inline-block" class="org-latex org-latex-inline" />, to see how many times exactly is <img src="ltx/9e788d12d3a.svg" alt="\(I^\ell[r,u,v]\)" style="height: 1.2073em; vertical-align: -0.2814em; display: inline-block" class="org-latex org-latex-inline" /> is used we build the equations <img src="ltx/089d12a9e1b.svg" alt="\(r=z,u=y+i,v=x+j\)" style="height: 0.8588em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" /> from the indicies in the formula, and solve for <img src="ltx/0726ae59ed9.svg" alt="\(k,i,j\)" style="height: 0.8971em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" />, the values we get for <img src="ltx/0726ae59ed9.svg" alt="\(k,i,j\)" style="height: 0.8971em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" /> are the indicies of the pixels which <img src="ltx/9e788d12d3a.svg" alt="\(I^\ell[r,u,v]\)" style="height: 1.2073em; vertical-align: -0.2814em; display: inline-block" class="org-latex org-latex-inline" /> "relates to"
we get <img src="ltx/3b72a39271f.svg" alt="\(k=k,i=u-y,j=v-x\)" style="height: 0.9000em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" />, now we know that <img src="ltx/03af2776283.svg" alt="\((k,i,j) \in \mathbb{R}^{t^\ell \times (H^\ell_I-(H_K^\ell-1)) \times (W^\ell_I-(W_K^\ell-1)))}\)" style="height: 1.4366em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" />, because thats the size of the output tensor <img src="ltx/7c52db4d64b.svg" alt="\(\hat Y^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />, and we know <img src="ltx/8163ea3eba1.svg" alt="\((r,u,v) \in \mathbb{R}^{D_I^\ell \times H_I^\ell \times W_I^\ell}\)" style="height: 1.4366em; vertical-align: -0.2746em; display: inline-block" class="org-latex org-latex-inline" />.
</p>

<p>
what the equations <img src="ltx/3b72a39271f.svg" alt="\(k=k,i=u-y,j=v-x\)" style="height: 0.9000em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" /> basically tell us is that, since <img src="ltx/53c0c910d3c.svg" alt="\(j\)" style="height: 0.8559em; vertical-align: -0.2589em; display: inline-block" class="org-latex org-latex-inline" /> only depends on the value of <img src="ltx/7c2b7d259de.svg" alt="\(v\)" style="height: 0.4923em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />, which we have, and on <img src="ltx/bd05245349f.svg" alt="\(x\)" style="height: 0.5051em; vertical-align: -0.0590em; display: inline-block" class="org-latex org-latex-inline" />, which we need to iterate for, as <img src="ltx/e83b5eec2b6.svg" alt="\(0 \le x &amp;lt; W_K^\ell\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" /> we only need to do <img src="ltx/ec73c143e57.svg" alt="\(W_K^\ell \times t^\ell\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" /> for it (we have to iterate for <img src="ltx/116c7269a4e.svg" alt="\(1 \le k &amp;lt; t^\ell\)" style="height: 1.1485em; vertical-align: -0.2226em; display: inline-block" class="org-latex org-latex-inline" /> as its a free variable and doesnt depend on others), combining that with <img src="ltx/b7a09af24c5.svg" alt="\(i\)" style="height: 0.6746em; vertical-align: -0.0786em; display: inline-block" class="org-latex org-latex-inline" />, which depends on the then-constant <img src="ltx/b6baeba3ff8.svg" alt="\(u\)" style="height: 0.4913em; vertical-align: -0.0531em; display: inline-block" class="org-latex org-latex-inline" /> (<img src="ltx/78320534b1d.svg" alt="\(r,u,v\)" style="height: 0.6060em; vertical-align: -0.1599em; display: inline-block" class="org-latex org-latex-inline" /> arent constants, but in this context they are, thats the assumption here) and <img src="ltx/e4f3ae35206.svg" alt="\(y\)" style="height: 0.7109em; vertical-align: -0.2618em; display: inline-block" class="org-latex org-latex-inline" /> which varies as <img src="ltx/c43f48cda50.svg" alt="\(0 \le y &amp;lt; H_K^\ell\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" />, in total this gives us <img src="ltx/89e5cad908a.svg" alt="\(t^\ell \times W_K^\ell \times H_K^\ell\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" /> iterations for a single arbitrary entry in <img src="ltx/2fb480524b0.svg" alt="\(I^\ell\)" style="height: 0.9751em; vertical-align: -0.0492em; display: inline-block" class="org-latex org-latex-inline" />, which is definitely (alot) less (and also independent of the size of the input) than <img src="ltx/a6077d57a2c.svg" alt="\(t^\ell \times (H^\ell_I-(H_K^\ell-1)) \times (W^\ell_I-(W_K^\ell-1)))\)" style="height: 1.2626em; vertical-align: -0.3367em; display: inline-block" class="org-latex org-latex-inline" /> which is the number of iterations we were doing before.
</p>

<p>
combining all of those ideas we get the formula:
<img src="ltx/0490cc1f2e1.svg" alt="\[ \Delta I^\ell[r,u,v] = \sum_{k=0}^{t^\ell-1} \sum_{y=0}^{H_K^\ell-1} \sum_{x=0}^{W_K^\ell-1} \frac{\partial L}{\partial \hat Y^\ell[k,u-y,v-x]}\frac{\partial \hat Y_k^\ell[k,u-y,v-x]}{\partial I^\ell[r,u,v]} \]" style="height: 3.4904em; display: block" class="org-latex org-latex-block" />
we substitute this in <a href="/convolutional_neural_network.html">eq-convnet-I-delta-1</a> in place of the previous version, and redo the differentation process:
</p>

<div id="orgfb4dd57" class="equation-container">
<span class="equation">
<img src="ltx/929f48e0f29.svg" alt="\begin{align}
  \Delta I^\ell[r,u,v] &amp;amp;= \frac{\partial L}{\partial I^\ell[r,u,v]}\\
  &amp;amp;= \sum_{k=0}^{t^\ell-1} \sum_{y=0}^{H_K^\ell-1} \sum_{x=0}^{W_K^\ell-1} \frac{\partial L}{\partial \hat Y^\ell[k,u-y,v-x]}\frac{\partial \hat Y^\ell[k,u-y,v-x]}{\partial I^\ell[r,u,v]}\\
  &amp;amp;= \sum_{k=0}^{t^\ell-1} \sum_{y=0}^{H_K^\ell-1} \sum_{x=0}^{W_K^\ell-1} \frac{\partial L}{\partial \hat Y^\ell[k,u-y,v-x]}\frac{\partial \phi(S^\ell[k,u-y,v-x])}{\partial I^\ell[r,u,v]}\\
  &amp;amp;= \sum_{k=0}^{t^\ell-1} \sum_{y=0}^{H_K^\ell-1} \sum_{x=0}^{W_K^\ell-1} \Delta\hat Y^\ell[k,u-y,v-x]\frac{\partial \phi(S^\ell[k,u-y,v-x])}{\partial I^\ell[r,u,v]}\\
  &amp;amp;= \sum_{k=0}^{t^\ell-1} \sum_{y=0}^{H_K^\ell-1} \sum_{x=0}^{W_K^\ell-1} \Delta S^\ell[k,u-y,v-x]\frac{\partial S^\ell[k,u-y,v-x]}{\partial I^\ell[r,u,v]}\\
  &amp;amp;= \sum_{k=0}^{t^\ell-1} \sum_{y=0}^{H_K^\ell-1} \sum_{x=0}^{W_K^\ell-1} \Delta S^\ell[k,u-y,v-x]\frac{\partial \sum_{z_1=0}^{D_K^\ell-1}\sum_{y_1=0}^{H_K^\ell-1}\sum_{x_1=0}^{W_K^\ell-1}I^\ell[z_1,y_1+u-y,x_1+v-x]K^\ell_k[D_K^\ell-z_1-1,H_K^\ell-y_1-1,W_K^\ell-x_1-1]+B^\ell[k]}{\partial I^\ell[r,u,v]}\\
  &amp;amp;= \sum_{k=0}^{t^\ell-1} \sum_{y=0}^{H_K^\ell-1} \sum_{x=0}^{W_K^\ell-1} \Delta S^\ell[k,u-y,v-x] K_k^\ell[\underbrace{D_K^\ell-r-1,H_K^\ell-y-1,W_K^\ell-x-1}_{(D^\ell_K,H_K^\ell,W_K^\ell)-(r,y,x)-(1,1,1)}]
\end{align}
" style="height: 27.8189em; vertical-align: -0.4020em; display: inline-block" class="org-latex org-latex-inline" />
</span>
</div>
<p>
we can better the formulas for all deltas even further but thats for the future, maybe.
</p>

<p>
after coming up with this, i had to go back and edit the backpropagation function for convolutional layer to use the new formula instead of the old one
</p>

</div>
</div>
</div>
<div id="outline-container-org5dc8fed" class="outline-3">
<h3 id="org5dc8fed">distributed training</h3>
<div class="outline-text-3" id="text-org5dc8fed">
<p>
perhaps distributed training would help, so i decided to work on a simple multithreaded solution before going for a gpu solution
here implement distributed training by data parallelism with the help of lparallel, two packages are needed, <code>lparallel</code> and <code>serapeum</code>, both installable from quicklisp
</p>
<div class="org-src-container">
<pre class="src src-lisp">(ql:quickload <span style="font-weight: bold;">:serapeum</span>)
(ql:quickload <span style="font-weight: bold;">:lparallel</span>)
</pre>
</div>
<div class="org-src-container">
<pre class="src src-lisp" id="orge47595d">(<span style="font-weight: bold;">defgeneric</span> <span style="font-weight: bold;">copy</span> (obj)
  (<span style="font-weight: bold;">:documentation</span> <span style="font-style: italic;">"make a copy of an object"</span>))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">copy</span> ((nw network))
  <span style="font-style: italic;">"copy a neural network"</span>
  (make-network <span style="font-weight: bold;">:layers</span> (mapcar #'copy (network-layers nw))
                <span style="font-weight: bold;">:learning-rate</span> (network-learning-rate nw)))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">copy</span> ((l layer))
  <span style="font-style: italic;">"copy a layer, a layer that inherits from this might have to add its own code to copy objects that arent copied by the base copy (this copy)"</span>
  (<span style="font-weight: bold;">let</span> ((new-weights)
        (new-biases))
    (<span style="font-weight: bold;">when</span> (layer-weights l)
      (setf new-weights (make-array (array-dimensions (layer-weights l))))
      (copy-into-array new-weights (layer-weights l)))
    (<span style="font-weight: bold;">when</span> (layer-biases l)
      (setf new-biases (make-array (array-dimensions (layer-biases l))))
      (copy-into-array new-biases (layer-biases l)))
    (make-instance (type-of l)
                   <span style="font-weight: bold;">:weights</span> new-weights
                   <span style="font-weight: bold;">:biases</span> new-biases
                   <span style="font-weight: bold;">:activation-function</span> (layer-activation-function l)
                   <span style="font-weight: bold;">:activation-function-derivative</span> (layer-activation-function-derivative l))))

(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">copy</span> <span style="font-weight: bold;">:around</span> ((l pooling-layer))
  <span style="font-style: italic;">"a pooling-layer has to copy more objects than the usual layer in which case the base copy function is not sufficient, this fixes that"</span>
  (<span style="font-weight: bold;">let</span> ((new-layer (call-next-method)))
    (setf (pooling-layer-rows new-layer) (pooling-layer-rows l))
    (setf (pooling-layer-cols new-layer) (pooling-layer-cols l))
    (setf (pooling-layer-function new-layer) (pooling-layer-function l))
    (setf (pooling-layer-unpooling-function new-layer) (pooling-layer-unpooling-function l))
    new-layer))

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">zeroize-network-weights</span> (nw)
  <span style="font-style: italic;">"turn all the parameters of the network 0"</span>
  (<span style="font-weight: bold;">loop</span> for layer in (network-layers nw) do
    (<span style="font-weight: bold;">when</span> (layer-weights layer)
      (setf (layer-weights layer)
            (make-array (array-dimensions (layer-weights layer)))))))

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">add-network-weights</span> (dest-nw src-nw)
  <span style="font-style: italic;">"add the weights of src-nw to the weights of dest-nw, src-nw has to be a copy of dest-nw"</span>
  (<span style="font-weight: bold;">loop</span> for dest-layer in (network-layers dest-nw)
        for src-layer in (network-layers src-nw)
        do (<span style="font-weight: bold;">when</span> (layer-weights dest-layer)
             (array-map-into
              #'+
              (layer-weights dest-layer)
              (layer-weights dest-layer)
              (layer-weights src-layer)))))

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">divide-network-weights</span> (nw num)
  <span style="font-style: italic;">"divide all the weights of a network by num"</span>
  (<span style="font-weight: bold;">loop</span> for layer in (network-layers nw) do
    (<span style="font-weight: bold;">when</span> (layer-weights layer)
      (array-map-into
       (<span style="font-weight: bold;">lambda</span> (weight) (/ weight num))
       (layer-weights layer)
       (layer-weights layer)))))

<span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">whether to terminate training or not</span>
(<span style="font-weight: bold;">defparameter</span> <span style="font-weight: bold; font-style: italic;">*lparallel-training*</span> nil)

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">network-train-distributed-cpu</span> (nw samples <span style="font-weight: bold; text-decoration: underline;">&amp;optional</span> (epochs 2))
  <span style="font-style: italic;">"samples should be conses of type simple-vector, train nw with lparallel with the number of cores your cpu has"</span>
  (setf *lparallel-training* t)
  (<span style="font-weight: bold;">let*</span> ((nw-alias (copy nw)) <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">updates are done to nw-alias, at the end of training copied to nw</span>
         (batch-size 10) <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">each core is gonna get that many x,y samples</span>
         (workers (serapeum:count-cpus)) <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">set workers to number of available cpu cores</span>
         (total-samples (length samples))
         (total-batches (floor total-samples batch-size)) <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">floor is just integer division here</span>
         (lparallel:*kernel* (lparallel:make-kernel workers))) <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">lparallel's kernel takes care of parallelism</span>
    (<span style="font-weight: bold;">when</span> (&gt; (mod total-samples batch-size) 0) (incf total-batches 1))

    <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">total-rounds is the number of times we construct workers and give them each a network to train</span>
    (<span style="font-weight: bold;">loop</span> for epoch from 0 below epochs do
      (<span style="font-weight: bold;">let*</span> ((total-rounds (floor total-batches workers))
             (channel (lparallel:make-channel))
             (batch-idx 0))
        (<span style="font-weight: bold;">when</span> (&gt; (mod total-batches workers) 0) (incf total-rounds))
        <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">on each round we push batches to workers</span>
        (<span style="font-weight: bold;">loop</span> for round from 0 below total-rounds while *lparallel-training* do
          (<span style="font-weight: bold;">let</span> ((active-workers 0) <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">on a round we might not need all the workers so we gotta keep track of how many workers are actually active to know how many results to ask the lparallel kernel for</span>
                (lparallel:*task-category* 'nn)) <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">this allows for (lparallel:kill-tasks 'nn)</span>
            (<span style="font-weight: bold;">loop</span> for worker-idx from 0 below workers do
              (<span style="font-weight: bold;">when</span> (&lt; batch-idx total-batches)
                (<span style="font-weight: bold;">let</span> ((batch-samples (subseq samples
                                             (* batch-idx batch-size)
                                             (+ (* batch-idx batch-size) batch-size)))
                      (nw-copy (copy nw-alias)))
                  (format t <span style="font-style: italic;">"pushing batch ~a/~a~%"</span> (1+ batch-idx) total-batches)
                  (lparallel:submit-task
                   channel
                   (<span style="font-weight: bold;">lambda</span> ()
                     (<span style="font-weight: bold;">loop</span> for sample in batch-samples while *lparallel-training* do
                       (<span style="font-weight: bold;">let*</span> ((x (car sample))
                              (y (cdr sample))
                              (out (network-feedforward nw-copy x)))
                         (network-propbackward nw-copy x y out)))
                     nw-copy)))
                (incf batch-idx 1)
                (incf active-workers 1)))
            <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">reset the nw weights to 0, as its new weights will be the averages of the copies</span>
            (zeroize-network-weights nw-alias)
            (<span style="font-weight: bold;">loop</span> for worker-idx from 0 below active-workers do
              (<span style="font-weight: bold;">let</span> ((trained-nw-copy (lparallel:receive-result channel)))
                <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">(format t "received from worker ~a~%" worker-idx)</span>
                (add-network-weights nw-alias trained-nw-copy)))
            <span style="font-weight: bold; font-style: italic;">;; </span><span style="font-weight: bold; font-style: italic;">(format t "~a workers done~%" active-workers)</span>
            (divide-network-weights nw-alias active-workers)
            (zeroize-network-weights nw)
            (add-network-weights nw nw-alias)))
        (format t <span style="font-style: italic;">"~%epoch done~A~%"</span> epoch))
      (lparallel:end-kernel))))
</pre>
</div>
<p>
it seems that the <code>kill-tasks</code> function from the <code>lparallel</code> library doesnt work, so i had to implement my own way of terminating the threads and the training process using <code>*lparallel-training*</code>, so when we wanna stop training we set it to <code>nil</code>, using perhaps <code>slime-interactive-eval</code> if using slime
although cpu multithreading can speed up training, a gpu is still alot faster in cases like this
</p>
</div>
</div>
<div id="outline-container-org95b6d20" class="outline-3">
<h3 id="org95b6d20">analyzing accuracy and loss</h3>
<div class="outline-text-3" id="text-org95b6d20">
<p>
we also need a function to get the total error of the network:
</p>
<div class="org-src-container">
<pre class="src src-lisp" id="org7911b66">(<span style="font-weight: bold;">defmethod</span> <span style="font-weight: bold;">network-test</span> ((nw network) samples)
  (<span style="font-weight: bold;">let</span> ((loss 0))
    (<span style="font-weight: bold;">loop</span> for sample in samples do
      (<span style="font-weight: bold;">let</span> ((x (car sample))
            (y (cdr sample)))
        (<span style="font-weight: bold;">multiple-value-bind</span> (activations unsquashed-activations)
            (network-feedforward nw x)
          (<span style="font-weight: bold;">let*</span> ((output-layer (car (car activations)))
                 (loss-to-add (reduce-array (array-map #'abs (array-map #'- output-layer y)) #'+)))
            (incf loss loss-to-add)))))
    loss))
</pre>
</div>
</div>
</div>
<div id="outline-container-orgd1798cb" class="outline-3">
<h3 id="orgd1798cb">saving parameters to disk</h3>
<div class="outline-text-3" id="text-orgd1798cb">
<p>
we may also want to save the weights of a network to a file for later reloading:
</p>
<div class="math-block note" data-before="note" data-after="" id="org468fc02">
<p>
TODO: currently this doesnt save biases, needs to be fixed
</p>

</div>
<div class="org-src-container">
<pre class="src src-lisp" id="org4493237">(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">network-save-weights</span> (nw filepath)
  <span style="font-style: italic;">"save the weight tensors of the layers of the network nw to file specified by filepath"</span>
  (<span style="font-weight: bold;">let</span> ((weight-tensors-list))
    (<span style="font-weight: bold;">with-open-file</span> (stream filepath
                            <span style="font-weight: bold;">:direction</span> <span style="font-weight: bold;">:output</span>
                            <span style="font-weight: bold;">:if-exists</span> <span style="font-weight: bold;">:supersede</span>
                            <span style="font-weight: bold;">:if-does-not-exist</span> <span style="font-weight: bold;">:create</span>)
      (<span style="font-weight: bold;">loop</span> for layer in (network-layers nw) do
        (setf weight-tensors-list
              (append weight-tensors-list (list (layer-weights layer)))))
      (format stream <span style="font-style: italic;">"~A~%"</span> weight-tensors-list))))

(<span style="font-weight: bold;">defun</span> <span style="font-weight: bold;">network-load-weights</span> (nw filepath)
  <span style="font-style: italic;">"load the weight tensors of a network nw from file specified by filepath"</span>
  (<span style="font-weight: bold;">let</span> ((weight-tensors-list (<span style="font-weight: bold;">with-open-file</span> (in filepath)
                               (read in))))
    (<span style="font-weight: bold;">loop</span> for layer in (network-layers nw) do
      (<span style="font-weight: bold;">let</span> ((weight-tensor (pop weight-tensors-list)))
        (setf (layer-weights layer) weight-tensor)))))
</pre>
</div>
<p>
we simply save them in their lisp representation, and read them later as lisp objects</p>
</div>
</div>
</div>
</div>
</body>
</html>
