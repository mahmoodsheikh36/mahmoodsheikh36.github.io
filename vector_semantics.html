<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="mahmood sheikh" />
<meta name="generator" content="Org Mode" />
<title>vector semantics</title><!-- lambda icon, frail attempt -->
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%221em%22 font-size=%22100%22 color=%22red%22>Î»</text></svg>">
<!-- not-so-awesome awesome font -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
<link rel="stylesheet" href="/main.css">
<!-- for dark mode -->
<script src="darkmode.js"></script>
<script src="search.js"></script>
<script src="main.js"></script>
</head>
<body>
<div id="preamble" class="status">
<div class="navbar">
  <a href='/'>home</a>
  <a href='/blog.html'>blog</a>
  <a href='/search.html'>search</a>
  <a href='/about.html'>about</a>
  <div id="darkModeToggle" onclick="toggleDarkMode()">
    &#9680; <!-- Circle with left half black -->
  </div>
</div><h1 class="main-title">vector semantics</h1>
</div>
<div id="content" class="content">
<div class="dummy" id="org0aa2144">
<p>
vectors semantics is the standard way to represent word meaning in nlp, helping us model many of the aspects of word meaning we saw in the previous section. the roots of the model lie in the 1950s when two big ideas converged: Osgood's (1957) idea mentioned above to use a point in three-dimensional space to represent the connotation of a word, and the proposal by linguists like Joos (1950), Harris (1954), and Firth (1957) to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings.<br />
<i>vector semantics</i> instantiates the linguistic <a href="/distributional_hypothesis.html#1714589793">distributional hypothesis</a> by learning representations of the meanings of words, called 1714590068, directly from their distributions in texts. these representations are used in every natural language processing application that makes use of meaning, and the static embeddings like BERT.<br />
the idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived from the distributions of word neighbors. <a href="/vector.html#def-vec">vector</a>s for representing words are called <i>embeddings</i> (although the term is sometimes more strictly applied only to dense vectors like word2vec, rather than sparse tf-idf or PPMI vectors).<br />
The word "embedding" derives from its mathematical sense as a mapping from one space or structure to another, although the meaning has shifted; see the end of (refer to Daniel Jurafsky, James H. Martin, 2020 chapter 6 vector semantics and embeddings).<br />
</p>

</div>
</div>
</body>
</html>
