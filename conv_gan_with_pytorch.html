<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="mahmood" />
<meta name="description" content="an example convolutional generative adversarial network written with pytorch" />
<meta name="generator" content="Org Mode" />
<title>conv gan with pytorch</title><!-- lambda icon, frail attempt -->
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%221em%22 font-size=%22100%22 color=%22red%22>Î»</text></svg>">
<!-- not-so-awesome awesome font -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">

<link rel="stylesheet" href="/main.css">
</head>
<body>
<div id="preamble" class="status">
<div class="navbar">
  <a href='/'>home</a>
  <a href='/blog.html'>blog</a>
  <a href='/archive.html'>archive</a>
  <a href='/about.html'>about</a>
</div><h1 class="main-title">conv gan with pytorch</h1><span class="desc"></span>
</div>
<div id="content" class="content">
<ul class="org-ul">
<li>convolutional generative adversarial network</li>
<li>pytorch</li>
</ul>

<div class="math-block note" data-before="note" data-after="" id="org7352cea">
<p>
some information was quoted from <a href="https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras/">this great article</a> (the code was rewritten in pytorch and alot of things differ though!).
</p>

</div>
<div id="outline-container-org47e77a1" class="outline-2">
<h2 id="org47e77a1">essential packages</h2>
<div class="outline-text-2" id="text-org47e77a1">
<p>
we need to import some essential packages:
</p>
<div class="org-src-container">
<pre class="src src-python" id="org57e21de"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-activate-venv()&gt;&gt;</span>
<span style="font-weight: bold;">import</span> matplotlib.pyplot <span style="font-weight: bold;">as</span> plt
<span style="font-weight: bold;">import</span> torch
<span style="font-weight: bold;">import</span> torch.nn <span style="font-weight: bold;">as</span> nn
<span style="font-weight: bold;">import</span> torch.nn.functional <span style="font-weight: bold;">as</span> F
torch.manual_seed(0) <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">for reproducibility</span>
</pre>
</div>
<p>
it's common <code>PyTorch</code> practice to initialize a variable, usually named <code>device</code>, that will hold the device we're training on (CPU or GPU).
</p>

<p>
here, if the machine we're using has a cuda-supported gpu available, we use that, otherwise we use the cpu, this variable stores our choice and is used later to pass data to the training device, if we're training on the cpu, there would be no point in copying the data from the cpu memory to another block of cpu memory, but it would still work.
</p>

<div class="org-src-container">
<pre class="src src-python" id="orgd873c37"><span style="font-weight: bold; font-style: italic;">device</span> = torch.device(<span style="font-style: italic;">"cuda:0"</span> <span style="font-weight: bold;">if</span> torch.cuda.is_available() <span style="font-weight: bold;">else</span> <span style="font-style: italic;">"cpu"</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-org2ad282f" class="outline-2">
<h2 id="org2ad282f">load the data</h2>
<div class="outline-text-2" id="text-org2ad282f">
<p>
we start by loading the mnist dataset (the 28x28 images of hand-written digits), we use the <code>torchvision</code> package from <code>pypi</code> which provides various datasets for machine learning
</p>
<div class="org-src-container">
<pre class="src src-python" id="org2b08756"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-essential-imports()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-pytorch-init()&gt;&gt;</span>
<span style="font-weight: bold;">import</span> torchvision.datasets
<span style="font-weight: bold;">import</span> os.path
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">download dataset to user's home directory and load it</span>
<span style="font-weight: bold; font-style: italic;">train_dataset</span> = torchvision.datasets.MNIST(root=os.path.expanduser(<span style="font-style: italic;">'~'</span>), download=<span style="font-weight: bold; text-decoration: underline;">True</span>, train=<span style="font-weight: bold; text-decoration: underline;">True</span>)
<span style="font-weight: bold; font-style: italic;">test_dataset</span> = torchvision.datasets.MNIST(root=os.path.expanduser(<span style="font-style: italic;">'~'</span>), download=<span style="font-weight: bold; text-decoration: underline;">True</span>, train=<span style="font-weight: bold; text-decoration: underline;">False</span>)
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">print some info</span>
<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">'train dataset:'</span>, train_dataset)
<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">'train data shape:'</span>, train_dataset.data.shape)
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">show some info about the first image in the train dataset</span>
<span style="font-weight: bold; font-style: italic;">img</span>, <span style="font-weight: bold; font-style: italic;">label</span> = test_dataset[0]
<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">'img:'</span>, img)
<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">'img size:'</span>, img.size)
<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">'label:'</span>, label)
</pre>
</div>

<pre class="example">
train dataset: Dataset MNIST
    Number of datapoints: 60000
    Root location: /home/mahmooz
    Split: Train
train data shape: torch.Size([60000, 28, 28])
img: &lt;PIL.Image.Image image mode=L size=28x28 at 0x7F2C4F3D9650&gt;
img size: (28, 28)
label: 7
</pre>


<p>
the function <code>gen_images_img</code> below is used to easily display a number of images in a grid layout:
</p>
<div class="org-src-container">
<pre class="src src-python" id="orge082574"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">generate an image of images and write it to a file</span>
<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">gen_images_img</span>(imgs, out_file, suptitle=<span style="font-weight: bold; text-decoration: underline;">None</span>):
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">make sure images has a size that is divisible by 10</span>
    <span style="font-weight: bold; font-style: italic;">n_cols</span> = 10
    <span style="font-weight: bold; font-style: italic;">n_rows</span> = imgs.shape[0] // n_cols
    <span style="font-weight: bold; font-style: italic;">num_imgs</span> = n_rows * n_cols
    <span style="font-weight: bold; font-style: italic;">imgs</span> = imgs[:num_imgs]
    <span style="font-weight: bold; font-style: italic;">fig</span> = plt.figure(figsize=(n_cols, n_rows))
    <span style="font-weight: bold;">for</span> i, img <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">enumerate</span>(imgs):
        plt.subplot(n_rows, n_cols, 1 + i)
        plt.axis(<span style="font-style: italic;">'off'</span>)
        plt.imshow(img, cmap=<span style="font-style: italic;">'gray_r'</span>)
    <span style="font-weight: bold;">if</span> suptitle:
        fig.suptitle(suptitle)
    plt.savefig(out_file, transparent=<span style="font-weight: bold; text-decoration: underline;">True</span>, bbox_inches=<span style="font-style: italic;">'tight'</span>)
    <span style="font-weight: bold;">return</span> out_file
</pre>
</div>
<p>
example usage of the function:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-load-dataset()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-gen-images-img()&gt;&gt;</span>
gen_images_img(train_x[:30].cpu().reshape((-1, 28, 28)), f)
</pre>
</div>


<div id="org0a516a2" class="figure">
<p><img src="/zuBb2Oa.png" />
</p>
</div>
</div>
</div>
<div id="outline-container-org9d862bd" class="outline-2">
<h2 id="org9d862bd">discriminator</h2>
<div class="outline-text-2" id="text-org9d862bd">
<p>
the discriminator model must take a sample image from our dataset as input and output a classification, a prediction as to whether the sample is real or fake. or perhaps the likelihood of the sample being real, which means the discriminator does binary classification.
</p>

<p>
pytorch makes defining neural nets easy by inheriting from <code>nn.Module</code>, here, since we're dealing with images, we use <a href="/convolutional_neural_network.html">convolutional layer</a>s, in combinations with dropout layers which arent "mandatory" but they help improve results, the last layers are a <a href="/convolutional_neural_network.html">flatten layer</a> and a linear layer, the output for a single image would be a float in the range 0-1.
</p>

<div class="org-src-container">
<pre class="src src-python" id="org0bbca84"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-essential-imports()&gt;&gt;</span>
<span style="font-weight: bold;">class</span> <span style="font-weight: bold; text-decoration: underline;">Discriminator</span>(nn.Module):
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__init__</span>(<span style="font-weight: bold;">self</span>):
        <span style="font-weight: bold;">super</span>(Discriminator, <span style="font-weight: bold;">self</span>).__init__()
        <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">conv1</span> = nn.Conv2d(1, 64, (3, 3), stride=(2, 2))
        <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">dropout1</span> = nn.Dropout(0.4)
        <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">conv2</span> = nn.Conv2d(64, 64, (3, 3), stride=(2, 2))
        <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">dropout2</span> = nn.Dropout(0.4)
        <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">flatten</span> = nn.Flatten()
        <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">linear</span> = nn.Linear(2304, 1)

    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">forward</span>(<span style="font-weight: bold;">self</span>, x):
        <span style="font-weight: bold; font-style: italic;">x</span> = F.relu(<span style="font-weight: bold;">self</span>.conv1(x))
        <span style="font-weight: bold; font-style: italic;">x</span> = <span style="font-weight: bold;">self</span>.dropout1(x)
        <span style="font-weight: bold; font-style: italic;">x</span> = F.relu(<span style="font-weight: bold;">self</span>.conv2(x))
        <span style="font-weight: bold; font-style: italic;">x</span> = <span style="font-weight: bold;">self</span>.dropout2(x)
        <span style="font-weight: bold; font-style: italic;">x</span> = <span style="font-weight: bold;">self</span>.flatten(x)
        <span style="font-weight: bold; font-style: italic;">x</span> = F.sigmoid(<span style="font-weight: bold;">self</span>.linear(x))
        <span style="font-weight: bold;">return</span> x

    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">freeze</span>(<span style="font-weight: bold;">self</span>):
      <span style="font-weight: bold;">for</span> p <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">self</span>.parameters():
          p.requires_grad_(<span style="font-weight: bold; text-decoration: underline;">False</span>)
</pre>
</div>
<p>
because the module inherits from <code>nn.Module</code>, it already is printable and aware of its layers and parameters:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-discriminator()&gt;&gt;</span>
Discriminator()
</pre>
</div>

<pre class="example">
Discriminator(
  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(2, 2))
  (dropout1): Dropout(p=0.4, inplace=False)
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2))
  (dropout2): Dropout(p=0.4, inplace=False)
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear): Linear(in_features=2304, out_features=1, bias=True)
)
</pre>


<p>
compare the output above to the <code>torch-summary</code> package from <code>pypi</code> which provides the summary function in a similar fashion to the <code>keras</code> library:
</p>
<div class="org-src-container">
<pre class="src src-python" id="org0c9e375"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-discriminator()&gt;&gt;</span>
<span style="font-weight: bold;">from</span> torchsummary <span style="font-weight: bold;">import</span> summary
summary(Discriminator().cuda(), (1, 28, 28))
</pre>
</div>

<pre class="example" id="orge617069">
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 13, 13]             640
           Dropout-2           [-1, 64, 13, 13]               0
            Conv2d-3             [-1, 64, 6, 6]          36,928
           Dropout-4             [-1, 64, 6, 6]               0
           Flatten-5                 [-1, 2304]               0
            Linear-6                    [-1, 1]           2,305
================================================================
Total params: 39,873
Trainable params: 39,873
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 0.22
Params size (MB): 0.15
Estimated Total Size (MB): 0.37
----------------------------------------------------------------
</pre>

<p>
a simple feed-forwarding test with a random tensor:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-discriminator()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;">disc</span> = Discriminator()
<span style="font-weight: bold; font-style: italic;">t</span> = disc(torch.randn(1, 1, 28, 28))
<span style="font-weight: bold;">print</span>(t.shape)
<span style="font-weight: bold;">print</span>(t)
</pre>
</div>

<pre class="example">
torch.Size([1, 1])
tensor([[0.4682]], grad_fn=&lt;SigmoidBackward0&gt;)
</pre>


<p>
the output tensor is simple a one-element (batched) tensor, as expected.
</p>

<p>
we could start training this model with real examples with a class label of one (for real) and randomly generated samples with a class label of zero (for fake).
</p>

<p>
we prepare the inputs and the desired outputs (x's and y's) of the network and copy them to the device that is used for training (could be gpu or cpu depending on the machine)
</p>
<div class="org-src-container">
<pre class="src src-python" id="orga4db318"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-load-dataset()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">copy the data from the mnist dataset for further manipulation</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">.detach().clone() is the recommended way of cloning a tensor with pytorch</span>
<span style="font-weight: bold; font-style: italic;">train_x</span> = train_dataset.data.detach().clone().to(device)
<span style="font-weight: bold; font-style: italic;">train_y</span> = train_dataset.targets.detach().clone().to(device)
</pre>
</div>
<p>
as the values in the original images range from 0 to 255 (for grayscale colors), we must normalize the data before training, we map those values onto the interval <img src="ltx/f6bf01adba0.svg" alt="\([0,1]\)" style="height: 1.0874em; vertical-align: -0.2773em; display: inline-block" class="org-latex org-latex-inline" /> which is easier to work with, this is standard practice in ML. we also need to expand the dimensions of the images into 3d because our models expect 3d tensors and the mnist dataset only contains 2d greyscaled images, so a batch of images would be of 4 dimensions:
</p>
<div class="org-src-container">
<pre class="src src-python" id="org9580d16"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-make-training-tensors()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">here, None means insert a new dimension, : means keep the original dimension, so we're adding a new dimension at position 2 (the old dimension of index 2 gets shifted into index 3) and preserving the ones at 0,1</span>
<span style="font-weight: bold; font-style: italic;">train_x</span> = train_x[:, <span style="font-weight: bold; text-decoration: underline;">None</span>, :, :]
<span style="font-weight: bold; font-style: italic;">train_x</span> = train_x / 255.0
<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">'before:'</span>, train_dataset.data.shape)
<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">'after:'</span>, train_x.shape)
</pre>
</div>

<pre class="example">
before: torch.Size([60000, 28, 28])
after: torch.Size([60000, 1, 28, 28])
</pre>
</div>
<div id="outline-container-orgfbfab99" class="outline-3">
<h3 id="orgfbfab99">training the discriminator</h3>
<div class="outline-text-3" id="text-orgfbfab99">
<p>
we could systematically enumerate all samples in the training dataset, and that is a good approach, but good training via stochastic gradient descent requires that the training dataset be shuffled prior to each epoch. a simpler approach is to select random samples of images from the training dataset.
</p>

<p>
the <code>generate_real_samples_for_discriminator()</code> function below takes the training dataset as an argument and selects a random subset of images; it also returns class labels for the images, specifically a class label of 1, to indicate that the images are real.
</p>

<div class="org-src-container">
<pre class="src src-python" id="orgf04bfba"><span style="font-weight: bold;">def</span> <span style="font-weight: bold;">generate_real_samples_for_discriminator</span>(dataset, n_samples):
    <span style="font-weight: bold; font-style: italic;">random_indicies</span> = torch.randint(dataset.shape[0], (n_samples,))
    <span style="font-weight: bold; font-style: italic;">x</span> = dataset[random_indicies]
    <span style="font-weight: bold; font-style: italic;">y</span> = torch.ones((n_samples, 1))
    <span style="font-weight: bold;">return</span> x, y
</pre>
</div>
<p>
we also need a source of fake images. we can generate images comprised of random pixel values, specifically random pixel values in the range <code>[0,1]</code> like our normalized real images.
</p>

<p>
the <code>generate_fake_samples_for_discriminator()</code> function below implements this behavior and generates images of random pixel values and their associated class label of 0, for fake.
</p>

<div class="org-src-container">
<pre class="src src-python" id="org0cee96a"><span style="font-weight: bold;">def</span> <span style="font-weight: bold;">generate_fake_samples_for_discriminator</span>(n_samples):
    <span style="font-weight: bold; font-style: italic;">x</span> = torch.randn((n_samples, 1, 28, 28))
    <span style="font-weight: bold; font-style: italic;">y</span> = torch.zeros((n_samples, 1))
    <span style="font-weight: bold;">return</span> x, y
</pre>
</div>
<p>
finally, we need to train the discriminator model. this involves repeatedly retrieving samples of real images and samples of generated (fake) images and updating the model for a fixed number of iterations.
</p>

<p>
we will ignore the idea of epochs for now (e.g. complete passes through the training dataset) and fit the discriminator model for a fixed number of batches. the model will learn to discriminate between real and fake images rapidly and it doesnt take many batches before it learns to discriminate perfectly.
</p>

<p>
the <code>train_discriminator()</code> function implements this, using a batch size of 256 images where 128 are real and 128 are fake each iteration.
</p>

<p>
we update the discriminator separately for real and fake examples so that we can calculate the accuracy of the model on each sample prior to the update. this gives insight into how the discriminator model is performing over time. we also use the matplotlib library to plot the loss over time.
</p>

<div class="org-src-container">
<pre class="src src-python" id="orgcaa2f6b"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-normalize-training-tensors()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-gen-real-samples()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-gen-fake-samples-for-discriminator()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-discriminator()&gt;&gt;</span>
<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">train_discriminator</span>(model, dataset, n_iter=100, batch_size=256):
    <span style="font-weight: bold; font-style: italic;">half_batch</span> = <span style="font-weight: bold;">int</span>(batch_size / 2)

    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">use the stochastic gradient descent optimizer</span>
    <span style="font-weight: bold; font-style: italic;">optimizer</span> = torch.optim.SGD(model.parameters(), lr=0.002)
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">use the squared error loss function</span>
    <span style="font-weight: bold; font-style: italic;">loss_fn</span> = nn.MSELoss()
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">to store loss history</span>
    <span style="font-weight: bold; font-style: italic;">real_loss_hist</span> = [] <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">loss of classification of real images</span>
    <span style="font-weight: bold; font-style: italic;">fake_loss_hist</span> = [] <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">loss of classification of fake images</span>

    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">each iteration handles a batch, but each batch is basically half real half fake images, so kinda 2 batches</span>
    <span style="font-weight: bold;">for</span> i <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">range</span>(n_iter):
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">grab real samples</span>
        <span style="font-weight: bold; font-style: italic;">X_real</span>, <span style="font-weight: bold; font-style: italic;">y_real</span> = generate_real_samples_for_discriminator(dataset, half_batch)
        <span style="font-weight: bold; font-style: italic;">X_real</span> = X_real.to(device)
        <span style="font-weight: bold; font-style: italic;">y_real</span> = y_real.to(device)

        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">train on real samples</span>
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">zero your gradients for every batch!</span>
        optimizer.zero_grad()
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">make predictions for this batch</span>
        <span style="font-weight: bold; font-style: italic;">outputs</span> = model(X_real)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">compute the loss and its gradients</span>
        <span style="font-weight: bold; font-style: italic;">loss_real</span> = loss_fn(outputs, y_real)
        loss_real.backward()
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">adjust learning weights</span>
        optimizer.step()

        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">grab fake samples</span>
        <span style="font-weight: bold; font-style: italic;">X_fake</span>, <span style="font-weight: bold; font-style: italic;">y_fake</span> = generate_fake_samples_for_discriminator(half_batch)
        <span style="font-weight: bold; font-style: italic;">X_fake</span> = X_fake.to(device)
        <span style="font-weight: bold; font-style: italic;">y_fake</span> = y_fake.to(device)

        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">train on fake samples</span>
        optimizer.zero_grad()
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">make predictions for this batch</span>
        <span style="font-weight: bold; font-style: italic;">outputs</span> = model(X_fake)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">compute the loss and its gradients</span>
        <span style="font-weight: bold; font-style: italic;">loss_fake</span> = loss_fn(outputs, y_fake)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">compute the gradients (backpropagation)</span>
        loss_fake.backward()
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">adjust learning weights</span>
        optimizer.step()

        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">.numpy() to make tensors usable by matplotlib</span>
        real_loss_hist.append(loss_real.item())
        fake_loss_hist.append(loss_fake.item())
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">print(f'loss fake: {loss_fake}, loss real: {loss_real}')</span>

    <span style="font-weight: bold;">return</span> real_loss_hist, fake_loss_hist
</pre>
</div>
<p>
an example run of the function:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-train-discriminator()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;">model</span> = Discriminator().to(device)
<span style="font-weight: bold; font-style: italic;">real_loss_hist</span>, <span style="font-weight: bold; font-style: italic;">fake_loss_hist</span> = train_discriminator(model, train_x, n_iter=500)
plt.figure()
plt.plot(real_loss_hist, label=<span style="font-style: italic;">'real images'</span>)
plt.plot(fake_loss_hist, label=<span style="font-style: italic;">'fake images'</span>)
plt.xlabel(<span style="font-style: italic;">'iteration'</span>)
plt.ylabel(<span style="font-style: italic;">'loss'</span>)
plt.title(<span style="font-style: italic;">'discriminator classification loss per batch/iteration'</span>)
plt.legend()
plt.savefig(f, transparent=<span style="font-weight: bold; text-decoration: underline;">True</span>)
f
</pre>
</div>


<div id="org69f7175" class="figure">
<p><img src="/ar1T72O.svg" />
</p>
</div>

<p>
as we can see, the loss gradually declines which means the discriminator (theoretically) gets better at recognizing fake images from real ones.
</p>
</div>
</div>
</div>
<div id="outline-container-org098fae7" class="outline-2">
<h2 id="org098fae7">generator</h2>
<div class="outline-text-2" id="text-org098fae7">
<p>
the generator model is responsible for creating new, fake but plausible images of handwritten digits. it does so by taking a point from the latent space as input and outputting a tensor representing a grayscale image.
</p>

<p>
the latent space is an arbitrarily defined finite vector space. it has no meaning, but by drawing points from this space and feeding them to the generator model during training, the generator model will assign meaning to the latent points and, in turn, to the latent space. at the end of training, the latent vector space would have an internal structure that is a compressed representation of the output space (the MNIST image dataset), that only the generator knows how to turn into plausible images that fool the discriminator.
</p>

<p>
in our case, developing a generator model requires that we transform a vector from the latent space to a 2D array with 28x28 values that represent a grayscale image. there are a number of ways to achieve this but there is one approach that has proven effective in deep convolutional generative adversarial network. it involves two main elements:
</p>
<ul class="org-ul">
<li>first, we use a <a href="/convolutional_neural_network.html">dense layer</a> as the first hidden layer that has enough nodes to represent a low-resolution version of the output image. specifically, an image half the size (one quarter the area) of the output image.</li>

<li>second, we dont just want one low-resolution version of the image; we want many parallel versions or interpretations of the input. this is a pattern in convolutional neural networks where we have many parallel filters resulting in multiple parallel activation maps, called feature maps, with different interpretations of the input. we want the same thing in reverse: many parallel versions of our output with different learned features that can be collapsed in the output layer into a final image.</li>
</ul>

<p>
therefore, the first hidden layer (the dense one) needs enough nodes for multiple low-resolution versions of our output image, in our case 128 images sized 7x7 (or more accurately, 1x7x7), so there would be a total of 6272 nodes in our first dense layer.
</p>

<p>
the activations from these nodes can then be reshaped into something image-like to pass into a convolutional layer, such as 128 different 7x7 feature maps.
</p>

<p>
the next major architectural innovation involves upsampling the low-resolution image to a higher resolution version of the image. there are two common ways to do this upsampling process, sometimes called <i>deconvolution</i>. one way is to use a 2d <code>Upsample</code> layer (like a reverse pooling layer) followed by a normal <code>Conv2d</code> layer. the other and perhaps more modern way is to combine these two operations into a single layer, called a <code>ConvTranspose2d</code>. we will use this latter approach for our generator.
</p>

<p>
the <code>ConvTranspose2d</code> layer can be configured with a stride of (2x2) that will quadruple the area of the input feature maps (double their width and height dimensions). it is also good practice to use a kernel size that is a factor of the stride (e.g. double) to avoid a checkerboard pattern that can be observed when upsampling. we also use a padding of 1x1 in correspondance with the stride size of 2x2 in the hidden layers to preserve the size of the output of the convolution operation.
</p>

<p>
this pattern of upsampling can be repeated to arrive at our 28x28 output image.
</p>

<p>
the output layer of the model is a <code>Conv2d</code> with one filter and a kernel size of 7x7 and padding of size 1x1, designed to create a single feature map and preserve its dimensions at 28x28 pixels that represent our final grayscale image. a sigmoid activation is used to ensure output values are in the desired range of [0,1].
</p>

<div class="org-src-container">
<pre class="src src-python" id="orge40ccb6"><span style="font-weight: bold;">class</span> <span style="font-weight: bold; text-decoration: underline;">Generator</span>(nn.Module):
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__init__</span>(<span style="font-weight: bold;">self</span>, latent_size):
        <span style="font-weight: bold;">super</span>(Generator, <span style="font-weight: bold;">self</span>).__init__()
        <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">linear1</span> = nn.Linear(latent_size, 128 * 7 * 7)
        <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">conv_tr_2d_1</span> = nn.ConvTranspose2d(128, 128, (4,4), stride=(2,2), padding=(1,1))
        <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">conv_tr_2d_2</span> = nn.ConvTranspose2d(128, 128, (4,4), stride=(2,2), padding=(1,1))
        <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">conv2d</span> = nn.Conv2d(128, 1, (7,7), padding=(3,3))
        <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">latent_size</span> = latent_size

    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">forward</span>(<span style="font-weight: bold;">self</span>, x):
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">foundation for 128 images of size 7x7, so 128x7x7 images in total which resemble the features of the desired output image (which is an image of a digit 0-9)</span>
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">we need to pad the images properly..</span>
        <span style="font-weight: bold; font-style: italic;">x</span> = <span style="font-weight: bold;">self</span>.linear1(x)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">NOTE: why does uncommenting the following line, which results in the first layer having an activation function, mess up the whole model and cause it to diverge and spiral into chaos? i havent figured this out yet</span>
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">x = F.leaky_relu(x)</span>
        <span style="font-weight: bold; font-style: italic;">x</span> = x.view((-1, 128, 7, 7))

        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">upscale to 14x14</span>
        <span style="font-weight: bold; font-style: italic;">x</span> = <span style="font-weight: bold;">self</span>.conv_tr_2d_1(x)
        <span style="font-weight: bold; font-style: italic;">x</span> = F.leaky_relu(x, 0.2)

        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">upscale to 28x28</span>
        <span style="font-weight: bold; font-style: italic;">x</span> = <span style="font-weight: bold;">self</span>.conv_tr_2d_2(x)
        <span style="font-weight: bold; font-style: italic;">x</span> = F.leaky_relu(x, 0.2)

        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">final conv layer - output layer</span>
        <span style="font-weight: bold; font-style: italic;">x</span> = <span style="font-weight: bold;">self</span>.conv2d(x)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">was sigmoid, switched to tanh, NOTE: why does using sigmoid (yet again, see note above) to not converge?</span>
        <span style="font-weight: bold; font-style: italic;">x</span> = F.tanh(x) <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">final output should be of size batch_sizex1x28x28</span>
        <span style="font-weight: bold;">return</span> x
</pre>
</div>
<p>
printing a summary of the model:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-discriminator-summary()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-generator()&gt;&gt;</span>
summary(Generator(100).to(device), (1, 100)) <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">forwarding test of a randomly perturbed noise vector</span>
</pre>
</div>

<pre class="example" id="org90a7b35">
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1              [-1, 1, 6272]         633,472
   ConvTranspose2d-2          [-1, 128, 14, 14]         262,272
   ConvTranspose2d-3          [-1, 128, 28, 28]         262,272
            Conv2d-4            [-1, 1, 28, 28]           6,273
================================================================
Total params: 1,164,289
Trainable params: 1,164,289
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 1.01
Params size (MB): 4.44
Estimated Total Size (MB): 5.45
----------------------------------------------------------------
</pre>

<p>
the input to the generator needs to be vectors generated from the latent space. the <code>generate_latent_points()</code> function below implements this and generates a batch with the desired number of points in the latent space that can be used as input to the generator model.
</p>

<div class="org-src-container">
<pre class="src src-python" id="org5076539"><span style="font-weight: bold;">def</span> <span style="font-weight: bold;">generate_latent_points</span>(latent_size, n_points):
  <span style="font-weight: bold;">return</span> torch.randn((n_points, latent_size))
</pre>
</div>
<p>
next, we can use the generated points as input to the generator model to generate new samples. we write the function <code>generate_fake_samples_with_generator()</code> which takes the generator model as an argument and uses it to generate the desired number of samples by first calling the <code>generate_latent_points()</code> function to generate the required number of points in latent space as input to the model. typically this function would be useful after the generator has been trained.
</p>

<div class="org-src-container">
<pre class="src src-python" id="orgf2c3c13"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-generate-latent-points()&gt;&gt;</span>
<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">generate_fake_samples_with_generator</span>(generator, n_samples):
    <span style="font-weight: bold; font-style: italic;">generator_in</span> = generate_latent_points(generator.latent_size, n_samples)
    <span style="font-weight: bold; font-style: italic;">generator_out</span> = generator(generator_in)
    <span style="font-weight: bold; font-style: italic;">labels</span> = torch.zeros((n_samples, 0)) <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">label 0 for fake</span>
    <span style="font-weight: bold;">return</span> generator_out, labels
</pre>
</div>
<p>
to show a few randomly perturbed images (the generator hasnt been trained yet):
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-generator()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-gen-real-samples()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-gen-fake-samples-with-generator()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-gen-images-img()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;">generator</span> = Generator(100)
<span style="font-weight: bold; font-style: italic;">n_samples</span> = 30
<span style="font-weight: bold; font-style: italic;">X</span>, <span style="font-weight: bold; font-style: italic;">_</span> = generate_fake_samples_with_generator(generator, n_samples)
gen_images_img(X.reshape((-1, 28, 28)), f)
</pre>
</div>


<div id="org0ab30cc" class="figure">
<p><img src="/dUD33Ha.png" />
</p>
</div>
</div>
<div id="outline-container-org3e9f828" class="outline-3">
<h3 id="org3e9f828">training the generator</h3>
<div class="outline-text-3" id="text-org3e9f828">
<p>
to train the generator, we need to attach it to the discriminator in a sequential manner, such that the output of the generator would be forwarded to the discriminator for classification, the discriminator would determine whether the data generated by the generator passes as a real image. this is why the training of disciminator beforehand to classify real data from fake ones is cruical, otherwise the generator wouldnt learn (this is one approach, another is described later).
</p>

<p>
during the training of the generator, we may want to freeze the parameters of the discriminator, because we wouldnt want the discriminator training and overfitting on fake data.
</p>

<p>
so the next step is constructing the full generational model by composing the generator and discriminator into one model.
</p>

<div class="org-src-container">
<pre class="src src-python" id="org5b4090e"><span style="font-weight: bold;">class</span> <span style="font-weight: bold; text-decoration: underline;">GAN</span>(nn.Module):
    <span style="font-style: italic;">"""generative adversarial network model"""</span>
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__init__</span>(<span style="font-weight: bold;">self</span>, generator, discriminator):
        <span style="font-weight: bold;">super</span>(GAN, <span style="font-weight: bold;">self</span>).__init__()
        <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">discriminator</span> = discriminator
        <span style="font-weight: bold;">self</span>.<span style="font-weight: bold; font-style: italic;">generator</span> = generator

    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">forward</span>(<span style="font-weight: bold;">self</span>, x):
        <span style="font-weight: bold; font-style: italic;">x</span> = <span style="font-weight: bold;">self</span>.generator(x)
        <span style="font-weight: bold; font-style: italic;">x</span> = <span style="font-weight: bold;">self</span>.discriminator(x)
        <span style="font-weight: bold;">return</span> x
</pre>
</div>
<p>
its that simple, a sequential composition of the two models.
</p>

<div class="org-src-container">
<pre class="src src-python" id="orgaf46abc"><span style="font-weight: bold;">def</span> <span style="font-weight: bold;">train_generator</span>(gan, n_iters=100, batch_size=256):
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">use the stochastic gradient descent optimizer with a learning rate of 0.0002</span>
    <span style="font-weight: bold; font-style: italic;">optimizer</span> = torch.optim.SGD(gan.generator.parameters(), lr=0.0002)
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">our loss function, the Binary Cross Entropy function</span>
    <span style="font-weight: bold; font-style: italic;">loss_fn</span> = nn.BCELoss()
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">tracking loss history</span>
    <span style="font-weight: bold; font-style: italic;">loss_hist</span> = []

    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">we train on one batch per iteration</span>
    <span style="font-weight: bold;">for</span> i <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">range</span>(n_iters):
        <span style="font-weight: bold; font-style: italic;">x</span> = generate_latent_points(gan.generator.latent_size, batch_size).to(device)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">the desired outputs are 1, for 'real', since our goal is to adjust the weights and generate images which pass as 'real'</span>
        <span style="font-weight: bold; font-style: italic;">y</span> = torch.ones((batch_size, 1)).to(device)

        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">zero your gradients for every batch!</span>
        optimizer.zero_grad()
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">make predictions for this batch</span>
        <span style="font-weight: bold; font-style: italic;">out</span> = gan(x)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">compute the loss and its gradients</span>
        <span style="font-weight: bold; font-style: italic;">loss</span> = loss_fn(out, y)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">compute the gradients</span>
        loss.backward()
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">adjust learning weights</span>
        optimizer.step()

        loss_hist.append(loss.item())

    <span style="font-weight: bold;">return</span> loss_hist
</pre>
</div>
<p>
an example usage:
</p>
<div class="org-src-container">
<pre class="src src-python" id="orgd969bdd"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-train-discriminator()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-generate-latent-points()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-generator()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-gan()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-train-generator()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;">latent_size</span> = 100
<span style="font-weight: bold; font-style: italic;">discriminator</span> = Discriminator().to(device)
train_discriminator(discriminator, train_x, n_iter=1000)
discriminator.freeze()
<span style="font-weight: bold; font-style: italic;">generator</span> = Generator(latent_size)
<span style="font-weight: bold; font-style: italic;">gan</span> = GAN(generator, discriminator).to(device)
<span style="font-weight: bold; font-style: italic;">loss_hist</span> = train_generator(gan, 500, 256) <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">increase 500 for better results</span>
plt.figure()
plt.plot(loss_hist)
plt.savefig(f, transparent=<span style="font-weight: bold; text-decoration: underline;">True</span>)
f
</pre>
</div>


<div id="org2a916b5" class="figure">
<p><img src="/YMD1rgW.png" />
</p>
</div>

<p>
in theory, this could work, in practice, i had trouble getting sensible results:
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-train-generator-2()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;">imgs</span> = generator(torch.randn((30,1,100)).to(device)).detach().cpu()
gen_images_img(imgs.reshape((-1, 28, 28)), f)
f
</pre>
</div>

<p>
<img src="/3PW7kEK.png" />
after 5000 batches these were the results i got, which arent much, with further training i could've gotten better results, but i needed a better approach to get faster convergence.
</p>

<p>
as we're only training the generator on random inputs, and the discriminator stays frozen, only one competitor is making progress, whereas if the two were competing, as in a true zero-sum game, we'd get better results, because as the discriminator gets better, the generator would also need to get better in order to be able to fool its competitor, so a better approach would be to train both at the same time, and to train the discriminator on the outputs of the generator, which we label as fake, so that it'd get better at detecting the generator's attempts at fooling it, which in turn makes the generator get better at trying to fool the discriminator.
</p>

<p>
the following method, <code>train_gan</code>, implements this approach, and on each epoch (iteration through the entire dataset), it generates some samples for later evaluation (ofcourse a better approach would be to copy the model itself), for further appreciation and more insight into how the model's output changes over epochs.
</p>

<div class="org-src-container">
<pre class="src src-python" id="org5685e52"><span style="font-weight: bold;">def</span> <span style="font-weight: bold;">train_gan</span>(gan, dataset, epochs=10, batch_size=256):
    <span style="font-weight: bold; font-style: italic;">loss_fn</span> = nn.BCELoss()
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">why does using SGD slow down training?</span>
    <span style="font-weight: bold; font-style: italic;">generator_optimizer</span> = torch.optim.Adam(gan.generator.parameters(), lr=0.0002)
    <span style="font-weight: bold; font-style: italic;">discriminator_optimizer</span> = torch.optim.Adam(gan.discriminator.parameters(), lr=0.0002)
    <span style="font-weight: bold; font-style: italic;">batches_per_epoch</span> = <span style="font-weight: bold;">int</span>(dataset.shape[0] // batch_size)
    <span style="font-weight: bold; font-style: italic;">loss_hist</span> = [] <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">loss history per epoch</span>
    <span style="font-weight: bold; font-style: italic;">sample_sets</span> = [] <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">sets of samples for each epoch</span>
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">the 50 points from the latent space to use for the image samples in sample_sets</span>
    <span style="font-weight: bold; font-style: italic;">sample_set_points</span> = torch.randn((50,gan.generator.latent_size)).to(device)
    <span style="font-weight: bold;">for</span> epoch <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">range</span>(epochs):
        <span style="font-weight: bold; font-style: italic;">loss</span> = 0
        <span style="font-weight: bold;">for</span> batch_i <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">range</span>(batches_per_epoch):
            <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">generate fake images</span>
            <span style="font-weight: bold; font-style: italic;">img_shape</span> = dataset.shape[1:]
            <span style="font-weight: bold; font-style: italic;">batch_latent_points</span> = torch.randn((batch_size,gan.generator.latent_size)).to(device)

            <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">train generator</span>
            <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">zero your gradients for every batch!</span>
            generator_optimizer.zero_grad()
            <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">make predictions for this batch</span>
            <span style="font-weight: bold; font-style: italic;">generator_out</span> = gan.generator(batch_latent_points)
            <span style="font-weight: bold; font-style: italic;">gan_out</span> = gan.discriminator(generator_out)
            <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">compute the loss and its gradients</span>
            <span style="font-weight: bold; font-style: italic;">gan_loss</span> = loss_fn(gan_out, torch.ones((batch_size, 1)).to(device))
            gan_loss.backward()
            <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">adjust learning weights</span>
            generator_optimizer.step()

            <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">train discriminator on fake images</span>
            <span style="font-weight: bold; font-style: italic;">batch_fake_imgs</span> = generator_out.detach()
            discriminator_optimizer.zero_grad()
            <span style="font-weight: bold; font-style: italic;">discriminator_out_fake</span> = gan.discriminator(batch_fake_imgs)
            <span style="font-weight: bold; font-style: italic;">discriminator_loss_fake</span> = loss_fn(discriminator_out_fake, torch.zeros((batch_size,1)).to(device))
            discriminator_loss_fake.backward()
            discriminator_optimizer.step()
            <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">train discriminator on real images</span>
            <span style="font-weight: bold; font-style: italic;">batch_real_imgs</span> = dataset[batch_i*batch_size:(batch_i+1)*batch_size]
            discriminator_optimizer.zero_grad()
            <span style="font-weight: bold; font-style: italic;">discriminator_out_real</span> = gan.discriminator(batch_real_imgs)
            <span style="font-weight: bold; font-style: italic;">discriminator_loss_real</span> = loss_fn(discriminator_out_real, torch.ones((batch_size,1)).to(device))
            <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">loss = (discriminator_loss_fake + discriminator_loss_real) / 2</span>
            discriminator_loss_real.backward()
            discriminator_optimizer.step()

            <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">print batch summary</span>
            <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">print(f'epoch {epoch}, batch {batch_i}, discriminator loss {discriminator_loss_fake.item()}, generator loss {gan_loss.item()}')</span>
            <span style="font-weight: bold; font-style: italic;">loss</span> += (discriminator_loss_fake.item() +
                     discriminator_loss_real.item() +
                     gan_loss.item()) / 3
        <span style="font-weight: bold; font-style: italic;">loss</span> /= batches_per_epoch <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">i dont think this is necessary as the loss is averaged over the batch by default (but it doesnt affect the model)</span>
        loss_hist.append(loss)
        <span style="font-weight: bold; font-style: italic;">sample_set</span> = generator(sample_set_points)
        sample_sets.append(sample_set.detach().cpu().clone())
    <span style="font-weight: bold;">return</span> sample_sets, loss_hist
</pre>
</div>
<p>
i did a test run of 10 epochs:
</p>
<div class="org-src-container">
<pre class="src src-python" id="org59b910a"><span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-train-discriminator()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-generate-latent-points()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-generator()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-gan()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-train-gan()&gt;&gt;</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&lt;&lt;src-gen-images-img()&gt;&gt;</span>
<span style="font-weight: bold;">import</span> os.path
<span style="font-weight: bold;">import</span> time
<span style="font-weight: bold; font-style: italic;">start_time</span> = time.time()
<span style="font-weight: bold; font-style: italic;">discriminator</span> = Discriminator()
<span style="font-weight: bold; font-style: italic;">latent_size</span> = 40
<span style="font-weight: bold; font-style: italic;">epochs</span> = 100
<span style="font-weight: bold; font-style: italic;">generator</span> = Generator(latent_size)
<span style="font-weight: bold; font-style: italic;">gan</span> = GAN(generator, discriminator).to(device)
<span style="font-weight: bold; font-style: italic;">sample_sets</span>, <span style="font-weight: bold; font-style: italic;">loss_hist</span> = train_gan(gan, train_x, epochs)
plt.figure()
plt.plot(loss_hist)
plt.savefig(f, transparent=<span style="font-weight: bold; text-decoration: underline;">True</span>)
<span style="font-weight: bold;">print</span>(f<span style="font-style: italic;">'[[</span>{f}<span style="font-style: italic;">]]'</span>)
<span style="font-weight: bold;">for</span> i, collage <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">enumerate</span>(sample_sets):
    <span style="font-weight: bold; font-style: italic;">filename</span> = os.path.join(d, f<span style="font-style: italic;">'</span>{i}<span style="font-style: italic;">.png'</span>)
    gen_images_img(collage.view((-1, 28, 28)), filename)
    <span style="font-weight: bold;">print</span>(f<span style="font-style: italic;">'samples after epoch </span>{i+1}<span style="font-style: italic;">:'</span>)
    <span style="font-weight: bold;">print</span>(f<span style="font-style: italic;">'[[</span>{filename}<span style="font-style: italic;">]]'</span>)
<span style="font-weight: bold;">print</span>(f<span style="font-style: italic;">'---time elapsed: </span>{time.time() - start_time}<span style="font-style: italic;"> seconds ---'</span>)
</pre>
</div>

<p>
<img src="/ZJpShnf.svg" />
samples after epoch 1:
<img src="/0.png" />
samples after epoch 2:
<img src="/1.png" />
&#x2026;
</p>

<p>
&#x2026;
</p>

<p>
&#x2026;
</p>

<p>
samples after epoch 99:
<img src="/98.png" />
samples after epoch 100:
<img src="/99.png" />
&#x2014;time elapsed: 7404.322350978851 seconds &#x2014;
</p>
<p>
in hindsight, i should've separated the different losses and plotted the discriminator's and the generator's losses each with its own graphs, because it makes no sense that the loss only goes up, but it happens because the discriminator gets better at its job faster than the generator and so the generator's loss goes up.
</p>

<p>
but the results are interesting, even after the first epoch, we can see that the model has already picked up on some subtle curvy patterns in the center of the image. we could have gotten better results with a few more training epochs and some luck.
</p>
</div>
</div>
</div>
<div id="outline-container-org67f950e" class="outline-2">
<h2 id="org67f950e">performance evaluation</h2>
<div class="outline-text-2" id="text-org67f950e">
<p>
generally, there are no objective ways to evaluate the performance of a GAN model. we cannot calculate this objective error score for generated images. it might be possible in the case of MNIST images because the images are so well constrained, but in general, it is not possible (yet).
</p>

<p>
instead, images must be subjectively evaluated for quality by a human agent. this means that we cannot know when to stop training without looking at examples of generated images. in turn, the adversarial nature of the training process means that the generator is changing after every batch, meaning that once "good enough" images can be generated, the subjective quality of the images may then begin to vary, improve, or even degrade with subsequent updates.
</p>
</div>
</div>
<div id="outline-container-org6574674" class="outline-2">
<h2 id="org6574674">latent output space visualization</h2>
<div class="outline-text-2" id="text-org6574674">
<p>
while we cant visualize dimensions higher than 3d, and while computer screens are 2d, we can still try to explore higher dimensions by "navigating" them and projecting the results. by navigation i mean choosing an axis and "hovering" over its possible values while holding the other variables constant, to see how the changes to the variable that corresponds to the axis we chose affect the space we project.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">def</span> <span style="font-weight: bold;">generate_traces</span>():
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">size of linspace to generate</span>
    <span style="font-weight: bold; font-style: italic;">num_points</span> = 50
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">our point, we will manipulate the number at the first index</span>
    <span style="font-weight: bold; font-style: italic;">latent_point</span> = torch.randn((latent_size,)).to(device)

    <span style="font-weight: bold; font-style: italic;">all_latent_points</span> = latent_point.repeat(num_points, 1)
    <span style="font-weight: bold; font-style: italic;">lin</span> = torch.linspace(-1, 1, steps=num_points)
    <span style="font-weight: bold;">for</span> i, point <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">enumerate</span>(all_latent_points):
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">update the value only for half the axes</span>
        <span style="font-weight: bold;">for</span> j <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">range</span>(<span style="font-weight: bold;">len</span>(point)//2):
            <span style="font-weight: bold; font-style: italic;">point</span>[j] = lin[i]

    <span style="font-weight: bold;">return</span> all_latent_points.detach().cpu(), generator(all_latent_points).detach().cpu()

<span style="font-weight: bold;">import</span> plotly.graph_objects <span style="font-weight: bold;">as</span> go

<span style="font-weight: bold; font-style: italic;">fig</span> = go.Figure()

<span style="font-weight: bold; font-style: italic;">xs</span>, <span style="font-weight: bold; font-style: italic;">ys</span> = generate_traces()

<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">add "traces", one for each slider step</span>
<span style="font-weight: bold;">for</span> i <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">range</span>(<span style="font-weight: bold;">len</span>(xs)):
    fig.add_trace(go.Heatmap(z=ys[i].view((28, 28)).numpy(),
                             colorscale=<span style="font-style: italic;">'gray'</span>,
                             visible=<span style="font-weight: bold; text-decoration: underline;">False</span>,
                             zmin=-1,
                             zmax=1,
                             showscale=<span style="font-weight: bold; text-decoration: underline;">False</span>))

<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">make 10th trace visible</span>
fig.data[10].visible = <span style="font-weight: bold; text-decoration: underline;">True</span>

<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">create and add slider</span>
<span style="font-weight: bold; font-style: italic;">steps</span> = []
<span style="font-weight: bold;">for</span> i <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">range</span>(<span style="font-weight: bold;">len</span>(fig.data)):
    <span style="font-weight: bold; font-style: italic;">step</span> = <span style="font-weight: bold;">dict</span>(
        method=<span style="font-style: italic;">"update"</span>,
        args=[{<span style="font-style: italic;">"visible"</span>: [<span style="font-weight: bold; text-decoration: underline;">False</span>] * <span style="font-weight: bold;">len</span>(fig.data)}],
              <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">{"title": "slider switched to step: " + str(i)}],  # layout attribute</span>
    )
    step[<span style="font-style: italic;">"args"</span>][0][<span style="font-style: italic;">"visible"</span>][i] = <span style="font-weight: bold; text-decoration: underline;">True</span>  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">toggle i'th trace to "visible"</span>
    <span style="font-weight: bold; font-style: italic;">step</span>[<span style="font-style: italic;">'label'</span>] = f<span style="font-style: italic;">'</span>{xs[i][0].item()}<span style="font-style: italic;">'</span> <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">the value for the first axis/var, which is the same for half of them</span>
    steps.append(step)

<span style="font-weight: bold; font-style: italic;">sliders</span> = [<span style="font-weight: bold;">dict</span>(
    active=10,
    currentvalue={<span style="font-style: italic;">"prefix"</span>: <span style="font-style: italic;">"x: "</span>},
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">pad={"t": 50},</span>
    steps=steps
)]

fig.update_layout(
    sliders=sliders
)

<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">fig.show() # you might want to uncomment this and comment the next ones</span>
fig.write_html(f)
<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">'visit manually:'</span>, f)
</pre>
</div>

<pre class="example">
visit manually: /home/mahmooz/brain/out/KYJ3p2o.html
</pre>


<iframe src="/KYJ3p2o.html" style="aspect-ratio: 1 / 1; width: 100%;"></iframe>
<p>
if you play with the scroll enough, you may subtly see the shape of one digit morphing into another, this is how the internal structure of a latent space behaves, two points that are close to each other may have a close role in the generator's output space.</p>
</div>
</div>
</div>
</body>
</html>
